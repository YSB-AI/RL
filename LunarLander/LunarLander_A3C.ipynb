{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from A3C_Agent import *\n",
    "from ENV_DETAILS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\LunarLander\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/LunarLander/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_a3c/' # Linux\n",
    "\n",
    "ENV = \"LunarLander-v2\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<LunarLander<LunarLander-v2>>>>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13725457,  1.046495  ,  1.6977865 , -2.3692205 , -1.0841154 ,\n",
       "       -2.9228    ,  0.90587187,  0.67300284], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
       "  -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
       "  1.       ], (8,), float32),\n",
       " (8,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00345535,  1.4060721 ,  0.3499771 , -0.21547431, -0.00399713,\n",
       "       -0.07927497,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " Discrete(4),\n",
       " (array([-0.00464497,  1.4085038 , -0.24120267, -0.06632995,  0.00734536,\n",
       "          0.09393398,  0.        ,  0.        ], dtype=float32),\n",
       "  -1.90531853937773,\n",
       "  False,\n",
       "  False,\n",
       "  {}))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()[0]\n",
    "env.action_space.n, env.action_space, env.step(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Asynch Actor-Critic \n",
    "\n",
    "This time we will implement the A3C not considering Actor and Critic as part of the same network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs_general/hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_A3C/\"\n",
    "\n",
    "if TUNING_TYPE == \"MANUAL\":\n",
    "    params = {}\n",
    "    params[\"n_enviroment\"] =[20]\n",
    "    params[\"discount\"] =[0.96, 0.97]\n",
    "    params[\"end_of_episode\"] = [400]\n",
    "    params[\"learning_rate\"] = [0.001]\n",
    "    params[\"entropy_factor\"] = [ 0.01, 0.05]\n",
    "    params[\"epsilon\"] = [1]\n",
    "    params[\"boltzman_factor\"] = [1]\n",
    "    params[\"exploration_technique\"] = ['soft', 'epsilon']\n",
    "    params[\"training_steps\"] = [3000000]\n",
    "    params[\"dense_units\"] = [32, 128]\n",
    "    params[\"time_to_update\"] = [400]\n",
    "    params[\"use_LSTM\"] =[False]\n",
    "\n",
    "\n",
    "    hyperparam_combination = list(itertools.product(*list(params.values())))\n",
    "    \n",
    "\n",
    "    try:\n",
    "\n",
    "        files = [name for name in os.listdir(logs_dir) if os.path.isfile(os.path.join(logs_dir, name)) and name != \"logfile.txt\" and name != \"merged_results.json\"]\n",
    "        if len(files) >= 1: merge_JsonFiles(main_hyper_dir, logs_dir, files)\n",
    "\n",
    "        res_file = logs_dir+\"merged_results.json\"\n",
    "        def without_keys(d, keys):\n",
    "            return {x: d[x] for x in d if x not in keys}\n",
    "\n",
    "\n",
    "        if os.path.isfile(res_file):\n",
    "            with open(res_file, 'r') as f:\n",
    "                complete_file = json.load(f)\n",
    "\n",
    "            newlist = sorted(complete_file, key=lambda d: d['mean_rewards'], reverse=True) \n",
    "            params = []\n",
    "            for i, f in enumerate(newlist):\n",
    "                label = \"disc : \"+str(f['discount'])+\" | \"+\"lr : \"+str(f['learning_rate'])+\" | \"+\"entropy : \"+str(f['entropy_factor'])+\" | \"+\"update : \"+str(f['time_to_update'])\n",
    "                plt.figure(figsize=[35,4])\n",
    "                plt.plot(f['rewards'], label = label)\n",
    "                plt.legend()\n",
    "                max_mean_reward = f['mean_rewards']\n",
    "                params.append(f)\n",
    "                print(without_keys(f,\"rewards\"))\n",
    "                if i == 10:\n",
    "                    break\n",
    "            plt.title(\"Evaluation rewards\"); plt.grid();\n",
    "            plt.show()\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35 Complete [01h 36m 17s]\n",
      "total_train_reward: 155.28341060132752\n",
      "\n",
      "Best total_train_reward So Far: 256.9092418548197\n",
      "Total elapsed time: 07h 07m 09s\n",
      "\n",
      "Search: Running Trial #36\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.99              |0.99              |discount\n",
      "0.049146          |0.027255          |entropy_factor\n",
      "0.0040571         |0.0012834         |learning_rate\n",
      "472               |162               |dense_units\n",
      "\n",
      "Trial number :  36\n",
      "Epoch: 2000 : Reward eval/Train: -776.1193574437958/-427.60788489459634 | epsilon : 1\n",
      "Epoch: 4000 : Reward eval/Train: -984.2079880481681/-1847.1211064787399 | epsilon : 1\n",
      "Epoch: 6000 : Reward eval/Train: -957.3014406315106/-618.5742916047288 | epsilon : 1\n",
      "Epoch: 8000 : Reward eval/Train: -728.3624438273151/-519.4282390801923 | epsilon : 1\n",
      "Epoch: 10000 : Reward eval/Train: -765.6866608673913/-728.456161183091 | epsilon : 1\n",
      "Epoch: 12000 : Reward eval/Train: -1571.5876951016671/-569.5907977323143 | epsilon : 1\n",
      "Epoch: 14000 : Reward eval/Train: -438.90633161206443/-583.9262214869691 | epsilon : 1\n",
      "Epoch: 16000 : Reward eval/Train: -653.8023734812764/-912.2339817538041 | epsilon : 1\n",
      "Epoch: 18000 : Reward eval/Train: -465.8209032867454/-817.2602543607908 | epsilon : 1\n",
      "Epoch: 20000 : Reward eval/Train: -518.5652281466964/-457.08593146429916 | epsilon : 1\n",
      "Epoch: 22000 : Reward eval/Train: -2821.8829651352407/-557.0668106220853 | epsilon : 1\n",
      "Epoch: 24000 : Reward eval/Train: -475.54559321660196/-396.41401779985455 | epsilon : 1\n",
      "Epoch: 26000 : Reward eval/Train: -624.1516908812147/-1409.759045777852 | epsilon : 1\n",
      "Epoch: 28000 : Reward eval/Train: -524.23318381421/-578.7017989557079 | epsilon : 1\n",
      "Epoch: 30000 : Reward eval/Train: -683.3397255643381/-374.12709039042034 | epsilon : 1\n",
      "Epoch: 32000 : Reward eval/Train: -787.6630649509683/-1203.6486683017208 | epsilon : 1\n",
      "Epoch: 34000 : Reward eval/Train: -1235.211973466848/-554.6388449298538 | epsilon : 1\n",
      "Epoch: 36000 : Reward eval/Train: -480.93789549347196/-1233.7564162861347 | epsilon : 1\n",
      "Epoch: 38000 : Reward eval/Train: -2148.7821518557294/-576.2179174603175 | epsilon : 1\n",
      "Epoch: 40000 : Reward eval/Train: -583.0950215689777/-514.756929403781 | epsilon : 1\n",
      "Epoch: 42000 : Reward eval/Train: -623.4378207525175/-566.7617533329023 | epsilon : 1\n",
      "Epoch: 44000 : Reward eval/Train: -671.9047609536569/-402.895796779852 | epsilon : 1\n",
      "Epoch: 46000 : Reward eval/Train: -474.31750272513386/-829.5700621941534 | epsilon : 1\n",
      "Epoch: 48000 : Reward eval/Train: -534.5819886021584/-447.1657418733659 | epsilon : 1\n",
      "Epoch: 50000 : Reward eval/Train: -2099.3247058270103/-1436.1340545588864 | epsilon : 1\n",
      "Epoch: 52000 : Reward eval/Train: -661.6010415167672/-540.1624114494794 | epsilon : 1\n",
      "Epoch: 54000 : Reward eval/Train: -613.3181311864447/-2668.731825594417 | epsilon : 1\n",
      "Epoch: 56000 : Reward eval/Train: -704.0092473753233/-749.3984737320569 | epsilon : 1\n",
      "Epoch: 58000 : Reward eval/Train: -622.1787822807213/-708.201244420127 | epsilon : 1\n",
      "Epoch: 60000 : Reward eval/Train: -527.5774405563857/-1338.1061950838055 | epsilon : 1\n",
      "Epoch: 62000 : Reward eval/Train: -445.14839915719233/-597.2652350892658 | epsilon : 1\n",
      "Epoch: 64000 : Reward eval/Train: -491.7260932067024/-2414.6688958423965 | epsilon : 1\n",
      "Epoch: 66000 : Reward eval/Train: -656.6277465748041/-740.9993174376066 | epsilon : 1\n",
      "Epoch: 68000 : Reward eval/Train: -675.701431329011/-642.3368813285389 | epsilon : 1\n",
      "Epoch: 70000 : Reward eval/Train: -458.98161564628634/-570.2708563389642 | epsilon : 1\n",
      "Epoch: 72000 : Reward eval/Train: -745.1710013271008/-682.6404222328237 | epsilon : 1\n",
      "Epoch: 74000 : Reward eval/Train: -400.93399872312654/-434.07741066760036 | epsilon : 1\n",
      "Epoch: 76000 : Reward eval/Train: -1099.1596437290725/-506.9184013467279 | epsilon : 1\n",
      "Epoch: 78000 : Reward eval/Train: -449.0767848758386/-491.3372475631043 | epsilon : 1\n",
      "Epoch: 80000 : Reward eval/Train: -900.5390960128179/-1328.1648141794512 | epsilon : 1\n",
      "Epoch: 82000 : Reward eval/Train: -764.9207226618764/-714.2530063828639 | epsilon : 1\n",
      "Epoch: 84000 : Reward eval/Train: -2238.4582004867784/-576.2237281384805 | epsilon : 1\n",
      "Epoch: 86000 : Reward eval/Train: -553.7760552558626/-1863.0527115829245 | epsilon : 1\n",
      "Epoch: 88000 : Reward eval/Train: -787.2705281840027/-736.5728879147536 | epsilon : 1\n",
      "Epoch: 90000 : Reward eval/Train: -505.5900101692106/-1362.1876034637996 | epsilon : 1\n",
      "Epoch: 92000 : Reward eval/Train: -655.5714867664159/-1011.9800277453082 | epsilon : 1\n",
      "Epoch: 94000 : Reward eval/Train: -797.3198753104031/-730.6704073868385 | epsilon : 1\n",
      "Epoch: 96000 : Reward eval/Train: -750.8905022604579/-573.986355507886 | epsilon : 1\n",
      "Epoch: 98000 : Reward eval/Train: -479.2281497059266/-605.9434217079756 | epsilon : 1\n",
      "Epoch: 100000 : Reward eval/Train: -605.5050194595219/-748.0796447516705 | epsilon : 1\n",
      "Epoch: 102000 : Reward eval/Train: -863.5363276695145/-645.8805565879908 | epsilon : 1\n",
      "Epoch: 104000 : Reward eval/Train: -569.0669310067867/-1125.2924521692814 | epsilon : 1\n",
      "Epoch: 106000 : Reward eval/Train: -499.72575031030436/-386.4823795814018 | epsilon : 1\n",
      "Epoch: 108000 : Reward eval/Train: -958.1657984361494/-691.2046600845079 | epsilon : 1\n",
      "Epoch: 110000 : Reward eval/Train: -1180.366583651135/-685.2797634116706 | epsilon : 1\n",
      "Epoch: 112000 : Reward eval/Train: -1996.2077311245355/-796.6457396815092 | epsilon : 1\n",
      "Epoch: 114000 : Reward eval/Train: -815.2142785320488/-569.8979873610411 | epsilon : 1\n",
      "Epoch: 116000 : Reward eval/Train: -664.8996154696982/-452.803074002299 | epsilon : 1\n",
      "Epoch: 118000 : Reward eval/Train: -804.1932558520634/-531.6968571162191 | epsilon : 1\n",
      "Epoch: 120000 : Reward eval/Train: -834.9937529298358/-708.1463208159026 | epsilon : 1\n",
      "Epoch: 122000 : Reward eval/Train: -571.9619381854354/-530.2743774232498 | epsilon : 1\n",
      "Epoch: 124000 : Reward eval/Train: -963.9091690264355/-769.6161407448155 | epsilon : 1\n",
      "Epoch: 126000 : Reward eval/Train: -504.74504198651533/-906.2059040053196 | epsilon : 1\n",
      "Epoch: 128000 : Reward eval/Train: -466.038432599174/-480.55695097180654 | epsilon : 1\n",
      "Epoch: 130000 : Reward eval/Train: -534.3715309478525/-624.0316057181458 | epsilon : 1\n",
      "Epoch: 132000 : Reward eval/Train: -1591.0503009009892/-820.5661314325773 | epsilon : 1\n",
      "Epoch: 134000 : Reward eval/Train: -590.8503929563385/-524.0757371378813 | epsilon : 1\n",
      "Epoch: 136000 : Reward eval/Train: -3600.1238406555117/-783.1310339322348 | epsilon : 1\n",
      "Epoch: 138000 : Reward eval/Train: -452.84335007471697/-606.7604922176367 | epsilon : 1\n",
      "Epoch: 140000 : Reward eval/Train: -669.9110288234588/-561.6477253188256 | epsilon : 1\n",
      "Epoch: 142000 : Reward eval/Train: -535.8671969636105/-8461.71384297904 | epsilon : 1\n",
      "Epoch: 144000 : Reward eval/Train: -1027.2737851002862/-880.5232218169926 | epsilon : 1\n",
      "Epoch: 146000 : Reward eval/Train: -1887.8679536336913/-584.2871441055482 | epsilon : 1\n",
      "Epoch: 148000 : Reward eval/Train: -638.5834544781569/-976.5719941160164 | epsilon : 1\n",
      "Epoch: 150000 : Reward eval/Train: -704.126719922985/-611.3398652739056 | epsilon : 1\n",
      "Epoch: 152000 : Reward eval/Train: -546.6011118597756/-543.4563145978445 | epsilon : 1\n",
      "Epoch: 154000 : Reward eval/Train: -624.2624648892585/-611.2235955327176 | epsilon : 1\n",
      "Epoch: 156000 : Reward eval/Train: -481.75850383327514/-558.7648495670054 | epsilon : 1\n",
      "Epoch: 158000 : Reward eval/Train: -679.7801318984355/-511.6351037185142 | epsilon : 1\n",
      "Epoch: 160000 : Reward eval/Train: -749.3026771394776/-424.0867557527268 | epsilon : 1\n",
      "Epoch: 162000 : Reward eval/Train: -814.2642387228605/-506.27807573638984 | epsilon : 1\n",
      "Epoch: 164000 : Reward eval/Train: -540.7021599448649/-1031.5524274435752 | epsilon : 1\n",
      "Epoch: 166000 : Reward eval/Train: -780.6927124779575/-486.4641436033857 | epsilon : 1\n",
      "Epoch: 168000 : Reward eval/Train: -483.0745888326089/-670.8156826568315 | epsilon : 1\n",
      "Epoch: 170000 : Reward eval/Train: -452.113699428941/-490.4717441271643 | epsilon : 1\n",
      "Epoch: 172000 : Reward eval/Train: -1320.956626137503/-587.3849897893995 | epsilon : 1\n",
      "Epoch: 174000 : Reward eval/Train: -548.9518961548226/-647.9750072989236 | epsilon : 1\n",
      "Epoch: 176000 : Reward eval/Train: -1641.5089646070476/-683.7687443158632 | epsilon : 1\n",
      "Epoch: 178000 : Reward eval/Train: -391.5215167983917/-1268.7279176516756 | epsilon : 1\n",
      "Epoch: 180000 : Reward eval/Train: -522.9969475222147/-912.430961354271 | epsilon : 1\n",
      "Epoch: 182000 : Reward eval/Train: -510.67258544475976/-415.1268957113829 | epsilon : 1\n",
      "Epoch: 184000 : Reward eval/Train: -599.9389477909026/-463.3732714052812 | epsilon : 1\n",
      "Epoch: 186000 : Reward eval/Train: -1171.3215612345027/-843.433683079883 | epsilon : 1\n",
      "Epoch: 188000 : Reward eval/Train: -1009.5361055847735/-814.0630346921438 | epsilon : 1\n",
      "Epoch: 190000 : Reward eval/Train: -616.3101676713304/-675.5222467039343 | epsilon : 1\n",
      "Epoch: 192000 : Reward eval/Train: -557.9749400195283/-2454.9399828881374 | epsilon : 1\n",
      "Epoch: 194000 : Reward eval/Train: -1278.7334453574044/-666.9445465450166 | epsilon : 1\n",
      "Epoch: 196000 : Reward eval/Train: -1127.0622587651253/-907.6108088146451 | epsilon : 1\n",
      "Epoch: 198000 : Reward eval/Train: -481.43187162026425/-428.40735707931793 | epsilon : 1\n",
      "Epoch: 200000 : Reward eval/Train: -2857.244580223404/-576.7185923573784 | epsilon : 1\n",
      "Epoch: 202000 : Reward eval/Train: -559.8608827063629/-843.5327577499862 | epsilon : 1\n",
      "Epoch: 204000 : Reward eval/Train: -517.7770408842275/-1343.1857472717588 | epsilon : 1\n",
      "Epoch: 206000 : Reward eval/Train: -752.8536013283638/-680.2122347602931 | epsilon : 1\n",
      "Epoch: 208000 : Reward eval/Train: -696.0722324574423/-684.9680783362758 | epsilon : 1\n",
      "Epoch: 210000 : Reward eval/Train: -550.2850078794684/-944.4347589901016 | epsilon : 1\n",
      "Epoch: 212000 : Reward eval/Train: -992.7735220210261/-584.3314461846294 | epsilon : 1\n",
      "Epoch: 214000 : Reward eval/Train: -488.50088677742383/-576.1873830546656 | epsilon : 1\n",
      "Epoch: 216000 : Reward eval/Train: -603.3155687040332/-610.3622870553315 | epsilon : 1\n",
      "Epoch: 218000 : Reward eval/Train: -484.8861900468975/-514.1241125076047 | epsilon : 1\n",
      "Epoch: 220000 : Reward eval/Train: -1474.5101645369814/-1831.5656540161676 | epsilon : 1\n",
      "Epoch: 222000 : Reward eval/Train: -937.4607326265732/-878.1481446751875 | epsilon : 1\n",
      "Epoch: 224000 : Reward eval/Train: -648.6014940001508/-545.265847158243 | epsilon : 1\n",
      "Epoch: 226000 : Reward eval/Train: -645.4734459386218/-831.6242561770473 | epsilon : 1\n",
      "Epoch: 228000 : Reward eval/Train: -2128.9698332008397/-751.8623105415818 | epsilon : 1\n",
      "Epoch: 230000 : Reward eval/Train: -500.80956473023787/-1555.9680989306567 | epsilon : 1\n",
      "Epoch: 232000 : Reward eval/Train: -576.5023263947969/-816.5763251325537 | epsilon : 1\n",
      "Epoch: 234000 : Reward eval/Train: -503.3725762797164/-611.3262187447467 | epsilon : 1\n",
      "Epoch: 236000 : Reward eval/Train: -1052.3879958916068/-519.9780293970766 | epsilon : 1\n",
      "Epoch: 238000 : Reward eval/Train: -632.8861085121251/-439.4392431520746 | epsilon : 1\n",
      "Epoch: 240000 : Reward eval/Train: -1300.946891091104/-715.2913458947371 | epsilon : 1\n",
      "Epoch: 242000 : Reward eval/Train: -639.5369441697919/-444.90291725376585 | epsilon : 1\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_A3C\"\n",
    "    project_name = \"keras_tunning_soft\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel(hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/A3C_soft/\", \n",
    "                          end_of_episode = EPISODES, n_enviroment = 5, \n",
    "                  evaluation_epoch = 2000, training_steps = 600000,\n",
    "                  sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                  discount_min = 0.90, discount_max = 0.99,\n",
    "                  entropy_min = 0.0001, entropy_max = 0.05,\n",
    "                  lr_min = 0.00001, lr_max = 0.005,\n",
    "                  dense_min = 32, dense_max = 500,\n",
    "                  lstm_min = 32, lstm_max = 128,\n",
    "                  time_to_update_min = 100, time_to_update_max=600,\n",
    "                  environment_name=ENV),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 40,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "    \n",
    "    # project_name = \"keras_tunning_epsilon\"\n",
    "    # tuner = kt.BayesianOptimization(\n",
    "    #         MyHyperModel(hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/A3C_epsilon/\", exploration_tech = \"epsilon\"),\n",
    "    #         objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "    #         max_trials = 50,\n",
    "    #         # distribution_strategy= strategy,\n",
    "    #         directory=dir,\n",
    "    #         project_name=project_name\n",
    "    #     )\n",
    "    # tuner.search(x=[0], y=[1])\n",
    "\n",
    "    # project_name = \"keras_tunning_boltzman\"\n",
    "    # tuner = kt.BayesianOptimization(\n",
    "    #         MyHyperModel(hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/A3C_boltzman/\", exploration_tech = \"boltzman\"),\n",
    "    #         objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "    #         max_trials = 50,\n",
    "    #         # distribution_strategy= strategy,\n",
    "    #         directory=dir,\n",
    "    #         project_name=project_name\n",
    "    #     )\n",
    "    # tuner.search(x=[0], y=[1])\n",
    "\n",
    "    # dir = r\"Hyperparam_kt_A3C_LSTM\"\n",
    "    # project_name = \"keras_tunning_soft\"\n",
    "    # tuner_lstm = kt.BayesianOptimization(\n",
    "    #         MyHyperModel(hyper_dir = dir+\"/\"+project_name, writer = \"logs_A3C_LSTM/A3C_soft/\" ,use_LSTM = True),\n",
    "    #         objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "    #         max_trials = 40,\n",
    "    #         # distribution_strategy= strategy,\n",
    "    #         directory=dir,\n",
    "    #         project_name='keras_tunning'\n",
    "    #     )\n",
    "    # tuner_lstm.search(x=[0], y=[1])\n",
    "\n",
    "    # project_name = \"keras_tunning_epsilon\"\n",
    "    # tuner_lstm = kt.BayesianOptimization(\n",
    "    #         MyHyperModel(hyper_dir = dir+\"/\"+project_name, writer = \"logs_A3C_LSTM/A3C_epsilon/\" ,use_LSTM = True, exploration_tech = \"epsilon\"),\n",
    "    #         objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "    #         max_trials = 40,\n",
    "    #         # distribution_strategy= strategy,\n",
    "    #         directory=dir,\n",
    "    #         project_name='keras_tunning'\n",
    "    #     )\n",
    "    # tuner_lstm.search(x=[0], y=[1])\n",
    "\n",
    "    # project_name = \"keras_tunning_boltzman\"\n",
    "    # tuner_lstm = kt.BayesianOptimization(\n",
    "    #         MyHyperModel(hyper_dir = dir+\"/\"+project_name, writer = \"logs_A3C_LSTM/A3C_boltzman/\" ,use_LSTM = True, exploration_tech = \"boltzman\"),\n",
    "    #         objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "    #         max_trials = 40,\n",
    "    #         # distribution_strategy= strategy,\n",
    "    #         directory=dir,\n",
    "    #         project_name='keras_tunning'\n",
    "    #     )\n",
    "    # tuner_lstm.search(x=[0], y=[1])\n",
    "   \n",
    "else : \n",
    "    \n",
    "    for exploration_tech in ['boltzman', 'epsilon']:\n",
    "        print(\"Acquiring parameters ....\")\n",
    "        writer= \"Training/fit_A3C/\"\n",
    "\n",
    "        n_enviroment = 10\n",
    "        training_steps = 1000000\n",
    "        learning_rate= 0.001\n",
    "        entropy_factor = 0.1\n",
    "        discount = 0.99\n",
    "        dense_units = 512\n",
    "        lstm_units = 128\n",
    "        time_to_update= 100\n",
    "        end_of_episode = 300\n",
    "        use_LSTM = False\n",
    "\n",
    "        \n",
    "        run_training(training_steps, learning_rate, entropy_factor, exploration_tech, discount, time_to_update, dense_units, lstm_units, n_enviroment, writer, use_LSTM, end_of_episode, environment_name = \"LunarLander-v2\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial id :23 | Score :256.9092418548197 --> {'discount': 0.99, 'entropy_factor': 0.02725512838544938, 'learning_rate': 0.0012833657996505648, 'dense_units': 162}\n",
      "Trial id :07 | Score :255.4684100390277 --> {'discount': 0.99, 'entropy_factor': 0.04058888538676041, 'learning_rate': 0.00095066954602934, 'dense_units': 372}\n",
      "Trial id :28 | Score :251.59047128674828 --> {'discount': 0.99, 'entropy_factor': 0.023885694097790046, 'learning_rate': 0.002973449264458015, 'dense_units': 193}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exploration_tech = \"soft\"\n",
    "\n",
    "hyperparam_combination=[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=3):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "\n",
    "    n_env = 30\n",
    "    end_ep = 1000\n",
    "    ep = 1\n",
    "    bolt_fact = 1\n",
    "    train_steps = 5000000\n",
    "    disc = trials.hyperparameters.values[\"discount\"]\n",
    "    lr = trials.hyperparameters.values[\"learning_rate\"]\n",
    "    entropy_fact = trials.hyperparameters.values[\"entropy_factor\"]\n",
    "    d = trials.hyperparameters.values[\"dense_units\"]\n",
    "    time_to_update = 100 if exploration_tech == \"soft\" else trials.hyperparameters.values[\"time_to_update\"]\n",
    "    ulstm = False\n",
    "\n",
    "    hyperparam_combination.append((n_env, disc, end_ep, lr, entropy_fact, ep, bolt_fact, exploration_tech, train_steps, d, time_to_update, ulstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " [(30,\n",
       "   0.99,\n",
       "   1000,\n",
       "   0.0012833657996505648,\n",
       "   0.02725512838544938,\n",
       "   1,\n",
       "   1,\n",
       "   'soft',\n",
       "   5000000,\n",
       "   162,\n",
       "   100,\n",
       "   False),\n",
       "  (30,\n",
       "   0.99,\n",
       "   1000,\n",
       "   0.00095066954602934,\n",
       "   0.04058888538676041,\n",
       "   1,\n",
       "   1,\n",
       "   'soft',\n",
       "   5000000,\n",
       "   372,\n",
       "   100,\n",
       "   False),\n",
       "  (30,\n",
       "   0.99,\n",
       "   1000,\n",
       "   0.002973449264458015,\n",
       "   0.023885694097790046,\n",
       "   1,\n",
       "   1,\n",
       "   'soft',\n",
       "   5000000,\n",
       "   193,\n",
       "   100,\n",
       "   False)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyperparam_combination), hyperparam_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_hyperparam(TUNING_TYPE= \"MANUAL\", TUNING_TYPE = TUNING_TYPE, hyperparam_combination = hyperparam_combination,  total_files = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from Hyperparam_kt_A3C/keras_tunning_soft/tuner0.json\n",
      "Trial id :23 | Score :256.9092418548197 --> {'discount': 0.99, 'entropy_factor': 0.02725512838544938, 'learning_rate': 0.0012833657996505648, 'dense_units': 162}\n",
      "Trial number :  30\n",
      "Moviepy - Building video ./A3C_soft_video.mp4.\n",
      "Moviepy - Writing video ./A3C_soft_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./A3C_soft_video.mp4\n",
      "Final mean reward ' soft ': 251.18667792553265\n",
      "INFO:tensorflow:Reloading Tuner from Hyperparam_kt_A3C/keras_tunning_epsilon/tuner0.json\n",
      "Trial id :21 | Score :243.8703166088034 --> {'discount': 0.99, 'entropy_factor': 0.05, 'learning_rate': 0.005, 'dense_units': 500, 'time_to_update': 500}\n",
      "Trial number :  26\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "Moviepy - Building video ./A3C_epsilon_video.mp4.\n",
      "Moviepy - Writing video ./A3C_epsilon_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./A3C_epsilon_video.mp4\n",
      "Final mean reward ' epsilon ': 236.6058385368619\n",
      "INFO:tensorflow:Reloading Tuner from Hyperparam_kt_A3C/keras_tunning_boltzman/tuner0.json\n",
      "Trial id :02 | Score :266.5749316351991 --> {'discount': 0.99, 'entropy_factor': 0.013874529377827684, 'learning_rate': 0.0007061589601898285, 'dense_units': 294, 'time_to_update': 300}\n",
      "Trial number :  30\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "Moviepy - Building video ./A3C_boltzman_video.mp4.\n",
      "Moviepy - Writing video ./A3C_boltzman_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./A3C_boltzman_video.mp4\n",
      "Final mean reward ' boltzman ': 220.153031865886\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_env = gym.make(ENV, render_mode = \"rgb_array\")\n",
    "dir = r\"Hyperparam_kt_A3C\"\n",
    "\n",
    "for exploration_tech in [\"soft\" ,\"epsilon\", \"boltzman\"]:\n",
    "\n",
    "    project_name = \"keras_tunning_\"+exploration_tech\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/A3C_\"+exploration_tech+\"/\", exploration_tech =exploration_tech, \n",
    "                          end_of_episode = EPISODES, n_enviroment = 5, \n",
    "                  evaluation_epoch = 2000, training_steps = 600000,\n",
    "                  sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                  discount_min = 0.90, discount_max = 0.99,\n",
    "                  entropy_min = 0.0001, entropy_max = 0.05,\n",
    "                  lr_min = 0.00001, lr_max = 0.005,\n",
    "                  dense_min = 32, dense_max = 500,\n",
    "                  lstm_min = 32, lstm_max = 128,\n",
    "                  time_to_update_min = 100, time_to_update_max=600,\n",
    "                  environment_name=ENV),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 40,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "\n",
    "    for trials in tuner.oracle.get_best_trials(num_trials=1):\n",
    "        print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "        \n",
    "\n",
    "    env_model = tuner.get_best_models()[0]\n",
    "    final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./A3C_\"+exploration_tech+\"_video.mp4\")\n",
    "    print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e97097829de60fff0360ae9237ac6de2911bdcd7f22e62fb944aef663864db87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
