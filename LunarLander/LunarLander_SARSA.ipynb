{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 16:00:32.770622: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Selected port: 47029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 16:00:34.510996: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:47029/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from SARSA_Agent import *\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENV = \"LunarLander-v2\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<LunarLander<LunarLander-v2>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(ENV)#,new_step_api=True\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.12129   , -0.8256469 ,  1.9506097 ,  1.41141   , -1.356025  ,\n",
       "        -0.08034442, -0.86012703, -0.81464326], dtype=float32),\n",
       " (8,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s, s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-inf, inf, (8,), float32), (8,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " Discrete(4),\n",
       " (array([ 0.00774088,  1.4172187 ,  0.3865181 ,  0.12703912, -0.00727374,\n",
       "         -0.05514999,  0.        ,  0.        ], dtype=float32),\n",
       "  1.110616457898858,\n",
       "  False,\n",
       "  {}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space.n, env.action_space, env.step(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_SARSA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_kt_SARSA/keras_tunning_epsilon/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_SARSA\"\n",
    "    exploration_tech = \"epsilon\"\n",
    "\n",
    "    project_name = \"keras_tunning_\"+exploration_tech\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/SARSA_\"+exploration_tech+\"/\", exploration_tech =exploration_tech,\n",
    "                        sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "                end_of_episode = EPISODES, batch = 32,  evaluation_epoch = 2000, \n",
    "                training_steps = 700000, \n",
    "                time_to_update_min = 200, time_to_update_max = 800,\n",
    "                lr_min = 0.000005, lr_max = 0.005,\n",
    "                discount_min = 0.90, discount_max = 0.99,\n",
    "                dense_min = 32, dense_max = 256,\n",
    "                environment_name = ENV),\n",
    "        objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "        max_trials = 50,\n",
    "        # distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "        directory= dir,\n",
    "        project_name=project_name\n",
    "    )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "\n",
    "    exploration_tech = \"boltzman\"\n",
    "\n",
    "    project_name = \"keras_tunning_\"+exploration_tech\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/SARSA_\"+exploration_tech+\"/\", exploration_tech =exploration_tech,\n",
    "                        sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "                end_of_episode = EPISODES, batch = 32,  evaluation_epoch = 2000, \n",
    "                training_steps = 700000, \n",
    "                time_to_update_min = 200, time_to_update_max = 800,\n",
    "                lr_min = 0.000005, lr_max = 0.005,\n",
    "                discount_min = 0.90, discount_max = 0.99,\n",
    "                dense_min = 32, dense_max = 256,\n",
    "                environment_name = ENV),\n",
    "        objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "        max_trials = 50,\n",
    "        # distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "        directory= dir,\n",
    "        project_name=project_name\n",
    "    )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "\n",
    "\n",
    "else : \n",
    "    print(\"Acquiring parameters ....\")\n",
    "    learning_rate = 0.1\n",
    "    training_steps = 5000000\n",
    "    exploration_tech = \"epsilon\"\n",
    "    discount = 0.99\n",
    "    time_to_update = 100\n",
    "    max_n_layers = 1\n",
    "    dense_units = [32]\n",
    "    end_of_episode = EPISODES\n",
    "    sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH\n",
    "    sucess_criteria_value = SUCESS_CRITERIA_VALUE\n",
    "    run_training(training_steps, learning_rate,  exploration_tech, discount, time_to_update, max_n_layers, dense_units, writer, end_of_episode, sucess_criteria_epochs, sucess_criteria_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_kt_SARSA/keras_tunning_boltzman/tuner0.json\n",
      "Trial id :24 | Score :-13.884199567491873 --> {'discount': 0.9500000000000001, 'learning_rate': 0.0017373768696360733, 'time_to_update': 400, 'n_dense_layers': 2, 'dense_units_0': 196, 'dense_units_1': 106}\n",
      "Trial id :22 | Score :-19.616048433071466 --> {'discount': 0.9400000000000001, 'learning_rate': 0.0014975550737667965, 'time_to_update': 400, 'n_dense_layers': 2, 'dense_units_0': 183, 'dense_units_1': 118}\n",
      "Trial id :43 | Score :-23.511146140800573 --> {'discount': 0.9, 'learning_rate': 0.00015485484197871015, 'time_to_update': 200, 'n_dense_layers': 2, 'dense_units_0': 117, 'dense_units_1': 122}\n",
      "Trial number :  50\n",
      "[<SARSA_Agent.SARSA_Agent_Optimization object at 0x7fb9e726f9d0>]\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).SARSA_agent.Qvalues.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).SARSA_agent.Qvalues.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).SARSA_agent.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).SARSA_agent.dense_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).SARSA_agent.dense_layers.1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).SARSA_agent.dense_layers.1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).SARSA_agent.Qvalues.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).SARSA_agent.Qvalues.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).SARSA_agent.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).SARSA_agent.dense_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).SARSA_agent.dense_layers.1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).SARSA_agent.dense_layers.1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).SARSA_agent.Qvalues.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).SARSA_agent.Qvalues.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).SARSA_agent.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).SARSA_agent.dense_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).SARSA_agent.dense_layers.1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).SARSA_agent.dense_layers.1.bias\n",
      "Trial number :  50\n",
      "0.0059099197\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sarsa_agent\" \"                 f\"(type SARSAAgent).\n\nIn this `tf.Variable` creation, the initial value's shape ((106, 4)) is not compatible with the explicitly supplied `shape` argument ((32, 4)).\n\nCall arguments received by layer \"sarsa_agent\" \"                 f\"(type SARSAAgent):\n  • observations=tf.Tensor(shape=(1, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(tuner\u001b[38;5;241m.\u001b[39mget_best_models())\n\u001b[1;32m     27\u001b[0m env_model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m final_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_tries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexploration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexploration_tech\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mvideo_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./SARSA_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mexploration_tech\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_video.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal mean reward \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,exploration_tech,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(final_rewards))\n",
      "File \u001b[0;32m/media/n/New Disk/Artificial_Intelligence/Portfolio/RL_updated/LunarLander/../SARSA_Agent.py:479\u001b[0m, in \u001b[0;36mfinal_evaluation\u001b[0;34m(eval_model, eval_env, n_tries, exploration, video_name, sucess_criteria_epochs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: video\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[0;32m--> 479\u001b[0m current_qvalue \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSARSA_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m action,_ \u001b[38;5;241m=\u001b[39m eval_model\u001b[38;5;241m.\u001b[39msample_actions(current_qvalue,\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, exploration\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m\"\u001b[39m, inference\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    481\u001b[0m action \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/media/n/New Disk/Artificial_Intelligence/Portfolio/RL_updated/LunarLander/../SARSA_Agent.py:332\u001b[0m, in \u001b[0;36mSARSAAgent.call\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    330\u001b[0m x \u001b[38;5;241m=\u001b[39m observations\n\u001b[1;32m    331\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md0(x)\n\u001b[0;32m--> 332\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"sarsa_agent\" \"                 f\"(type SARSAAgent).\n\nIn this `tf.Variable` creation, the initial value's shape ((106, 4)) is not compatible with the explicitly supplied `shape` argument ((32, 4)).\n\nCall arguments received by layer \"sarsa_agent\" \"                 f\"(type SARSAAgent):\n  • observations=tf.Tensor(shape=(1, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "val_env = gym.make(ENV)\n",
    "\n",
    "for exploration_tech in [\"epsilon\", \"boltzman\"]:\n",
    "\n",
    "    project_name = \"keras_tunning_\"+exploration_tech\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/SARSA_\"+exploration_tech+\"/\", exploration_tech =exploration_tech,\n",
    "                        sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "                end_of_episode = EPISODES, batch = 32,  evaluation_epoch = 2000, \n",
    "                training_steps = 700000, \n",
    "                time_to_update_min = 200, time_to_update_max = 800,\n",
    "                lr_min = 0.000005, lr_max = 0.005,\n",
    "                discount_min = 0.90, discount_max = 0.99,\n",
    "                dense_min = 32, dense_max = 256,\n",
    "                environment_name = ENV),\n",
    "        objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "        max_trials = 50,\n",
    "        directory= dir,\n",
    "        project_name=project_name\n",
    "    )\n",
    "\n",
    "    for trials in tuner.oracle.get_best_trials(num_trials=3):\n",
    "        print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "        \n",
    "\n",
    "    print(tuner.get_best_models())\n",
    "    env_model = tuner.get_best_models()[0]\n",
    "    final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./SARSA_\"+exploration_tech+\"_video.mp4\")\n",
    "    print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e97097829de60fff0360ae9237ac6de2911bdcd7f22e62fb944aef663864db87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
