{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KungFuMaster_A3C_TF1.ipynb","provenance":[{"file_id":"13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl","timestamp":1578567519575},{"file_id":"1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI","timestamp":1578558966437}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5CF91STZRE__","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1596132797851,"user_tz":-60,"elapsed":15507,"user":{"displayName":"yaniel barbosa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeEwoPVbOEW9Pwf_Z5Q-LyxKX0GvQrJ-S-pfKh=s64","userId":"13057870575473273651"}},"outputId":"3d151f5d-12b9-435f-c258-e1097518dd6c"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","# Clear any logs from previous runs\n","!rm -rf ./graphs\n","\n","%tensorflow_version 2.x\n","\n","!apt-get update -y\n","!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install x11-utils\n","!apt install xvfb\n","\n","!dpkg --configure -a\n","!pip install tqdm\n","\n","\n","#http://fnl.es/a-quick-reference-for-working-with-tensorflow.html"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\r0% [Waiting for headers] [Connected to cloud.r-project.org (13.32.87.34)] [Wait\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n","Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n","Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n","Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,033 kB]\n","Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,555 B]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [100 kB]\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [880 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Ign:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n","Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [232 kB]\n","0% [11 Release.gpg gpgv 564 B] [18 Packages 0 B/232 kB 0%]^C\n","^C\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  libxxf86dga1\n","Suggested packages:\n","  mesa-utils\n","The following NEW packages will be installed:\n","  libxxf86dga1 x11-utils\n","0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 209 kB of archives.\n","After this operation, 711 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n","21% [2 x11-utils 14.2 kB/196 kB 7%]^C\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-440\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,266 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n","Fetched 784 kB in 1s (1,081 kB/s)\n","debconf: apt-extracttemplates failed: No such file or directory\n","E: Sub-process /usr/sbin/dpkg-preconfigure --apt || true received signal 2.\n","E: Failure running script /usr/sbin/dpkg-preconfigure --apt || true\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-722ee340bd5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'apt install xvfb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dpkg --configure -a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tqdm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"mO9OTlED7Mjy","colab_type":"code","colab":{}},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    #%tensorflow_version 1.xe\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","        !touch .setup_complete\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbIutVUTrpuI","colab_type":"code","colab":{}},"source":["import time\n","import cv2\n","import numpy as np\n","from gym.core import Wrapper\n","from gym.spaces.box import Box\n","\n","class PreprocessAtari(Wrapper):\n","  def __init__(self, env, height=42, width=42, color=False,\n","              crop=lambda img: img, n_frames=4, reward_scale=1):\n","    \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n","    super(PreprocessAtari, self).__init__(env)\n","    self.img_size = (height, width)\n","    self.crop = crop\n","    self.color = color\n","\n","    self.reward_scale = reward_scale\n","    n_channels = (3 * n_frames) if color else n_frames\n","\n","    obs_shape =  (height, width, n_channels)\n","\n","    self.observation_space = Box(0.0, 1.0, obs_shape)\n","    self.framebuffer = np.zeros(obs_shape, 'float32')\n","\n","  def reset(self):\n","    \"\"\"Resets the game, returns initial frames\"\"\"\n","    self.framebuffer = np.zeros_like(self.framebuffer)\n","    self.update_buffer(self.env.reset()) # #Environment returns new_image, r, done, info\n","    return self.framebuffer\n","\n","  def step(self, action):\n","    \"\"\"Plays the game for 1 step, returns frame buffer\"\"\"\n","    new_img, r, done, info = self.env.step(action)\n","    self.update_buffer(new_img)\n","\n","    return self.framebuffer, r * self.reward_scale, done, info\n","\n","  ### image processing ###\n","\n","  def update_buffer(self, img):\n","    img = self.preproc_image(img)\n","    offset = 3 if self.color else 1\n","    axis = -1\n","    cropped_framebuffer = self.framebuffer[:, :, :-offset]\n","    self.framebuffer = np.concatenate([img, cropped_framebuffer], axis=axis)\n","\n","  def preproc_image(self, img):\n","    \"\"\"what happens to the observation\"\"\"\n","    img = self.crop(img)\n","    img = cv2.resize(img / 255, self.img_size, interpolation=cv2.INTER_LINEAR)\n","    if not self.color:\n","        img = img.mean(-1, keepdims=True)\n","    return img\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfiseaz6hyLv","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import gym\n","from IPython import display as ipythondisplay\n","from IPython.display import clear_output\n","from pyvirtualdisplay import Display\n","\n","import tensorflow.compat.v1 as tf\n","from collections import deque\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Conv2D, Dense, Dropout, Input , Concatenate,Flatten,ConvLSTM2D,LSTM,Reshape , BatchNormalization,TimeDistributed\n","from tensorflow.keras.optimizers import Adam\n","import random\n","import tqdm\n","import pandas as pd\n","\n","tf.disable_eager_execution()\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import math\n","\n","!pip install -q tf-agents\n","from tf_agents.utils import  value_ops"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8ob-ht5eQxp","colab_type":"code","colab":{}},"source":["\n","\n","def make_env():\n","  env = gym.make(\"KungFuMasterDeterministic-v0\")\n","  env = PreprocessAtari(\n","      env, height=42, width=42,\n","      crop=lambda img: img[60:-30, 5:],\n","      color=False, n_frames=4)\n","  return env\n","\n","env = make_env()\n","\n","obs_shape = env.observation_space.shape\n","n_actions = env.action_space.n\n","\n","print(\"Observation shape:\", obs_shape)\n","print(\"Num actions:\", n_actions)\n","print(\"Action names:\", env.env.env.get_action_meanings())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nrn-RQKxygNJ","colab_type":"code","colab":{}},"source":["class Agent:\n","  def __init__(self, name, state_shape, n_actions,sess, isConvLSTM,  reuse=False):\n","    with tf.variable_scope(name, reuse=reuse):\n","\n","      inputs = Input(shape=state_shape)\n","      x = Conv2D(32, (3, 3), strides=2, activation='relu')(inputs)\n","      x = Conv2D(32, (3, 3), strides=2, activation='relu')(x)\n","      x = Conv2D(32, (3, 3), strides=2, activation='relu')(x)\n","      x = Flatten()(x)\n","      if (isConvLSTM == False):\n","        x = Dense(128, activation='relu')(x)\n","      else:\n","        rnn_in = Reshape((1 ,x.shape[1]), input_shape=x.shape)(x)\n","        x = LSTM(128, return_sequences=False, name =\"lstm_enc_1\")(rnn_in)\n","\n","      logits = Dense(n_actions, activation='linear')(x)\n","      state_value = Dense(1, activation='linear')(x)\n","\n","      self.network = Model(inputs=inputs, outputs=[logits, state_value])\n","\n","      # prepare a graph for agent step\n","      self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n","      self.agent_outputs = self.symbolic_step(self.state_t)\n","    \n","  def symbolic_step(self, state_t):\n","    l,s, = self.network(state_t)\n","\n","    logits = l\n","    state_values = s[:,0]\n","\n","    return logits, state_values\n","  \n","  def sample_actions(self, state_t):\n","    \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n","    \n","    logits,_= sess.run(self.agent_outputs, {self.state_t: state_t})\n","    policy = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n","    selected_action =np.array([np.random.choice(len(p), p=p) for p in policy])\n","    return selected_action\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFNaZVEWYTj-","colab_type":"code","colab":{}},"source":["class KungFuMaster:\n","  def __init__(self, sess, env, agent, lstm,gae):\n","    \n","    self.filepath = \"/content/drive/My Drive/Colab Notebooks/RL/KungFuMasterDeterministic/data.txt\"\n","    self.batch_size =10\n","    self.memory = deque(maxlen= self.batch_size)\n","    self.nEnviroment=10\n","    self.Training= 200000\n","    self.learning_rate= 1e-4\n","    self.entropy_factor = 0.001\n","\n","    self.rewards_history = []\n","    self.entropy_history = []\n","    self.actor_history =[]\n","    self.critic_history =[]\n","    self.advantage_history = []\n","    self.target_history =[]\n","    self.v_history =[]\n","    self.actionList =[]\n","    self.game_reward = 0\n","    \n","    self.is_gae = gae\n","    self.isConvLSTM = lstm \n","    self.env = env\n","    self.obs_shape = self.env.observation_space.shape\n","    self.n_actions = self.env.action_space.n\n","\n","    print(\"Observation shape:\", self.obs_shape)\n","    print(\"Num actions:\", self.n_actions)\n","    self.reward_step =0\n","    self.writer = tf.summary.FileWriter('./graphs', sess.graph)\n","\n","    self.agent = agent\n","    self.states_ph = tf.placeholder('float32', [None,] + list(self.obs_shape))    \n","    self.next_states_ph = tf.placeholder('float32', [None,] + list(self.obs_shape))\n","    self.actions_ph = tf.placeholder('int32', (None,))\n","    self.rewards_ph = tf.placeholder('float32', (None,))\n","    self.is_done_ph = tf.placeholder('float32', (None,))\n","\n","    # logits[n_envs, n_actions] and state_values[n_envs, n_actions]\n","    self.logits, self.state_values = self.agent.symbolic_step(self.states_ph)\n","    self.next_logits, self.next_state_values = self.agent.symbolic_step(self.next_states_ph)\n","\n","    # There is no next state if the episode is done!\n","    self.next_state_values = self.next_state_values * (1 - self.is_done_ph)\n","\n","    # probabilities and log-probabilities for all actions\n","    self.probs = tf.nn.softmax(self.logits, axis=-1)            # [n_envs, n_actions]\n","    self.logprobs = tf.nn.log_softmax(self.logits, axis=-1)     # [n_envs, n_actions]\n","\n","    # log-probabilities only for agent's chosen actions\n","    self.logp_actions = tf.reduce_sum(self.logprobs * tf.one_hot(self.actions_ph, self.n_actions), axis=-1) # [n_envs,]\n","\n","    self.gamma = 0.99\n","    self.advantage = self.rewards_ph + self.gamma*self.next_state_values - self.state_values\n","    self.entropy = -tf.reduce_sum(self.probs * self.logprobs, 1, name=\"entropy\")\n","    self.target_state_values = self.rewards_ph+self.gamma*self.next_state_values\n","\n","    self.actor_loss = -tf.reduce_mean(self.logp_actions * tf.stop_gradient(self.advantage), axis=0) - self.entropy_factor * tf.reduce_mean(self.entropy, axis=0)\n","    self.critic_loss = tf.reduce_mean((self.state_values - tf.stop_gradient(self.target_state_values))**2, axis=0)\n","    self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.actor_loss + self.critic_loss)\n","\n","    self.summary_adv = tf.summary.scalar('advantage_', tf.reduce_mean(self.advantage, axis=-1))\n","    self.summary_ent = tf.summary.scalar('entropy_',tf.reduce_mean(self.entropy, axis=-1))\n","    self.summary_target = tf.summary.scalar('target_state_values_',tf.reduce_mean(self.target_state_values, axis=-1))\n","    self.summary_act_loss = tf.summary.scalar('actor_loss_', self.actor_loss)\n","    self.summary_crit_loss = tf.summary.scalar('critic_loss_', self.critic_loss)\n","    self.summary_batchreward = tf.summary.scalar('batchreward', tf.reduce_mean(self.rewards_ph, axis=-1))\n","    self.summary_v = tf.summary.scalar('state_values', tf.reduce_mean(self.state_values, axis=-1))\n","    self.summary_r = tf.summary.scalar('mean_reward', np.mean(self.game_reward))\n","\n","  def evaluate(self, n_games=1):\n","      self.game_rewards = []\n","      for _ in range(n_games):\n","          state = self.env.reset()\n","\n","          total_reward = 0\n","          while True:\n","              action = self.agent.sample_actions([state])[0]\n","              state, reward, done, info = self.env.step(action)\n","              total_reward += reward\n","              if done:\n","                  break\n","\n","          self.game_rewards.append(total_reward)\n","      \n","      self.game_reward = np.mean(self.game_rewards)\n","      self.writer.add_summary(sess.run(self.summary_r),self.reward_step)\n","      self.reward_step +=1\n","\n","      return self.game_rewards\n","\n","\n","  def train(self,batch_states,batch_actions,batch_next_states,batch_rewards,batch_done,step):\n","\n","    feed_dict = {\n","            self.states_ph: batch_states,\n","            self.actions_ph: batch_actions,\n","            self.next_states_ph: batch_next_states,\n","            self.rewards_ph: batch_rewards,\n","            self.is_done_ph: batch_done,\n","        }\n","\n","    _, ent_t,act,crit,adv,targ,v,sum_adv,sum_ent,sum_target,sum_act_loss,sum_crit_loss,sum_batchreward,sum_v = sess.run([self.train_step, self.entropy,self.actor_loss ,self.critic_loss,self.advantage,\n","                                             self.target_state_values,self.state_values,\n","                                             self.summary_adv,self.summary_ent,self.summary_target,self.summary_act_loss,self.summary_crit_loss,\n","                                             self.summary_batchreward,self.summary_v], feed_dict)\n","\n","    self.writer.add_summary(sum_adv, step)\n","    self.writer.add_summary(sum_ent, step)\n","    self.writer.add_summary(sum_target, step)\n","    self.writer.add_summary(sum_act_loss, step)\n","    self.writer.add_summary(sum_crit_loss, step)\n","    self.writer.add_summary(sum_batchreward, step)\n","    self.writer.add_summary(sum_v, step)\n","      \n","      \n","    self.entropy_history.append(np.mean(ent_t))\n","    self.actor_history.append(np.mean(act))\n","    self.critic_history.append(np.mean(crit))\n","    self.advantage_history.append(np.mean(adv))\n","    self.target_history.append(np.mean(targ))\n","    self.v_history.append(np.mean(v))\n","    for a in batch_actions:\n","      self.actionList.append(a)\n","\n","  def ewma(self,x, span=100):\n","    return pd.DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n","\n","  def plots(self):\n","    ipythondisplay.clear_output(wait=True)\n","    plt.figure(figsize=[35,13])\n","    plt.subplot(2,4,1)\n","    plt.plot(self.rewards_history, label='reward')\n","    plt.plot(self.ewma(np.array(self.rewards_history), span=10), marker='.', label='rewards ewma@10')\n","    plt.title(\"Session rewards\"); plt.grid(); plt.legend()\n","\n","    plt.subplot(2,4,2)\n","    plt.plot(self.actor_history, label='Actor Loss')\n","    plt.plot(self.ewma(np.array(self.actor_history), span=10), marker='.', label='Actor Loss ewma@10')\n","    plt.title(\"Policy Function\"); plt.grid(); plt.legend()  \n","\n","    plt.subplot(2,4,3)\n","    plt.plot(self.entropy_history, label='Entropy')\n","    plt.plot(self.ewma(np.array(self.entropy_history), span=10), marker='.', label='Entropy ewma@10')\n","    plt.title(\"Policy Function\"); plt.grid(); plt.legend()      \n","    \n","    plt.subplot(2,4,4)\n","    plt.plot(self.target_history, label='Target ')\n","    plt.plot(self.v_history, label='V')\n","    plt.title(\"Target/V\"); plt.grid(); plt.legend()\n","\n","    plt.subplot(2,4,5)\n","    plt.plot(self.advantage_history, label='Advantage')\n","    plt.title(\"Advantage\"); plt.grid(); plt.legend()  \n","\n","    plt.subplot(2,4,6)\n","    plt.plot(self.critic_history, label='Critic Loss')\n","    plt.plot(self.ewma(np.array(self.critic_history), span=10), marker='.', label='Critic Loss ewma@10')\n","    plt.title(\"Value function\"); plt.grid(); plt.legend()   \n","\n","    plt.subplot(2,4,7)\n","    plt.plot(self.actionList, label='Actions')\n","    plt.title(\"Actions\"); plt.grid(); plt.legend()  \n","\n","    plt.show()\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDOML1tgNu8Y","colab_type":"code","colab":{}},"source":["class EnvBatch:\n","  def __init__(self, n_envs = 10):\n","    \"\"\" Creates n_envs environments and babysits them for ya' \"\"\"\n","    self.envs = [make_env() for _ in range(n_envs)]\n","    \n","  def reset(self):\n","    \"\"\" Reset all games and return [n_envs, *obs_shape] observations \"\"\"\n","    return np.array([env.reset() for env in self.envs])\n","  \n","  def step(self, actions):\n","    \"\"\"\n","    Send a vector[batch_size] of actions into respective environments\n","    :returns: observations[n_envs, *obs_shape], rewards[n_envs], done[n_envs,], info[n_envs]\n","    \"\"\"\n","\n","    results = [env.step(a) for env, a in zip(self.envs, actions)]\n","    new_obs, rewards, done, infos = map(np.array, zip(*results))\n","    \n","    # reset environments automatically\n","    for i in range(len(self.envs)):\n","        if done[i]:\n","            new_obs[i] = self.envs[i].reset()\n","    \n","    return new_obs, rewards, done, infos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nc8Dm79YIOIA","colab_type":"code","colab":{}},"source":["sess = tf.Session(graph= tf.get_default_graph())\n","RESET = 1\n","is_gae = False\n","isConvLSTM = True "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fwa8ZW7be0yD","colab_type":"code","colab":{}},"source":["display = Display(visible=0, size=(400, 300))\n","display.start()\n","agent = Agent(\"network\",env.observation_space.shape, env.action_space.n,sess, isConvLSTM)\n","kungFu = KungFuMaster(sess,env,agent, isConvLSTM,is_gae)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_-jYFzAe1tD","colab_type":"code","colab":{}},"source":["\n","checkpoint_directory = \"/content/drive/My Drive/Colab Notebooks/RL/KungFuMasterDeterministic/\"\n","\n","if (RESET == 1):\n","  # ************ RESET everything **************************\n","  f=open(kungFu.filepath, \"w\")\n","  f.write(\"\")\n","  f.close()\n","  sess.run(tf.global_variables_initializer())\n","  saver = tf.train.Saver()\n","\n","else:\n","  # ************ LOAD everything **************************\n","  saver = tf.train.Saver()\n","  saver.restore(sess, tf.train.latest_checkpoint(checkpoint_directory))\n","\n","  f=open(kungFu.filepath,'r')\n","  for line in f:\n","    kungFu.rewards_history.append(float(line.split()[0]))\n","    kungFu.actor_history.append(float(line.split()[1]))\n","    kungFu.entropy_history.append(float(line.split()[2]))\n","    kungFu.target_history.append(float(line.split()[3]))\n","    kungFu.v_history.append(float(line.split()[4]))\n","    kungFu.advantage_history.append(float(line.split()[5]))\n","    kungFu.critic_history.append(float(line.split()[6]))\n","  f.close()\n","  kungFu.plots()\n","\n","\n","#https://itnext.io/how-to-use-tensorboard-5d82f8654496"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ucyi8_que4oW","colab_type":"code","colab":{}},"source":["# Validate\n","rewards = kungFu.evaluate(n_games=3)\n","print(rewards)\n","\n","env_batch = EnvBatch(10)\n","batch_states = env_batch.reset()\n","\n","batch_actions = kungFu.agent.sample_actions(batch_states)\n","batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n","\n","print(\"State shape:\", batch_states.shape)\n","print(\"Actions:\", batch_actions)\n","print(\"Rewards:\", batch_rewards)\n","print(\"Done:\", batch_done)\n","\n","#Initialize\n","env_batch = EnvBatch(10)\n","batch_states = env_batch.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5khtSEEsbs_","colab_type":"code","colab":{}},"source":["  %tensorboard --logdir graphs\n","  time.sleep(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9ebUmaSsboG","colab_type":"code","colab":{}},"source":["f=open(kungFu.filepath, \"w\")\n","with tqdm.trange(len(kungFu.entropy_history), kungFu.Training) as t:\n","  for i in t:\n","    batch_actions = kungFu.agent.sample_actions(batch_states)\n","    batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n","\n","    batch_rewards = batch_rewards * 0.01\n","    kungFu.train(batch_states,batch_actions,batch_next_states,batch_rewards,batch_done,i)\n","    batch_states = batch_next_states\n","    \n","    if i % 500 == 0:\n","\n","      if i % 2500 == 0:\n","        saver.save(sess,checkpoint_directory)\n","        kungFu.rewards_history.append(np.mean(kungFu.evaluate(n_games=3)))\n","        if kungFu.rewards_history[-1] >= 5000:\n","          print(\"Your agent has earned the yellow belt\")\n","\n","        f.write(\"%f %f %f %f %f %f %f\\n\" % (kungFu.rewards_history[-1],kungFu.actor_history[-1], kungFu.entropy_history[-1],\n","                                          kungFu.target_history[-1],kungFu.v_history[-1],kungFu.advantage_history[-1],kungFu.critic_history[-1]))\n","        \n","        if len(kungFu.rewards_history)> 100 :\n","          if np.mean(kungFu.rewards_history[-100:]) >= 200:\n","            print(\"Your agent reached the objetive\")\n","            break\n","\n","      if i %10000 ==0:\n","        f.close()\n","        f=open(kungFu.filepath, \"a+\")\n","      kungFu.plots()\n","      \n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqZPqueR2aKT","colab_type":"code","colab":{}},"source":["#https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks\n","#https://github.com/tensorflow/tensorboard/issues/3186  -  went to chrome://settings/content/cookies and had to unable the \"block indirect cookies\" setting"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmD2WJw72iMr","colab_type":"code","colab":{}},"source":["#https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/hyperparameter_tuning_with_hparams.ipynb#scrollTo=mVtYvbbIWRkV\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bV0fxF6Ehv1_","colab_type":"code","colab":{}},"source":["display = Display(visible=0, size=(400, 300))\n","display.start()\n","\n","\n","def evaluate2(agent, env, n_games=1):\n","  \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n","  game_rewards = []\n","  for _ in range(n_games):\n","      state = env.reset()\n","\n","      total_reward = 0\n","      while True:\n","          action = agent.sample_actions([state])[0]\n","          state, reward, done, info = env.step(action)\n","          total_reward += reward\n","          if done:\n","              break\n","\n","      game_rewards.append(total_reward)\n","  return game_rewards\n","\n","env_monitor = gym.wrappers.Monitor(env, directory=\"/content/drive/My Drive/Colab Notebooks/RL/KungFuMasterDeterministic/videos\", force=True)\n","final_rewards = evaluate2(agent, env_monitor, n_games=10)\n","env_monitor.close()\n","\n","print(\"Final mean reward:\", np.mean(final_rewards))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3W86uZW38rPS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tCVxe2DYm2uo","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVo1vJIjm-Mq","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}