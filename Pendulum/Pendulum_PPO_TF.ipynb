{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 14:14:27.686999: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_tf import *\n",
    "from ENV_DETAILS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\Pendulum\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/Pendulum/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_a3c/' # Linux\n",
    "\n",
    "ENV = \"Pendulum-v1\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PendulumEnv<Pendulum-v1>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09696076,  0.5834242 , -1.089912  ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1. -1. -8.], [1. 1. 8.], (3,), float32), 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6571847"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-2.0, 2.0, (1,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-2.0, 2.0, (1,), float32),\n",
       " Box(-2.0, 2.0, (1,), float32),\n",
       " (array([ 0.551725  , -0.8340261 , -0.09397238], dtype=float32),\n",
       "  -0.9838920097072988,\n",
       "  False,\n",
       "  {}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space, env.action_space, env.step(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing keras model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring parameters ....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.4172543 , -0.90878975, -0.46718445], dtype=float32), False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Acquiring parameters ....\")\n",
    "writer= \"Training/fit_ppo/\"\n",
    "\n",
    "entropy_factor = 0.05\n",
    "discount = 0.99\n",
    "dense_units_actor = [256,256]\n",
    "num_layers_actor = 2\n",
    "dense_units_critic = [256,256]\n",
    "num_layers_crit =2\n",
    "\n",
    "model = get_model( discount, dense_units_actor,  dense_units_critic, num_layers_actor, num_layers_crit, writer, \n",
    "                environment_name = ENV,reward_scaler = 1, lr_actor= 0.00001, lr_critic= 0.00001, \n",
    "                gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 50, memory_size= 50)\n",
    "                 \n",
    "obs = model.train_env.reset()\n",
    "model.call([1, obs] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.36029434, -0.9328387 , -1.2367705 ], dtype=float32), False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([1, obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial number :  1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "1/1 [==============================] - 1s 760ms/step - actor_loss: 18957.3047 - total_train_reward: -253.7482\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 662ms/step - actor_loss: 17722.7832 - total_train_reward: -241.8282\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 683ms/step - actor_loss: 34918.6445 - total_train_reward: -337.9176\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 646ms/step - actor_loss: 37837.6641 - total_train_reward: -351.3461\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 649ms/step - actor_loss: 57769.2852 - total_train_reward: -431.8658\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 686ms/step - actor_loss: 18305.0195 - total_train_reward: -241.8495\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 680ms/step - actor_loss: 58147.6797 - total_train_reward: -428.5230\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 653ms/step - actor_loss: 18800.6602 - total_train_reward: -243.1319\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 640ms/step - actor_loss: 18299.5781 - total_train_reward: -242.2218\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 669ms/step - actor_loss: 18715.1016 - total_train_reward: -243.2010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4500515a50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model( discount, dense_units_actor,  dense_units_critic, num_layers_actor, num_layers_crit, writer, \n",
    "                environment_name = ENV,reward_scaler = 1, lr_actor= 0.00001, lr_critic= 0.00001, \n",
    "                gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 50, memory_size= 50)\n",
    "\n",
    "model.compile()\n",
    "model.fit([-1], [-1], epochs= 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs_general/hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_PPO_tf/\"\n",
    "\n",
    "dir = r\"Hyperparam_kt_ppo\"\n",
    "project_name = \"keras_tunning_ppo_tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \n",
    "\n",
    "    max_layers_act = 3\n",
    "    num_layers_crit= 3\n",
    "    training_epoch = 50\n",
    "    memory_size = env._max_episode_steps\n",
    "    \n",
    "    discount =  hp.Float('discount',0.85, 0.99, step =0.01)\n",
    "    gae_lambda =  hp.Float('gae_lambda',0.85, 0.97, step =0.01)\n",
    "    lr_actor = hp.Float('lr_actor', 0.000001, 0.005)\n",
    "    lr_critic = hp.Float('lr_critic', 0.000001, 0.005)\n",
    "    policy_clip = hp.Float('policy_clip', 0.1, 0.3, step =0.1)\n",
    "    entropy_coeff = hp.Float('entropy_coeff', 0.001, 0.1)\n",
    "\n",
    "    if max_layers_act>1 :\n",
    "        num_layer_a = hp.Int('n_dense_layers_actor', 1, max_layers_act)\n",
    "    else : \n",
    "        num_layer_a = max_layers_act\n",
    "    dense_units_act =  [hp.Int('dense_units_act_'+str(i), 32, 512) for i in range(num_layer_a)]\n",
    "\n",
    "    if num_layers_crit>1 :\n",
    "        num_layer_c = hp.Int('n_dense_layers_critic', 1, num_layers_crit)\n",
    "    else : \n",
    "        num_layer_c = num_layers_crit\n",
    "\n",
    "    dense_units_crit =  [hp.Int('dense_units_crit_'+str(i), 32, 512) for i in range(num_layers_crit)]\n",
    "\n",
    "    model = PPO(\n",
    "        discount = discount, \n",
    "        dense_units_act = dense_units_act,\n",
    "        dense_units_crit= dense_units_crit, \n",
    "        num_layer_act  = num_layer_a, \n",
    "        num_layer_crit= num_layer_c,\n",
    "        writer = \"logs_hyper/ppo_tf/\",\n",
    "        trial_n = get_valid_trials_number(dir+\"/\"+project_name ),\n",
    "        evaluation_epoch = env._max_episode_steps,\n",
    "        environment_name = ENV,\n",
    "        reward_scaler = 1,\n",
    "        lr_actor = lr_actor, lr_critic = lr_critic, \n",
    "        gae_lambda=gae_lambda,\n",
    "        policy_clip = policy_clip,\n",
    "        training_epoch = training_epoch,\n",
    "        entropy_coeff = entropy_coeff,\n",
    "        memory_size = memory_size\n",
    "        )\n",
    "    \n",
    "    model.compile()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [05h 04m 10s]\n",
      "total_train_reward: -610.8777259528645\n",
      "\n",
      "Best total_train_reward So Far: -610.740117344646\n",
      "Total elapsed time: 14h 53m 40s\n",
      "\n",
      "Search: Running Trial #4\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.98              |0.94              |discount\n",
      "0.86              |0.95              |gae_lambda\n",
      "0.0021516         |0.0018657         |lr_actor\n",
      "0.00040776        |0.001466          |lr_critic\n",
      "0.2               |0.1               |policy_clip\n",
      "0.022371          |0.055969          |entropy_coeff\n",
      "2                 |2                 |n_dense_layers_actor\n",
      "159               |244               |dense_units_act_0\n",
      "2                 |2                 |n_dense_layers_critic\n",
      "484               |98                |dense_units_crit_0\n",
      "218               |90                |dense_units_crit_1\n",
      "457               |286               |dense_units_crit_2\n",
      "133               |32                |dense_units_act_1\n",
      "319               |None              |dense_units_act_2\n",
      "\n",
      "Trial number :  4\n",
      "Epoch 1/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 44953.3516 - total_train_reward: -1253.3897\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 60318.3203 - total_train_reward: -1463.1380\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 38154.2109 - total_train_reward: -1363.3920\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 24530.4590 - total_train_reward: -1169.4457\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5357.3301 - total_train_reward: -870.6243\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1023.8310 - total_train_reward: -1298.6439\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 26.7660 - total_train_reward: -1175.6229\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 22.6219 - total_train_reward: -1323.4691\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.0523 - total_train_reward: -1127.2344\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1040 - total_train_reward: -1079.1617\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.9677 - total_train_reward: -1130.0666\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1355 - total_train_reward: -1070.0305\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3688 - total_train_reward: -989.4733\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.7591 - total_train_reward: -916.9793\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 88.5207 - total_train_reward: -1729.3553\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.8952 - total_train_reward: -1066.9601\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.8941 - total_train_reward: -1115.3144\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -18.1567 - total_train_reward: -1535.7822\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.1134 - total_train_reward: -1105.8472\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.9367 - total_train_reward: -1337.1222\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.9633 - total_train_reward: -1465.0096\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4976 - total_train_reward: -1173.1159\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.8842 - total_train_reward: -1394.0945\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.2889 - total_train_reward: -1026.4426\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.1816 - total_train_reward: -910.9532\n",
      "Epoch 26/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.2284 - total_train_reward: -990.3585\n",
      "Epoch 27/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1406 - total_train_reward: -994.4850\n",
      "Epoch 28/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7950 - total_train_reward: -1139.0209\n",
      "Epoch 29/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.4264 - total_train_reward: -874.4932\n",
      "Epoch 30/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.1513 - total_train_reward: -970.0160\n",
      "Epoch 31/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0068 - total_train_reward: -887.1158\n",
      "Epoch 32/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2981 - total_train_reward: -1548.2960\n",
      "Epoch 33/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7913 - total_train_reward: -1834.8201\n",
      "Epoch 34/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3724 - total_train_reward: -1064.9797\n",
      "Epoch 35/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8588 - total_train_reward: -879.7473\n",
      "Epoch 36/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.8983 - total_train_reward: -1180.5466\n",
      "Epoch 37/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.8849 - total_train_reward: -1436.3602\n",
      "Epoch 38/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.4805 - total_train_reward: -1205.7105\n",
      "Epoch 39/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -15.8771 - total_train_reward: -1477.8498\n",
      "Epoch 40/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.0006 - total_train_reward: -1267.7995\n",
      "Epoch 41/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.7868 - total_train_reward: -1059.0227\n",
      "Epoch 42/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3821 - total_train_reward: -1388.9699\n",
      "Epoch 43/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.1874 - total_train_reward: -1272.3645\n",
      "Epoch 44/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.3879 - total_train_reward: -1695.8872\n",
      "Epoch 45/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.5693 - total_train_reward: -1698.5551\n",
      "Epoch 46/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9335 - total_train_reward: -901.2248\n",
      "Epoch 47/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.6001 - total_train_reward: -1850.1677\n",
      "Epoch 48/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9149 - total_train_reward: -898.6025\n",
      "Epoch 49/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.1747 - total_train_reward: -907.4433\n",
      "Epoch 50/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.0346 - total_train_reward: -1534.2468\n",
      "Epoch 51/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.2880 - total_train_reward: -1830.0435\n",
      "Epoch 52/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.4739 - total_train_reward: -1810.9456\n",
      "Epoch 53/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.7539 - total_train_reward: -1363.5973\n",
      "Epoch 54/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.3704 - total_train_reward: -1060.7175\n",
      "Epoch 55/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.1135 - total_train_reward: -1869.0152\n",
      "Epoch 56/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.4550 - total_train_reward: -1515.7222\n",
      "Epoch 57/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4265 - total_train_reward: -1459.7329\n",
      "Epoch 58/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.2370 - total_train_reward: -1759.2967\n",
      "Epoch 59/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9202 - total_train_reward: -1166.7923\n",
      "Epoch 60/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.3727 - total_train_reward: -990.7077\n",
      "Epoch 61/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.6422 - total_train_reward: -897.7011\n",
      "Epoch 62/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.0822 - total_train_reward: -965.9383\n",
      "Epoch 63/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -16.2772 - total_train_reward: -1657.6583\n",
      "Epoch 64/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.3203 - total_train_reward: -916.5458\n",
      "Epoch 65/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.6243 - total_train_reward: -834.6990\n",
      "Epoch 66/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.6315 - total_train_reward: -1167.0018\n",
      "Epoch 67/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.8387 - total_train_reward: -1291.5304\n",
      "Epoch 68/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1596 - total_train_reward: -1627.1148\n",
      "Epoch 69/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7474 - total_train_reward: -860.3371\n",
      "Epoch 70/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.5862 - total_train_reward: -1277.8064\n",
      "Epoch 71/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.6343 - total_train_reward: -1183.7030\n",
      "Epoch 72/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.1395 - total_train_reward: -960.5901\n",
      "Epoch 73/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6611 - total_train_reward: -1062.2145\n",
      "Epoch 74/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.8080 - total_train_reward: -1485.1252\n",
      "Epoch 75/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7364 - total_train_reward: -1155.6166\n",
      "Epoch 76/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.4996 - total_train_reward: -904.9260\n",
      "Epoch 77/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7267 - total_train_reward: -1173.9358\n",
      "Epoch 78/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.5722 - total_train_reward: -1415.4345\n",
      "Epoch 79/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.0399 - total_train_reward: -1046.5415\n",
      "Epoch 80/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1882 - total_train_reward: -1097.2335\n",
      "Epoch 81/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.3906 - total_train_reward: -1327.3663\n",
      "Epoch 82/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.5714 - total_train_reward: -896.1982\n",
      "Epoch 83/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2914 - total_train_reward: -991.8500\n",
      "Epoch 84/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.8327 - total_train_reward: -1232.3120\n",
      "Epoch 85/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9743 - total_train_reward: -1345.4700\n",
      "Epoch 86/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9567 - total_train_reward: -1054.4285\n",
      "Epoch 87/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4039 - total_train_reward: -1266.6707\n",
      "Epoch 88/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.4205 - total_train_reward: -910.8123\n",
      "Epoch 89/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1060 - total_train_reward: -983.5169\n",
      "Epoch 90/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2140 - total_train_reward: -1173.6457\n",
      "Epoch 91/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.6937 - total_train_reward: -1824.7185\n",
      "Epoch 92/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.0508 - total_train_reward: -1184.2890\n",
      "Epoch 93/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.1314 - total_train_reward: -1285.1703\n",
      "Epoch 94/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9079 - total_train_reward: -1176.8305\n",
      "Epoch 95/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.7614 - total_train_reward: -1203.9933\n",
      "Epoch 96/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.3033 - total_train_reward: -949.5465\n",
      "Epoch 97/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4021 - total_train_reward: -968.7414\n",
      "Epoch 98/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6568 - total_train_reward: -1387.3317\n",
      "Epoch 99/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.1169 - total_train_reward: -899.9269\n",
      "Epoch 100/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6426 - total_train_reward: -1262.8204\n",
      "Epoch 101/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.4028 - total_train_reward: -1463.2257\n",
      "Epoch 102/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.4603 - total_train_reward: -1615.1160\n",
      "Epoch 103/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7273 - total_train_reward: -873.6115\n",
      "Epoch 104/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.6670 - total_train_reward: -1274.0362\n",
      "Epoch 105/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.0500 - total_train_reward: -1012.7056\n",
      "Epoch 106/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.5247 - total_train_reward: -1810.3336\n",
      "Epoch 107/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.4109 - total_train_reward: -1179.6658\n",
      "Epoch 108/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2644 - total_train_reward: -1036.3834\n",
      "Epoch 109/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.4808 - total_train_reward: -1651.9066\n",
      "Epoch 110/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.8191 - total_train_reward: -1557.8678\n",
      "Epoch 111/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9372 - total_train_reward: -1276.2628\n",
      "Epoch 112/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.0940 - total_train_reward: -743.5580\n",
      "Epoch 113/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.7138 - total_train_reward: -891.9404\n",
      "Epoch 114/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.7873 - total_train_reward: -1166.6953\n",
      "Epoch 115/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0105 - total_train_reward: -861.7622\n",
      "Epoch 116/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7924 - total_train_reward: -862.3289\n",
      "Epoch 117/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3070 - total_train_reward: -1204.6160\n",
      "Epoch 118/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.3032 - total_train_reward: -1069.1274\n",
      "Epoch 119/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6582 - total_train_reward: -1864.2201\n",
      "Epoch 120/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4802 - total_train_reward: -1358.6525\n",
      "Epoch 121/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7394 - total_train_reward: -1482.0165\n",
      "Epoch 122/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.1670 - total_train_reward: -1092.3480\n",
      "Epoch 123/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.5354 - total_train_reward: -1499.7233\n",
      "Epoch 124/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4421 - total_train_reward: -1329.0341\n",
      "Epoch 125/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1478 - total_train_reward: -861.6946\n",
      "Epoch 126/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.8850 - total_train_reward: -1752.8495\n",
      "Epoch 127/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.7528 - total_train_reward: -1231.7195\n",
      "Epoch 128/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.1904 - total_train_reward: -1055.1761\n",
      "Epoch 129/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9203 - total_train_reward: -1008.4597\n",
      "Epoch 130/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2809 - total_train_reward: -1709.2871\n",
      "Epoch 131/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.7146 - total_train_reward: -895.4132\n",
      "Epoch 132/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.4809 - total_train_reward: -868.0931\n",
      "Epoch 133/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9679 - total_train_reward: -963.7551\n",
      "Epoch 134/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3988 - total_train_reward: -1164.2663\n",
      "Epoch 135/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.2874 - total_train_reward: -1853.7772\n",
      "Epoch 136/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.5805 - total_train_reward: -864.7140\n",
      "Epoch 137/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2338 - total_train_reward: -874.8284\n",
      "Epoch 138/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9024 - total_train_reward: -1717.7574\n",
      "Epoch 139/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.5881 - total_train_reward: -1287.5004\n",
      "Epoch 140/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.1733 - total_train_reward: -1390.0677\n",
      "Epoch 141/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.8985 - total_train_reward: -967.8444\n",
      "Epoch 142/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6079 - total_train_reward: -1171.6116\n",
      "Epoch 143/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.0471 - total_train_reward: -1589.9610\n",
      "Epoch 144/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.9270 - total_train_reward: -1306.5242\n",
      "Epoch 145/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9194 - total_train_reward: -1423.8647\n",
      "Epoch 146/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.7695 - total_train_reward: -862.3678\n",
      "Epoch 147/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8626 - total_train_reward: -1769.6328\n",
      "Epoch 148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.7638 - total_train_reward: -1677.3630\n",
      "Epoch 149/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.3913 - total_train_reward: -1674.7389\n",
      "Epoch 150/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.7824 - total_train_reward: -1249.7653\n",
      "Epoch 151/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1711 - total_train_reward: -826.0374\n",
      "Epoch 152/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.3125 - total_train_reward: -1831.9832\n",
      "Epoch 153/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8480 - total_train_reward: -967.2704\n",
      "Epoch 154/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0416 - total_train_reward: -1206.9766\n",
      "Epoch 155/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4523 - total_train_reward: -1547.9562\n",
      "Epoch 156/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.5767 - total_train_reward: -1572.2202\n",
      "Epoch 157/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7197 - total_train_reward: -623.9312\n",
      "Epoch 158/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.0230 - total_train_reward: -615.4630\n",
      "Epoch 159/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.9845 - total_train_reward: -1320.6193\n",
      "Epoch 160/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.2209 - total_train_reward: -1525.6246\n",
      "Epoch 161/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.4895 - total_train_reward: -1889.1448\n",
      "Epoch 162/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1825 - total_train_reward: -1109.7093\n",
      "Epoch 163/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.0981 - total_train_reward: -776.8094\n",
      "Epoch 164/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.7483 - total_train_reward: -928.3125\n",
      "Epoch 165/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7053 - total_train_reward: -745.7659\n",
      "Epoch 166/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.1368 - total_train_reward: -767.4004\n",
      "Epoch 167/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.5489 - total_train_reward: -977.2290\n",
      "Epoch 168/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.1348 - total_train_reward: -915.6608\n",
      "Epoch 169/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.8179 - total_train_reward: -1071.0899\n",
      "Epoch 170/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.7245 - total_train_reward: -1403.6188\n",
      "Epoch 171/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.6693 - total_train_reward: -1782.0001\n",
      "Epoch 172/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.4806 - total_train_reward: -1245.8666\n",
      "Epoch 173/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6212 - total_train_reward: -1167.2850\n",
      "Epoch 174/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0100 - total_train_reward: -1167.6463\n",
      "Epoch 175/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.4314 - total_train_reward: -972.1992\n",
      "Epoch 176/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1967 - total_train_reward: -747.2882\n",
      "Epoch 177/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.8907 - total_train_reward: -1909.9642\n",
      "Epoch 178/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -16.2881 - total_train_reward: -1860.1915\n",
      "Epoch 179/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.0844 - total_train_reward: -1429.5667\n",
      "Epoch 180/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.8872 - total_train_reward: -1494.7412\n",
      "Epoch 181/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1122 - total_train_reward: -745.5887\n",
      "Epoch 182/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.5144 - total_train_reward: -1184.5811\n",
      "Epoch 183/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.2847 - total_train_reward: -1543.6283\n",
      "Epoch 184/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -15.2035 - total_train_reward: -1637.2579\n",
      "Epoch 185/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.2636 - total_train_reward: -1752.5043\n",
      "Epoch 186/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.2709 - total_train_reward: -1005.3460\n",
      "Epoch 187/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0133 - total_train_reward: -1308.3682\n",
      "Epoch 188/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.8062 - total_train_reward: -1613.7396\n",
      "Epoch 189/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.8896 - total_train_reward: -1037.4458\n",
      "Epoch 190/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.6821 - total_train_reward: -968.8192\n",
      "Epoch 191/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.7237 - total_train_reward: -1671.1054\n",
      "Epoch 192/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4672 - total_train_reward: -1492.7821\n",
      "Epoch 193/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4359 - total_train_reward: -1352.3687\n",
      "Epoch 194/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8384 - total_train_reward: -1746.3460\n",
      "Epoch 195/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9148 - total_train_reward: -1217.7625\n",
      "Epoch 196/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9395 - total_train_reward: -740.1017\n",
      "Epoch 197/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.3284 - total_train_reward: -1288.5131\n",
      "Epoch 198/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.4147 - total_train_reward: -1343.6121\n",
      "Epoch 199/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.2539 - total_train_reward: -1018.8175\n",
      "Epoch 200/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.6779 - total_train_reward: -992.8594\n",
      "Epoch 201/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0512 - total_train_reward: -1152.2119\n",
      "Epoch 202/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.5085 - total_train_reward: -1441.0287\n",
      "Epoch 203/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.8528 - total_train_reward: -1297.4554\n",
      "Epoch 204/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.6122 - total_train_reward: -1473.8123\n",
      "Epoch 205/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.2993 - total_train_reward: -730.0048\n",
      "Epoch 206/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.9171 - total_train_reward: -1017.2191\n",
      "Epoch 207/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0455 - total_train_reward: -1057.6549\n",
      "Epoch 208/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0345 - total_train_reward: -863.4662\n",
      "Epoch 209/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.7189 - total_train_reward: -865.7936\n",
      "Epoch 210/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.6462 - total_train_reward: -1304.3100\n",
      "Epoch 211/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.6523 - total_train_reward: -1637.7755\n",
      "Epoch 212/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4504 - total_train_reward: -1204.2418\n",
      "Epoch 213/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2706 - total_train_reward: -1274.5757\n",
      "Epoch 214/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4392 - total_train_reward: -1432.7423\n",
      "Epoch 215/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1527 - total_train_reward: -1231.7098\n",
      "Epoch 216/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2742 - total_train_reward: -1137.0980\n",
      "Epoch 217/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4006 - total_train_reward: -1450.9708\n",
      "Epoch 218/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.2341 - total_train_reward: -1793.2124\n",
      "Epoch 219/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.1615 - total_train_reward: -1259.1650\n",
      "Epoch 220/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.8358 - total_train_reward: -968.5351\n",
      "Epoch 221/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.9574 - total_train_reward: -1293.8219\n",
      "Epoch 222/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.0599 - total_train_reward: -1485.2703\n",
      "Epoch 223/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3165 - total_train_reward: -1298.3028\n",
      "Epoch 224/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9798 - total_train_reward: -740.4806\n",
      "Epoch 225/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.5234 - total_train_reward: -1468.1439\n",
      "Epoch 226/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8967 - total_train_reward: -1167.9725\n",
      "Epoch 227/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.2801 - total_train_reward: -1514.7435\n",
      "Epoch 228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7274 - total_train_reward: -1076.4380\n",
      "Epoch 229/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.8228 - total_train_reward: -1267.3523\n",
      "Epoch 230/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.7464 - total_train_reward: -1077.5822\n",
      "Epoch 231/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0415 - total_train_reward: -1070.6062\n",
      "Epoch 232/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.1653 - total_train_reward: -1660.4186\n",
      "Epoch 233/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.0826 - total_train_reward: -1447.0640\n",
      "Epoch 234/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.2293 - total_train_reward: -1050.7670\n",
      "Epoch 235/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6072 - total_train_reward: -1790.5835\n",
      "Epoch 236/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.3443 - total_train_reward: -1047.5992\n",
      "Epoch 237/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.9234 - total_train_reward: -1205.4870\n",
      "Epoch 238/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3308 - total_train_reward: -1172.0788\n",
      "Epoch 239/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.3075 - total_train_reward: -1077.0980\n",
      "Epoch 240/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.4512 - total_train_reward: -1369.0748\n",
      "Epoch 241/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.4464 - total_train_reward: -1238.9611\n",
      "Epoch 242/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.8480 - total_train_reward: -1052.8325\n",
      "Epoch 243/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3196 - total_train_reward: -1352.4633\n",
      "Epoch 244/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.8536 - total_train_reward: -1759.2267\n",
      "Epoch 245/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.8394 - total_train_reward: -1130.7198\n",
      "Epoch 246/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.1906 - total_train_reward: -1060.9562\n",
      "Epoch 247/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.5577 - total_train_reward: -1134.8270\n",
      "Epoch 248/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.6571 - total_train_reward: -968.0784\n",
      "Epoch 249/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 13.0170 - total_train_reward: -1262.0263\n",
      "Epoch 250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7885 - total_train_reward: -1067.7306\n",
      "Epoch 251/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3231 - total_train_reward: -1074.9587\n",
      "Epoch 252/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.6029 - total_train_reward: -1188.6388\n",
      "Epoch 253/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.5389 - total_train_reward: -1333.2883\n",
      "Epoch 254/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.5384 - total_train_reward: -1727.0502\n",
      "Epoch 255/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 9.5455 - total_train_reward: -1208.1672\n",
      "Epoch 256/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.9712 - total_train_reward: -1423.3161\n",
      "Epoch 257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8444 - total_train_reward: -1226.2384\n",
      "Epoch 258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8593 - total_train_reward: -961.4099\n",
      "Epoch 259/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.0346 - total_train_reward: -1289.7268\n",
      "Epoch 260/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9789 - total_train_reward: -1136.3712\n",
      "Epoch 261/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 9.2355 - total_train_reward: -1208.3994\n",
      "Epoch 262/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.3043 - total_train_reward: -1705.7134\n",
      "Epoch 263/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.2082 - total_train_reward: -1214.5565\n",
      "Epoch 264/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.3884 - total_train_reward: -1527.2381\n",
      "Epoch 265/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.7524 - total_train_reward: -1721.6884\n",
      "Epoch 266/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.0672 - total_train_reward: -1574.6949\n",
      "Epoch 267/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.7334 - total_train_reward: -1276.5234\n",
      "Epoch 268/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 23.4033 - total_train_reward: -949.0381\n",
      "Epoch 269/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 20.2740 - total_train_reward: -1757.7341\n",
      "Epoch 270/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 10.7245 - total_train_reward: -1298.2193\n",
      "Epoch 271/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.5781 - total_train_reward: -1565.2141\n",
      "Epoch 272/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.9001 - total_train_reward: -1268.9984\n",
      "Epoch 273/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 24.3442 - total_train_reward: -1380.1501\n",
      "Epoch 274/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.8947 - total_train_reward: -1579.1936\n",
      "Epoch 275/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.4054 - total_train_reward: -1161.7237\n",
      "Epoch 276/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.8802 - total_train_reward: -1252.8125\n",
      "Epoch 277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.7067 - total_train_reward: -948.3538\n",
      "Epoch 278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9222 - total_train_reward: -1543.3008\n",
      "Epoch 279/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.0504 - total_train_reward: -1151.9021\n",
      "Epoch 280/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.3371 - total_train_reward: -936.1807\n",
      "Epoch 281/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.9547 - total_train_reward: -1350.4092\n",
      "Epoch 282/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.0086 - total_train_reward: -1328.0448\n",
      "Epoch 283/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 33.1621 - total_train_reward: -1465.1724\n",
      "Epoch 284/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.6097 - total_train_reward: -1169.9260\n",
      "Epoch 285/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 12.2129 - total_train_reward: -1223.4642\n",
      "Epoch 286/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.8439 - total_train_reward: -1169.8484\n",
      "Epoch 287/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.2863 - total_train_reward: -1324.2919\n",
      "Epoch 288/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 37.5165 - total_train_reward: -1536.7771\n",
      "Epoch 289/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -15.4429 - total_train_reward: -1581.5658\n",
      "Epoch 290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1125 - total_train_reward: -1015.7460\n",
      "Epoch 291/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9245 - total_train_reward: -1653.4387\n",
      "Epoch 292/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.7609 - total_train_reward: -1438.1333\n",
      "Epoch 293/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.5266 - total_train_reward: -1165.2417\n",
      "Epoch 294/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.2260 - total_train_reward: -943.0453\n",
      "Epoch 295/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.0623 - total_train_reward: -948.6739\n",
      "Epoch 296/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 15.1159 - total_train_reward: -1379.8916\n",
      "Epoch 297/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.4321 - total_train_reward: -1641.5207\n",
      "Epoch 298/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.5253 - total_train_reward: -1686.0054\n",
      "Epoch 299/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5040 - total_train_reward: -1592.3049\n",
      "Epoch 300/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.8077 - total_train_reward: -1284.3942\n",
      "Epoch 301/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9021 - total_train_reward: -1705.4576\n",
      "Epoch 302/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3558 - total_train_reward: -1287.7690\n",
      "Epoch 303/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.4183 - total_train_reward: -1375.5116\n",
      "Epoch 304/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.9161 - total_train_reward: -1651.2513\n",
      "Epoch 305/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.9059 - total_train_reward: -1589.3097\n",
      "Epoch 306/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 59.8363 - total_train_reward: -943.5159\n",
      "Epoch 307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 20.9538 - total_train_reward: -1161.3861\n",
      "Epoch 308/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 16.3191 - total_train_reward: -1376.3921\n",
      "Epoch 309/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0557 - total_train_reward: -1277.9363\n",
      "Epoch 310/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.2190 - total_train_reward: -1242.4244\n",
      "Epoch 311/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 11.0554 - total_train_reward: -1282.4972\n",
      "Epoch 312/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 12.1529 - total_train_reward: -1169.3342\n",
      "Epoch 313/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.8624 - total_train_reward: -1170.7011\n",
      "Epoch 314/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.6642 - total_train_reward: -1287.8618\n",
      "Epoch 315/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.0430 - total_train_reward: -1173.2585\n",
      "Epoch 316/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 13.0104 - total_train_reward: -1389.6075\n",
      "Epoch 317/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.5056 - total_train_reward: -1162.5214\n",
      "Epoch 318/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.6980 - total_train_reward: -1308.6057\n",
      "Epoch 319/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 21.6077 - total_train_reward: -1443.2132\n",
      "Epoch 320/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.9312 - total_train_reward: -1372.7576\n",
      "Epoch 321/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3997 - total_train_reward: -1320.1332\n",
      "Epoch 322/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.4660 - total_train_reward: -1355.8343\n",
      "Epoch 323/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.0047 - total_train_reward: -1402.4726\n",
      "Epoch 324/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0135 - total_train_reward: -1534.1847\n",
      "Epoch 325/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.5577 - total_train_reward: -1487.2887\n",
      "Epoch 326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1990 - total_train_reward: -1219.4763\n",
      "Epoch 327/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 10.1979 - total_train_reward: -1779.7735\n",
      "Epoch 328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6227 - total_train_reward: -1172.0040\n",
      "Epoch 329/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.4119 - total_train_reward: -1066.7470\n",
      "Epoch 330/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 10.9866 - total_train_reward: -762.5298\n",
      "Epoch 331/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.4051 - total_train_reward: -1776.9043\n",
      "Epoch 332/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.8416 - total_train_reward: -1530.0970\n",
      "Epoch 333/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.6525 - total_train_reward: -752.4235\n",
      "Epoch 334/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 23.5724 - total_train_reward: -1214.2216\n",
      "Epoch 335/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.1333 - total_train_reward: -1186.1004\n",
      "Epoch 336/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.1002 - total_train_reward: -1388.1709\n",
      "Epoch 337/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6036 - total_train_reward: -1259.2340\n",
      "Epoch 338/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -15.5054 - total_train_reward: -1586.8782\n",
      "Epoch 339/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.5874 - total_train_reward: -1328.3998\n",
      "Epoch 340/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 9.4384 - total_train_reward: -1200.8922\n",
      "Epoch 341/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.4454 - total_train_reward: -1921.4984\n",
      "Epoch 342/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 10.4901 - total_train_reward: -988.0144\n",
      "Epoch 343/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.0527 - total_train_reward: -1273.5920\n",
      "Epoch 344/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.0029 - total_train_reward: -964.9457\n",
      "Epoch 345/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.5080 - total_train_reward: -1631.7588\n",
      "Epoch 346/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6601 - total_train_reward: -1744.3787\n",
      "Epoch 347/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 9.7032 - total_train_reward: -1089.9997\n",
      "Epoch 348/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 19.7455 - total_train_reward: -1355.7426\n",
      "Epoch 349/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.0392 - total_train_reward: -962.7357\n",
      "Epoch 350/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.2318 - total_train_reward: -948.5827\n",
      "Epoch 351/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.3309 - total_train_reward: -1306.7709\n",
      "Epoch 352/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 11.6793 - total_train_reward: -1242.3194\n",
      "Epoch 353/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -16.5794 - total_train_reward: -1515.5049\n",
      "Epoch 354/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.0112 - total_train_reward: -1113.4911\n",
      "Epoch 355/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.7585 - total_train_reward: -1269.0206\n",
      "Epoch 356/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.0361 - total_train_reward: -1174.2969\n",
      "Epoch 357/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.8406 - total_train_reward: -1207.7848\n",
      "Epoch 358/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.8182 - total_train_reward: -1660.1076\n",
      "Epoch 359/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.8040 - total_train_reward: -1626.6664\n",
      "Epoch 360/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.2050 - total_train_reward: -1110.3689\n",
      "Epoch 361/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.6900 - total_train_reward: -729.9934\n",
      "Epoch 362/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.0353 - total_train_reward: -1836.2555\n",
      "Epoch 363/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.7619 - total_train_reward: -1845.0458\n",
      "Epoch 364/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.3427 - total_train_reward: -1270.9378\n",
      "Epoch 365/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.4505 - total_train_reward: -1100.5593\n",
      "Epoch 366/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.0161 - total_train_reward: -1313.4099\n",
      "Epoch 367/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 24.5457 - total_train_reward: -1291.7826\n",
      "Epoch 368/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.9223 - total_train_reward: -1197.1710\n",
      "Epoch 369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0460 - total_train_reward: -845.1280\n",
      "Epoch 370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6863 - total_train_reward: -1436.7046\n",
      "Epoch 371/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.7137 - total_train_reward: -1169.5979\n",
      "Epoch 372/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.2338 - total_train_reward: -1203.5661\n",
      "Epoch 373/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.1546 - total_train_reward: -1317.4843\n",
      "Epoch 374/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.2079 - total_train_reward: -1049.1561\n",
      "Epoch 375/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.4256 - total_train_reward: -1601.6538\n",
      "Epoch 376/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.6572 - total_train_reward: -1172.1951\n",
      "Epoch 377/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.6940 - total_train_reward: -1739.2015\n",
      "Epoch 378/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.7459 - total_train_reward: -1459.4871\n",
      "Epoch 379/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.6301 - total_train_reward: -687.4996\n",
      "Epoch 380/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 12.0737 - total_train_reward: -1113.9644\n",
      "Epoch 381/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.9614 - total_train_reward: -1396.5475\n",
      "Epoch 382/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.8408 - total_train_reward: -1489.9933\n",
      "Epoch 383/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.6783 - total_train_reward: -1912.4715\n",
      "Epoch 384/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -19.0484 - total_train_reward: -1415.1011\n",
      "Epoch 385/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1672 - total_train_reward: -1220.6200\n",
      "Epoch 386/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3967 - total_train_reward: -1682.5657\n",
      "Epoch 387/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.8212 - total_train_reward: -962.9590\n",
      "Epoch 388/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.8155 - total_train_reward: -1567.9356\n",
      "Epoch 389/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0396 - total_train_reward: -1079.1585\n",
      "Epoch 390/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.5989 - total_train_reward: -950.2469\n",
      "Epoch 391/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4794 - total_train_reward: -968.8904\n",
      "Epoch 392/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.5805 - total_train_reward: -1069.0002\n",
      "Epoch 393/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.0567 - total_train_reward: -850.4479\n",
      "Epoch 394/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9879 - total_train_reward: -1243.0832\n",
      "Epoch 395/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.7176 - total_train_reward: -1167.3996\n",
      "Epoch 396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8548 - total_train_reward: -1060.7257\n",
      "Epoch 397/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3525 - total_train_reward: -1084.1604\n",
      "Epoch 398/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7993 - total_train_reward: -930.4968\n",
      "Epoch 399/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8147 - total_train_reward: -1470.0329\n",
      "Epoch 400/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.2681 - total_train_reward: -1171.2628\n",
      "Epoch 401/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.4729 - total_train_reward: -1300.1803\n",
      "Epoch 402/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.6872 - total_train_reward: -1454.4092\n",
      "Epoch 403/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.7116 - total_train_reward: -1282.1549\n",
      "Epoch 404/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.3599 - total_train_reward: -1277.9635\n",
      "Epoch 405/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.1394 - total_train_reward: -1322.4956\n",
      "Epoch 406/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.7744 - total_train_reward: -832.0999\n",
      "Epoch 407/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.6791 - total_train_reward: -1347.5172\n",
      "Epoch 408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0071 - total_train_reward: -1263.8286\n",
      "Epoch 409/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8661 - total_train_reward: -1286.8900\n",
      "Epoch 410/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.4752 - total_train_reward: -1216.4056\n",
      "Epoch 411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1125 - total_train_reward: -1111.8374\n",
      "Epoch 412/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.9647 - total_train_reward: -1559.7669\n",
      "Epoch 413/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.2476 - total_train_reward: -1648.4344\n",
      "Epoch 414/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9862 - total_train_reward: -1067.5496\n",
      "Epoch 415/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9358 - total_train_reward: -851.9465\n",
      "Epoch 416/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.3046 - total_train_reward: -1154.3100\n",
      "Epoch 417/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6326 - total_train_reward: -981.2925\n",
      "Epoch 418/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0939 - total_train_reward: -1171.6468\n",
      "Epoch 419/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.7200 - total_train_reward: -1438.7332\n",
      "Epoch 420/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.5586 - total_train_reward: -1150.8264\n",
      "Epoch 421/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.3385 - total_train_reward: -1643.0379\n",
      "Epoch 422/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.0881 - total_train_reward: -1649.8299\n",
      "Epoch 423/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8062 - total_train_reward: -1159.6415\n",
      "Epoch 424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9730 - total_train_reward: -1288.3326\n",
      "Epoch 425/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9047 - total_train_reward: -963.9937\n",
      "Epoch 426/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.2022 - total_train_reward: -1044.9448\n",
      "Epoch 427/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.4674 - total_train_reward: -1165.4460\n",
      "Epoch 428/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1006 - total_train_reward: -1164.3224\n",
      "Epoch 429/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.5685 - total_train_reward: -1633.9121\n",
      "Epoch 430/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9635 - total_train_reward: -969.8288\n",
      "Epoch 431/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.4198 - total_train_reward: -1278.1763\n",
      "Epoch 432/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -16.3002 - total_train_reward: -1786.7692\n",
      "Epoch 433/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6812 - total_train_reward: -1816.8922\n",
      "Epoch 434/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9532 - total_train_reward: -1128.7664\n",
      "Epoch 435/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.4258 - total_train_reward: -964.8690\n",
      "Epoch 436/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.9341 - total_train_reward: -1289.1310\n",
      "Epoch 437/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.2958 - total_train_reward: -1604.9256\n",
      "Epoch 438/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.2836 - total_train_reward: -1466.8723\n",
      "Epoch 439/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9795 - total_train_reward: -1223.3350\n",
      "Epoch 440/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9428 - total_train_reward: -1864.4360\n",
      "Epoch 441/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.0970 - total_train_reward: -1263.3494\n",
      "Epoch 442/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2892 - total_train_reward: -1239.0363\n",
      "Epoch 443/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.9997 - total_train_reward: -1652.1453\n",
      "Epoch 444/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.4153 - total_train_reward: -1040.6231\n",
      "Epoch 445/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.0405 - total_train_reward: -1669.0651\n",
      "Epoch 446/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8319 - total_train_reward: -1236.2235\n",
      "Epoch 447/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.4420 - total_train_reward: -1067.9333\n",
      "Epoch 448/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -15.6496 - total_train_reward: -1546.1063\n",
      "Epoch 449/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.7811 - total_train_reward: -1522.4239\n",
      "Epoch 450/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.0473 - total_train_reward: -1299.0188\n",
      "Epoch 451/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.8295 - total_train_reward: -871.3354\n",
      "Epoch 452/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.0803 - total_train_reward: -1433.3656\n",
      "Epoch 453/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.6443 - total_train_reward: -1174.1931\n",
      "Epoch 454/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.7478 - total_train_reward: -1292.4224\n",
      "Epoch 455/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.3486 - total_train_reward: -1437.0072\n",
      "Epoch 456/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 11.8407 - total_train_reward: -893.3522\n",
      "Epoch 457/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.2518 - total_train_reward: -1252.9431\n",
      "Epoch 458/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.7730 - total_train_reward: -1505.5321\n",
      "Epoch 459/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.2516 - total_train_reward: -1037.6728\n",
      "Epoch 460/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0540 - total_train_reward: -1033.5903\n",
      "Epoch 461/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.2864 - total_train_reward: -1282.2765\n",
      "Epoch 462/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.8550 - total_train_reward: -1040.3497\n",
      "Epoch 463/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8274 - total_train_reward: -1308.7590\n",
      "Epoch 464/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.9747 - total_train_reward: -1405.1700\n",
      "Epoch 465/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3110 - total_train_reward: -1258.0173\n",
      "Epoch 466/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.8902 - total_train_reward: -1069.0957\n",
      "Epoch 467/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.5804 - total_train_reward: -1042.0552\n",
      "Epoch 468/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.8848 - total_train_reward: -1691.9893\n",
      "Epoch 469/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.5319 - total_train_reward: -909.1442\n",
      "Epoch 470/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.0167 - total_train_reward: -969.6146\n",
      "Epoch 471/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.9388 - total_train_reward: -1070.9555\n",
      "Epoch 472/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.5954 - total_train_reward: -1695.1253\n",
      "Epoch 473/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.0849 - total_train_reward: -1343.6572\n",
      "Epoch 474/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6172 - total_train_reward: -1489.6937\n",
      "Epoch 475/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5864 - total_train_reward: -1422.7172\n",
      "Epoch 476/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.5437 - total_train_reward: -1037.7701\n",
      "Epoch 477/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6568 - total_train_reward: -1177.9569\n",
      "Epoch 478/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.0540 - total_train_reward: -860.9363\n",
      "Epoch 479/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.6228 - total_train_reward: -1829.3284\n",
      "Epoch 480/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.1364 - total_train_reward: -1025.9675\n",
      "Epoch 481/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1278 - total_train_reward: -1175.2123\n",
      "Epoch 482/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.4008 - total_train_reward: -1362.2297\n",
      "Epoch 483/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5130 - total_train_reward: -1078.4143\n",
      "Epoch 484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2332 - total_train_reward: -868.4920\n",
      "Epoch 485/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7838 - total_train_reward: -1070.0704\n",
      "Epoch 486/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.9617 - total_train_reward: -1280.5773\n",
      "Epoch 487/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3842 - total_train_reward: -1168.4748\n",
      "Epoch 488/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.9363 - total_train_reward: -1065.7146\n",
      "Epoch 489/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9720 - total_train_reward: -1193.1988\n",
      "Epoch 490/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9561 - total_train_reward: -1038.6850\n",
      "Epoch 491/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.3160 - total_train_reward: -1570.6262\n",
      "Epoch 492/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6054 - total_train_reward: -1801.0570\n",
      "Epoch 493/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0394 - total_train_reward: -1074.0140\n",
      "Epoch 494/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.1671 - total_train_reward: -1068.9734\n",
      "Epoch 495/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.1758 - total_train_reward: -1614.9947\n",
      "Epoch 496/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5428 - total_train_reward: -1144.3490\n",
      "Epoch 497/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.9329 - total_train_reward: -1324.5679\n",
      "Epoch 498/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.5913 - total_train_reward: -1404.5259\n",
      "Epoch 499/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.1988 - total_train_reward: -1066.8424\n",
      "Epoch 500/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6342 - total_train_reward: -1175.1048\n",
      "Epoch 501/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1223 - total_train_reward: -1035.5065\n",
      "Epoch 502/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.2176 - total_train_reward: -1716.7332\n",
      "Epoch 503/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.5316 - total_train_reward: -1649.3744\n",
      "Epoch 504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1127 - total_train_reward: -1027.7941\n",
      "Epoch 505/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.6177 - total_train_reward: -1028.0288\n",
      "Epoch 506/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9479 - total_train_reward: -1196.7443\n",
      "Epoch 507/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.0033 - total_train_reward: -1051.5036\n",
      "Epoch 508/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.4119 - total_train_reward: -980.0320\n",
      "Epoch 509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6569 - total_train_reward: -1030.0246\n",
      "Epoch 510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5706 - total_train_reward: -1791.2598\n",
      "Epoch 511/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.8321 - total_train_reward: -1209.4480\n",
      "Epoch 512/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7713 - total_train_reward: -1683.5755\n",
      "Epoch 513/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.6627 - total_train_reward: -1290.8968\n",
      "Epoch 514/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1128 - total_train_reward: -1137.3749\n",
      "Epoch 515/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7133 - total_train_reward: -1269.9339\n",
      "Epoch 516/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.3830 - total_train_reward: -1560.1468\n",
      "Epoch 517/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.8167 - total_train_reward: -970.8539\n",
      "Epoch 518/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.3695 - total_train_reward: -1081.8980\n",
      "Epoch 519/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.3463 - total_train_reward: -1692.8811\n",
      "Epoch 520/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.2619 - total_train_reward: -1591.9502\n",
      "Epoch 521/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.4077 - total_train_reward: -969.3569\n",
      "Epoch 522/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8769 - total_train_reward: -865.6647\n",
      "Epoch 523/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 12.9063 - total_train_reward: -927.0569\n",
      "Epoch 524/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.2110 - total_train_reward: -1696.3133\n",
      "Epoch 525/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.7174 - total_train_reward: -1307.6546\n",
      "Epoch 526/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.2429 - total_train_reward: -900.2141\n",
      "Epoch 527/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.5367 - total_train_reward: -1339.0553\n",
      "Epoch 528/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.9808 - total_train_reward: -1551.4477\n",
      "Epoch 529/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7042 - total_train_reward: -1199.6724\n",
      "Epoch 530/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.1406 - total_train_reward: -927.0695\n",
      "Epoch 531/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.3774 - total_train_reward: -1070.1696\n",
      "Epoch 532/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1751 - total_train_reward: -1166.9611\n",
      "Epoch 533/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2083 - total_train_reward: -1956.3152\n",
      "Epoch 534/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.7074 - total_train_reward: -1292.9982\n",
      "Epoch 535/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.5043 - total_train_reward: -971.0093\n",
      "Epoch 536/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.8746 - total_train_reward: -934.8475\n",
      "Epoch 537/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5665 - total_train_reward: -1160.0317\n",
      "Epoch 538/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.6011 - total_train_reward: -1263.8799\n",
      "Epoch 539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8442 - total_train_reward: -1938.7371\n",
      "Epoch 540/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1806 - total_train_reward: -957.3789\n",
      "Epoch 541/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4670 - total_train_reward: -1208.4808\n",
      "Epoch 542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1519 - total_train_reward: -1172.1568\n",
      "Epoch 543/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.7934 - total_train_reward: -1760.8318\n",
      "Epoch 544/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9873 - total_train_reward: -1200.4514\n",
      "Epoch 545/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.0860 - total_train_reward: -916.9672\n",
      "Epoch 546/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6937 - total_train_reward: -1765.3039\n",
      "Epoch 547/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.0222 - total_train_reward: -1267.4827\n",
      "Epoch 548/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6668 - total_train_reward: -1065.6932\n",
      "Epoch 549/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.0278 - total_train_reward: -1385.9985\n",
      "Epoch 550/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.5481 - total_train_reward: -1294.9756\n",
      "Epoch 551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1120 - total_train_reward: -1766.2260\n",
      "Epoch 552/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.8428 - total_train_reward: -1289.2891\n",
      "Epoch 553/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9721 - total_train_reward: -1171.3171\n",
      "Epoch 554/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.5667 - total_train_reward: -1529.9430\n",
      "Epoch 555/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.4530 - total_train_reward: -1171.7234\n",
      "Epoch 556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8963 - total_train_reward: -1261.8554\n",
      "Epoch 557/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.7008 - total_train_reward: -1170.1989\n",
      "Epoch 558/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.4971 - total_train_reward: -897.7823\n",
      "Epoch 559/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.9638 - total_train_reward: -1070.1202\n",
      "Epoch 560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4474 - total_train_reward: -746.8113\n",
      "Epoch 561/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.2511 - total_train_reward: -746.2591\n",
      "Epoch 562/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.7887 - total_train_reward: -1070.0139\n",
      "Epoch 563/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.6705 - total_train_reward: -862.6473\n",
      "Epoch 564/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6300 - total_train_reward: -834.9339\n",
      "Epoch 565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3501 - total_train_reward: -897.5396\n",
      "Epoch 566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3528 - total_train_reward: -1591.4859\n",
      "Epoch 567/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.2420 - total_train_reward: -1018.9017\n",
      "Epoch 568/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.0402 - total_train_reward: -1184.2452\n",
      "Epoch 569/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.1535 - total_train_reward: -1329.2965\n",
      "Epoch 570/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.6578 - total_train_reward: -969.5212\n",
      "Epoch 571/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6020 - total_train_reward: -1520.0996\n",
      "Epoch 572/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.6686 - total_train_reward: -1277.6564\n",
      "Epoch 573/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.2209 - total_train_reward: -1432.6018\n",
      "Epoch 574/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7895 - total_train_reward: -1462.5951\n",
      "Epoch 575/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9767 - total_train_reward: -970.3550\n",
      "Epoch 576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6258 - total_train_reward: -1074.9454\n",
      "Epoch 577/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 13.6956 - total_train_reward: -627.2491\n",
      "Epoch 578/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.5475 - total_train_reward: -1636.7033\n",
      "Epoch 579/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7347 - total_train_reward: -1198.2306\n",
      "Epoch 580/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.1950 - total_train_reward: -863.1334\n",
      "Epoch 581/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.1677 - total_train_reward: -1166.0206\n",
      "Epoch 582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2713 - total_train_reward: -1337.8201\n",
      "Epoch 583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8688 - total_train_reward: -1182.6680\n",
      "Epoch 584/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2792 - total_train_reward: -1027.9934\n",
      "Epoch 585/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.2332 - total_train_reward: -860.3788\n",
      "Epoch 586/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0180 - total_train_reward: -1782.0746\n",
      "Epoch 587/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.4893 - total_train_reward: -696.8770\n",
      "Epoch 588/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.0794 - total_train_reward: -1095.1609\n",
      "Epoch 589/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.3028 - total_train_reward: -1286.4982\n",
      "Epoch 590/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.3339 - total_train_reward: -1452.5957\n",
      "Epoch 591/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.3919 - total_train_reward: -1281.8213\n",
      "Epoch 592/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.8292 - total_train_reward: -580.5995\n",
      "Epoch 593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0917 - total_train_reward: -1788.2650\n",
      "Epoch 594/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.8826 - total_train_reward: -1721.8500\n",
      "Epoch 595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2534 - total_train_reward: -1556.8618\n",
      "Epoch 596/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.9858 - total_train_reward: -1803.6619\n",
      "Epoch 597/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9126 - total_train_reward: -1243.2497\n",
      "Epoch 598/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.8223 - total_train_reward: -974.4264\n",
      "Epoch 599/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9644 - total_train_reward: -675.2123\n",
      "Epoch 600/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.8814 - total_train_reward: -1815.0765\n",
      "Epoch 601/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.1317 - total_train_reward: -1795.7360\n",
      "Epoch 602/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.9731 - total_train_reward: -1177.6141\n",
      "Epoch 603/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7500 - total_train_reward: -1096.2895\n",
      "Epoch 604/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5822 - total_train_reward: -1217.2368\n",
      "Epoch 605/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0048 - total_train_reward: -1305.5704\n",
      "Epoch 606/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.7534 - total_train_reward: -1271.6415\n",
      "Epoch 607/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6589 - total_train_reward: -1175.6962\n",
      "Epoch 608/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.8290 - total_train_reward: -1195.7999\n",
      "Epoch 609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0877 - total_train_reward: -945.9555\n",
      "Epoch 610/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7482 - total_train_reward: -969.7165\n",
      "Epoch 611/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.5714 - total_train_reward: -991.9900\n",
      "Epoch 612/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6202 - total_train_reward: -1171.4863\n",
      "Epoch 613/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0541 - total_train_reward: -1416.7758\n",
      "Epoch 614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5178 - total_train_reward: -1115.1068\n",
      "Epoch 615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1813 - total_train_reward: -970.3085\n",
      "Epoch 616/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3392 - total_train_reward: -1800.2166\n",
      "Epoch 617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3499 - total_train_reward: -1175.4402\n",
      "Epoch 618/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.2863 - total_train_reward: -1070.4600\n",
      "Epoch 619/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7537 - total_train_reward: -778.7807\n",
      "Epoch 620/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.6884 - total_train_reward: -1505.0053\n",
      "Epoch 621/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8280 - total_train_reward: -785.1812\n",
      "Epoch 622/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.5581 - total_train_reward: -970.5863\n",
      "Epoch 623/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9716 - total_train_reward: -1716.3068\n",
      "Epoch 624/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.8417 - total_train_reward: -1651.8557\n",
      "Epoch 625/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.1776 - total_train_reward: -1835.9468\n",
      "Epoch 626/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1519 - total_train_reward: -1165.0605\n",
      "Epoch 627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8772 - total_train_reward: -858.3335\n",
      "Epoch 628/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3027 - total_train_reward: -1392.0218\n",
      "Epoch 629/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6925 - total_train_reward: -1722.2043\n",
      "Epoch 630/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.0346 - total_train_reward: -1346.0659\n",
      "Epoch 631/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.5384 - total_train_reward: -1286.3291\n",
      "Epoch 632/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.4872 - total_train_reward: -1363.7964\n",
      "Epoch 633/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0709 - total_train_reward: -1381.7929\n",
      "Epoch 634/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9558 - total_train_reward: -1833.1536\n",
      "Epoch 635/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.7266 - total_train_reward: -1091.4274\n",
      "Epoch 636/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.9370 - total_train_reward: -1572.5603\n",
      "Epoch 637/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1721 - total_train_reward: -1246.3352\n",
      "Epoch 638/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7536 - total_train_reward: -631.7737\n",
      "Epoch 639/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.9496 - total_train_reward: -862.4020\n",
      "Epoch 640/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.8895 - total_train_reward: -1440.9996\n",
      "Epoch 641/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9514 - total_train_reward: -1066.6451\n",
      "Epoch 642/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7433 - total_train_reward: -1089.5686\n",
      "Epoch 643/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.1745 - total_train_reward: -1364.3534\n",
      "Epoch 644/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3701 - total_train_reward: -1224.7694\n",
      "Epoch 645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5085 - total_train_reward: -1072.9721\n",
      "Epoch 646/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.8658 - total_train_reward: -1470.7553\n",
      "Epoch 647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3414 - total_train_reward: -1171.1409\n",
      "Epoch 648/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.4082 - total_train_reward: -1178.5957\n",
      "Epoch 649/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.2287 - total_train_reward: -1789.7651\n",
      "Epoch 650/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.2997 - total_train_reward: -967.0144\n",
      "Epoch 651/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.5544 - total_train_reward: -1713.4565\n",
      "Epoch 652/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.1327 - total_train_reward: -1189.8102\n",
      "Epoch 653/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1984 - total_train_reward: -1313.9693\n",
      "Epoch 654/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.2697 - total_train_reward: -1076.4343\n",
      "Epoch 655/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.8768 - total_train_reward: -1290.1579\n",
      "Epoch 656/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8164 - total_train_reward: -1069.9479\n",
      "Epoch 657/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.8396 - total_train_reward: -1182.2323\n",
      "Epoch 658/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0966 - total_train_reward: -1729.8040\n",
      "Epoch 659/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.2202 - total_train_reward: -1556.9639\n",
      "Epoch 660/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2694 - total_train_reward: -1170.7687\n",
      "Epoch 661/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.4079 - total_train_reward: -1757.9958\n",
      "Epoch 662/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.6205 - total_train_reward: -1177.1533\n",
      "Epoch 663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8861 - total_train_reward: -1523.1355\n",
      "Epoch 664/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9623 - total_train_reward: -615.8225\n",
      "Epoch 665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4520 - total_train_reward: -1453.6557\n",
      "Epoch 666/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6002 - total_train_reward: -1182.1122\n",
      "Epoch 667/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.2138 - total_train_reward: -847.9306\n",
      "Epoch 668/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5111 - total_train_reward: -1118.4209\n",
      "Epoch 669/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.9838 - total_train_reward: -1092.9236\n",
      "Epoch 670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0304 - total_train_reward: -1736.3508\n",
      "Epoch 671/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.7878 - total_train_reward: -1019.3999\n",
      "Epoch 672/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -15.6025 - total_train_reward: -390.9063\n",
      "Epoch 673/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 23.1168 - total_train_reward: -1945.9516\n",
      "Epoch 674/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.6377 - total_train_reward: -1237.7720\n",
      "Epoch 675/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -18.9785 - total_train_reward: -1769.3615\n",
      "Epoch 676/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9561 - total_train_reward: -1168.7976\n",
      "Epoch 677/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 16.0173 - total_train_reward: -970.2977\n",
      "Epoch 678/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6197 - total_train_reward: -1261.5767\n",
      "Epoch 679/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9009 - total_train_reward: -1215.3511\n",
      "Epoch 680/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.0939 - total_train_reward: -1804.5800\n",
      "Epoch 681/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.6472 - total_train_reward: -1428.7865\n",
      "Epoch 682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2295 - total_train_reward: -1440.3457\n",
      "Epoch 683/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.1069 - total_train_reward: -632.3031\n",
      "Epoch 684/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7586 - total_train_reward: -1271.9306\n",
      "Epoch 685/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1754 - total_train_reward: -1258.2425\n",
      "Epoch 686/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9843 - total_train_reward: -1069.8333\n",
      "Epoch 687/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8685 - total_train_reward: -1149.4950\n",
      "Epoch 688/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.2352 - total_train_reward: -1300.6958\n",
      "Epoch 689/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.2462 - total_train_reward: -1858.9461\n",
      "Epoch 690/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.2199 - total_train_reward: -1068.9906\n",
      "Epoch 691/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1824 - total_train_reward: -1148.6184\n",
      "Epoch 692/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.4815 - total_train_reward: -969.6151\n",
      "Epoch 693/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9812 - total_train_reward: -1164.0401\n",
      "Epoch 694/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.2258 - total_train_reward: -1151.8764\n",
      "Epoch 695/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.8244 - total_train_reward: -1300.0454\n",
      "Epoch 696/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.2211 - total_train_reward: -817.1621\n",
      "Epoch 697/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6442 - total_train_reward: -970.0448\n",
      "Epoch 698/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -16.0906 - total_train_reward: -1653.7223\n",
      "Epoch 699/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.3584 - total_train_reward: -1493.3819\n",
      "Epoch 700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8862 - total_train_reward: -1074.1262\n",
      "Epoch 701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2939 - total_train_reward: -1721.9731\n",
      "Epoch 702/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4391 - total_train_reward: -1560.6680\n",
      "Epoch 703/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.7262 - total_train_reward: -1015.3434\n",
      "Epoch 704/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4818 - total_train_reward: -1159.1040\n",
      "Epoch 705/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.3761 - total_train_reward: -1787.3001\n",
      "Epoch 706/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.3337 - total_train_reward: -1556.2443\n",
      "Epoch 707/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.8135 - total_train_reward: -1540.7379\n",
      "Epoch 708/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1864 - total_train_reward: -1819.9649\n",
      "Epoch 709/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.1303 - total_train_reward: -1349.0483\n",
      "Epoch 710/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4303 - total_train_reward: -1750.3882\n",
      "Epoch 711/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5042 - total_train_reward: -969.8189\n",
      "Epoch 712/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.9136 - total_train_reward: -1671.2818\n",
      "Epoch 713/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.8309 - total_train_reward: -1282.3729\n",
      "Epoch 714/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8081 - total_train_reward: -863.8107\n",
      "Epoch 715/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.8121 - total_train_reward: -949.4449\n",
      "Epoch 716/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9934 - total_train_reward: -1668.4765\n",
      "Epoch 717/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4216 - total_train_reward: -1789.9806\n",
      "Epoch 718/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5393 - total_train_reward: -972.9779\n",
      "Epoch 719/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2310 - total_train_reward: -863.9750\n",
      "Epoch 720/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.7421 - total_train_reward: -1508.3894\n",
      "Epoch 721/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.9065 - total_train_reward: -1069.4484\n",
      "Epoch 722/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0931 - total_train_reward: -1196.8546\n",
      "Epoch 723/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.7217 - total_train_reward: -1380.2072\n",
      "Epoch 724/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.8026 - total_train_reward: -652.7647\n",
      "Epoch 725/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.5091 - total_train_reward: -1512.5021\n",
      "Epoch 726/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.3953 - total_train_reward: -747.4918\n",
      "Epoch 727/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9341 - total_train_reward: -1761.3526\n",
      "Epoch 728/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2388 - total_train_reward: -1183.3629\n",
      "Epoch 729/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.9053 - total_train_reward: -1595.3320\n",
      "Epoch 730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8790 - total_train_reward: -969.6321\n",
      "Epoch 731/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7106 - total_train_reward: -1162.8057\n",
      "Epoch 732/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.6943 - total_train_reward: -1050.7769\n",
      "Epoch 733/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6798 - total_train_reward: -1706.5700\n",
      "Epoch 734/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.3698 - total_train_reward: -1280.7154\n",
      "Epoch 735/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9988 - total_train_reward: -862.7911\n",
      "Epoch 736/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0757 - total_train_reward: -1631.3691\n",
      "Epoch 737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1764 - total_train_reward: -1643.9793\n",
      "Epoch 738/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5285 - total_train_reward: -747.8948\n",
      "Epoch 739/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9190 - total_train_reward: -1763.3352\n",
      "Epoch 740/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9151 - total_train_reward: -1168.2254\n",
      "Epoch 741/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.3028 - total_train_reward: -1449.7254\n",
      "Epoch 742/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.5519 - total_train_reward: -1762.5999\n",
      "Epoch 743/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.6987 - total_train_reward: -978.5828\n",
      "Epoch 744/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3442 - total_train_reward: -1316.8918\n",
      "Epoch 745/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.9430 - total_train_reward: -632.6142\n",
      "Epoch 746/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.5507 - total_train_reward: -626.8398\n",
      "Epoch 747/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7361 - total_train_reward: -1243.2145\n",
      "Epoch 748/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.0071 - total_train_reward: -626.4053\n",
      "Epoch 749/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.7724 - total_train_reward: -1090.0132\n",
      "Epoch 750/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5358 - total_train_reward: -1077.9975\n",
      "Epoch 751/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8050 - total_train_reward: -986.9032\n",
      "Epoch 752/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.2230 - total_train_reward: -1669.5335\n",
      "Epoch 753/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.5393 - total_train_reward: -1361.2125\n",
      "Epoch 754/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7832 - total_train_reward: -1851.1761\n",
      "Epoch 755/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.1240 - total_train_reward: -567.3806\n",
      "Epoch 756/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1312 - total_train_reward: -1837.3424\n",
      "Epoch 757/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.1404 - total_train_reward: -1172.3889\n",
      "Epoch 758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4083 - total_train_reward: -1283.1148\n",
      "Epoch 759/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8136 - total_train_reward: -1168.7466\n",
      "Epoch 760/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.0023 - total_train_reward: -860.0829\n",
      "Epoch 761/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6573 - total_train_reward: -969.6954\n",
      "Epoch 762/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.5151 - total_train_reward: -1456.3665\n",
      "Epoch 763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5479 - total_train_reward: -1147.6383\n",
      "Epoch 764/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.0905 - total_train_reward: -1831.6512\n",
      "Epoch 765/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.4987 - total_train_reward: -1335.2857\n",
      "Epoch 766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1200 - total_train_reward: -1311.1885\n",
      "Epoch 767/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9548 - total_train_reward: -1167.4692\n",
      "Epoch 768/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9180 - total_train_reward: -1232.3146\n",
      "Epoch 769/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.3306 - total_train_reward: -1310.0342\n",
      "Epoch 770/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.7289 - total_train_reward: -763.5748\n",
      "Epoch 771/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6339 - total_train_reward: -1619.2385\n",
      "Epoch 772/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.2422 - total_train_reward: -862.1239\n",
      "Epoch 773/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9763 - total_train_reward: -1030.9440\n",
      "Epoch 774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2872 - total_train_reward: -1206.7360\n",
      "Epoch 775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0309 - total_train_reward: -751.4160\n",
      "Epoch 776/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9069 - total_train_reward: -634.2521\n",
      "Epoch 777/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0345 - total_train_reward: -1347.8376\n",
      "Epoch 778/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0256 - total_train_reward: -1375.9441\n",
      "Epoch 779/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9242 - total_train_reward: -630.4739\n",
      "Epoch 780/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.4148 - total_train_reward: -783.2900\n",
      "Epoch 781/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.7491 - total_train_reward: -1665.2194\n",
      "Epoch 782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8575 - total_train_reward: -1325.9653\n",
      "Epoch 783/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.6706 - total_train_reward: -1067.4132\n",
      "Epoch 784/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9103 - total_train_reward: -1067.2597\n",
      "Epoch 785/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.1241 - total_train_reward: -1868.7944\n",
      "Epoch 786/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.6850 - total_train_reward: -1518.4738\n",
      "Epoch 787/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4498 - total_train_reward: -1650.4404\n",
      "Epoch 788/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.9291 - total_train_reward: -1286.8120\n",
      "Epoch 789/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.4585 - total_train_reward: -970.2000\n",
      "Epoch 790/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.0620 - total_train_reward: -1353.0664\n",
      "Epoch 791/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.2429 - total_train_reward: -1853.9905\n",
      "Epoch 792/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.0832 - total_train_reward: -1468.9269\n",
      "Epoch 793/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8416 - total_train_reward: -1176.1651\n",
      "Epoch 794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6486 - total_train_reward: -1300.7102\n",
      "Epoch 795/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.2512 - total_train_reward: -864.1324\n",
      "Epoch 796/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6422 - total_train_reward: -1736.1942\n",
      "Epoch 797/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.4785 - total_train_reward: -1163.1552\n",
      "Epoch 798/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.7703 - total_train_reward: -959.7174\n",
      "Epoch 799/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1147 - total_train_reward: -1595.7154\n",
      "Epoch 800/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6588 - total_train_reward: -1417.2048\n",
      "Epoch 801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4415 - total_train_reward: -1342.8880\n",
      "Epoch 802/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8602 - total_train_reward: -1164.8091\n",
      "Epoch 803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6805 - total_train_reward: -862.2914\n",
      "Epoch 804/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0515 - total_train_reward: -1171.4805\n",
      "Epoch 805/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.4100 - total_train_reward: -975.7969\n",
      "Epoch 806/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1399 - total_train_reward: -932.7824\n",
      "Epoch 807/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.1209 - total_train_reward: -1656.5728\n",
      "Epoch 808/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.3508 - total_train_reward: -1786.1036\n",
      "Epoch 809/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1757 - total_train_reward: -1885.2540\n",
      "Epoch 810/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.6180 - total_train_reward: -996.4895\n",
      "Epoch 811/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.5189 - total_train_reward: -1178.0290\n",
      "Epoch 812/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.0980 - total_train_reward: -748.4917\n",
      "Epoch 813/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.0788 - total_train_reward: -503.4818\n",
      "Epoch 814/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6568 - total_train_reward: -1444.2695\n",
      "Epoch 815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6967 - total_train_reward: -1253.9254\n",
      "Epoch 816/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.0411 - total_train_reward: -1465.0129\n",
      "Epoch 817/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.2422 - total_train_reward: -969.5905\n",
      "Epoch 818/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.3018 - total_train_reward: -1585.8821\n",
      "Epoch 819/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.4290 - total_train_reward: -1808.5692\n",
      "Epoch 820/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.5760 - total_train_reward: -970.1362\n",
      "Epoch 821/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0284 - total_train_reward: -703.6955\n",
      "Epoch 822/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.4160 - total_train_reward: -1486.7272\n",
      "Epoch 823/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 11.8142 - total_train_reward: -1912.2804\n",
      "Epoch 824/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.6987 - total_train_reward: -975.3477\n",
      "Epoch 825/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.4416 - total_train_reward: -748.0989\n",
      "Epoch 826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3688 - total_train_reward: -1266.7175\n",
      "Epoch 827/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9006 - total_train_reward: -642.1737\n",
      "Epoch 828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5019 - total_train_reward: -1178.3593\n",
      "Epoch 829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6217 - total_train_reward: -1486.8509\n",
      "Epoch 830/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.7076 - total_train_reward: -1125.6623\n",
      "Epoch 831/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.8314 - total_train_reward: -1085.0819\n",
      "Epoch 832/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.4994 - total_train_reward: -1637.7952\n",
      "Epoch 833/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.9412 - total_train_reward: -1340.9119\n",
      "Epoch 834/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3741 - total_train_reward: -632.7573\n",
      "Epoch 835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5335 - total_train_reward: -953.1007\n",
      "Epoch 836/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3318 - total_train_reward: -862.8491\n",
      "Epoch 837/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.3052 - total_train_reward: -1415.4366\n",
      "Epoch 838/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.3753 - total_train_reward: -969.9577\n",
      "Epoch 839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4810 - total_train_reward: -1573.4876\n",
      "Epoch 840/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.5954 - total_train_reward: -1915.8824\n",
      "Epoch 841/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.9238 - total_train_reward: -1812.7274\n",
      "Epoch 842/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.7371 - total_train_reward: -743.5502\n",
      "Epoch 843/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.1818 - total_train_reward: -1652.9257\n",
      "Epoch 844/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1440 - total_train_reward: -1294.9072\n",
      "Epoch 845/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.3035 - total_train_reward: -1205.2882\n",
      "Epoch 846/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9559 - total_train_reward: -1178.0226\n",
      "Epoch 847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5873 - total_train_reward: -1523.6945\n",
      "Epoch 848/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.0182 - total_train_reward: -991.1032\n",
      "Epoch 849/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.6546 - total_train_reward: -972.3997\n",
      "Epoch 850/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5284 - total_train_reward: -999.8392\n",
      "Epoch 851/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6615 - total_train_reward: -1281.5159\n",
      "Epoch 852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9923 - total_train_reward: -1366.7943\n",
      "Epoch 853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6663 - total_train_reward: -1152.0171\n",
      "Epoch 854/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.9654 - total_train_reward: -1164.5049\n",
      "Epoch 855/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.0496 - total_train_reward: -1336.1388\n",
      "Epoch 856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2811 - total_train_reward: -771.7534\n",
      "Epoch 857/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.0427 - total_train_reward: -1151.3956\n",
      "Epoch 858/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.4073 - total_train_reward: -1171.5401\n",
      "Epoch 859/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.6991 - total_train_reward: -898.5418\n",
      "Epoch 860/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.0232 - total_train_reward: -935.5462\n",
      "Epoch 861/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.1119 - total_train_reward: -862.9159\n",
      "Epoch 862/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.2380 - total_train_reward: -895.1178\n",
      "Epoch 863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5631 - total_train_reward: -1677.4534\n",
      "Epoch 864/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8227 - total_train_reward: -970.3388\n",
      "Epoch 865/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.6412 - total_train_reward: -1726.4495\n",
      "Epoch 866/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9536 - total_train_reward: -1170.3799\n",
      "Epoch 867/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6937 - total_train_reward: -1348.1757\n",
      "Epoch 868/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.1780 - total_train_reward: -969.7971\n",
      "Epoch 869/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8762 - total_train_reward: -1162.2770\n",
      "Epoch 870/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9747 - total_train_reward: -1175.2889\n",
      "Epoch 871/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.4765 - total_train_reward: -1174.3045\n",
      "Epoch 872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7018 - total_train_reward: -1228.8634\n",
      "Epoch 873/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.5524 - total_train_reward: -1096.5052\n",
      "Epoch 874/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.3244 - total_train_reward: -1671.1285\n",
      "Epoch 875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6438 - total_train_reward: -1162.5341\n",
      "Epoch 876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6774 - total_train_reward: -1308.5761\n",
      "Epoch 877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.4542 - total_train_reward: -1400.4644\n",
      "Epoch 878/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5231 - total_train_reward: -1072.5108\n",
      "Epoch 879/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.0815 - total_train_reward: -1240.2958\n",
      "Epoch 880/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.4534 - total_train_reward: -862.2718\n",
      "Epoch 881/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.6432 - total_train_reward: -1886.9716\n",
      "Epoch 882/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.1134 - total_train_reward: -1071.1355\n",
      "Epoch 883/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.1572 - total_train_reward: -1234.8459\n",
      "Epoch 884/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.7987 - total_train_reward: -1659.1518\n",
      "Epoch 885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3306 - total_train_reward: -1732.6785\n",
      "Epoch 886/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.2416 - total_train_reward: -1540.9514\n",
      "Epoch 887/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.3330 - total_train_reward: -1734.3864\n",
      "Epoch 888/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.7553 - total_train_reward: -1046.2572\n",
      "Epoch 889/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.2108 - total_train_reward: -1611.1260\n",
      "Epoch 890/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.5719 - total_train_reward: -1280.6465\n",
      "Epoch 891/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.1481 - total_train_reward: -845.5796\n",
      "Epoch 892/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5441 - total_train_reward: -1011.9164\n",
      "Epoch 893/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8414 - total_train_reward: -863.2291\n",
      "Epoch 894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7475 - total_train_reward: -626.9102\n",
      "Epoch 895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0863 - total_train_reward: -1074.9025\n",
      "Epoch 896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3770 - total_train_reward: -995.4323\n",
      "Epoch 897/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9695 - total_train_reward: -1437.1674\n",
      "Epoch 898/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.7445 - total_train_reward: -1159.5109\n",
      "Epoch 899/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0698 - total_train_reward: -504.9782\n",
      "Epoch 900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3033 - total_train_reward: -747.2831\n",
      "Epoch 901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1141 - total_train_reward: -1755.7936\n",
      "Epoch 902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6417 - total_train_reward: -1500.7361\n",
      "Epoch 903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7643 - total_train_reward: -1321.4767\n",
      "Epoch 904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3911 - total_train_reward: -761.8804\n",
      "Epoch 905/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.2128 - total_train_reward: -1786.2072\n",
      "Epoch 906/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.1373 - total_train_reward: -963.5912\n",
      "Epoch 907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8992 - total_train_reward: -1055.2349\n",
      "Epoch 908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2780 - total_train_reward: -1495.7941\n",
      "Epoch 909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1129 - total_train_reward: -862.6879\n",
      "Epoch 910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2740 - total_train_reward: -1737.2996\n",
      "Epoch 911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7548 - total_train_reward: -826.5636\n",
      "Epoch 912/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9424 - total_train_reward: -1168.7541\n",
      "Epoch 913/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7216 - total_train_reward: -1063.2651\n",
      "Epoch 914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6257 - total_train_reward: -901.3660\n",
      "Epoch 915/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.7714 - total_train_reward: -1368.8676\n",
      "Epoch 916/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.7072 - total_train_reward: -1208.1542\n",
      "Epoch 917/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2059 - total_train_reward: -1763.6534\n",
      "Epoch 918/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.6712 - total_train_reward: -1673.0662\n",
      "Epoch 919/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1686 - total_train_reward: -796.8613\n",
      "Epoch 920/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.9152 - total_train_reward: -875.1848\n",
      "Epoch 921/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.4801 - total_train_reward: -1177.6966\n",
      "Epoch 922/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.1426 - total_train_reward: -1462.5845\n",
      "Epoch 923/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8148 - total_train_reward: -1285.4588\n",
      "Epoch 924/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.2880 - total_train_reward: -767.2582\n",
      "Epoch 925/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.5346 - total_train_reward: -926.3197\n",
      "Epoch 926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3722 - total_train_reward: -857.7168\n",
      "Epoch 927/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8395 - total_train_reward: -1192.3213\n",
      "Epoch 928/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.1973 - total_train_reward: -907.9364\n",
      "Epoch 929/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.1613 - total_train_reward: -1061.4714\n",
      "Epoch 930/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0460 - total_train_reward: -788.5676\n",
      "Epoch 931/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.4962 - total_train_reward: -1677.8633\n",
      "Epoch 932/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.4732 - total_train_reward: -1163.3962\n",
      "Epoch 933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2959 - total_train_reward: -1381.7995\n",
      "Epoch 934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3672 - total_train_reward: -1176.3896\n",
      "Epoch 935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4366 - total_train_reward: -862.0288\n",
      "Epoch 936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0357 - total_train_reward: -1339.8329\n",
      "Epoch 937/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8271 - total_train_reward: -1329.3326\n",
      "Epoch 938/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.6043 - total_train_reward: -1411.3723\n",
      "Epoch 939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4988 - total_train_reward: -746.1088\n",
      "Epoch 940/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.3155 - total_train_reward: -1241.9000\n",
      "Epoch 941/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3404 - total_train_reward: -1576.8537\n",
      "Epoch 942/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.7632 - total_train_reward: -969.5547\n",
      "Epoch 943/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.0522 - total_train_reward: -1825.3984\n",
      "Epoch 944/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.6733 - total_train_reward: -1301.1230\n",
      "Epoch 945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1370 - total_train_reward: -1553.1932\n",
      "Epoch 946/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.5857 - total_train_reward: -1079.9997\n",
      "Epoch 947/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1386 - total_train_reward: -1393.6435\n",
      "Epoch 948/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1788 - total_train_reward: -1830.7571\n",
      "Epoch 949/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.1033 - total_train_reward: -1388.0820\n",
      "Epoch 950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8370 - total_train_reward: -1209.7913\n",
      "Epoch 951/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.7411 - total_train_reward: -969.6351\n",
      "Epoch 952/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7605 - total_train_reward: -1073.5672\n",
      "Epoch 953/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0972 - total_train_reward: -1157.0513\n",
      "Epoch 954/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3137 - total_train_reward: -1710.0113\n",
      "Epoch 955/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6441 - total_train_reward: -1788.2851\n",
      "Epoch 956/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.5109 - total_train_reward: -920.1168\n",
      "Epoch 957/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.1053 - total_train_reward: -1428.5527\n",
      "Epoch 958/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.7969 - total_train_reward: -746.7132\n",
      "Epoch 959/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8066 - total_train_reward: -1277.7037\n",
      "Epoch 960/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8995 - total_train_reward: -1887.1787\n",
      "Epoch 961/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.7945 - total_train_reward: -1566.9848\n",
      "Epoch 962/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.3905 - total_train_reward: -904.6344\n",
      "Epoch 963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0221 - total_train_reward: -1827.6814\n",
      "Epoch 964/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.7486 - total_train_reward: -960.2875\n",
      "Epoch 965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6732 - total_train_reward: -1387.2060\n",
      "Epoch 966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0094 - total_train_reward: -1348.0572\n",
      "Epoch 967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8328 - total_train_reward: -979.1112\n",
      "Epoch 968/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.3096 - total_train_reward: -1293.8415\n",
      "Epoch 969/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3866 - total_train_reward: -1250.6674\n",
      "Epoch 970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6492 - total_train_reward: -1059.6309\n",
      "Epoch 971/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.4262 - total_train_reward: -1701.5235\n",
      "Epoch 972/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.0861 - total_train_reward: -1187.2390\n",
      "Epoch 973/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6672 - total_train_reward: -795.5451\n",
      "Epoch 974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5591 - total_train_reward: -1173.3453\n",
      "Epoch 975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3124 - total_train_reward: -970.6879\n",
      "Epoch 976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7536 - total_train_reward: -1674.8096\n",
      "Epoch 977/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1993 - total_train_reward: -1171.3176\n",
      "Epoch 978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9528 - total_train_reward: -1349.5481\n",
      "Epoch 979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6547 - total_train_reward: -1654.5620\n",
      "Epoch 980/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1970 - total_train_reward: -623.5548\n",
      "Epoch 981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7498 - total_train_reward: -1422.7528\n",
      "Epoch 982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7655 - total_train_reward: -1695.2795\n",
      "Epoch 983/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.3094 - total_train_reward: -1748.0710\n",
      "Epoch 984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8049 - total_train_reward: -1623.5037\n",
      "Epoch 985/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.1141 - total_train_reward: -1854.3797\n",
      "Epoch 986/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.7870 - total_train_reward: -1069.5596\n",
      "Epoch 987/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.6513 - total_train_reward: -1283.5286\n",
      "Epoch 988/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5375 - total_train_reward: -1316.8311\n",
      "Epoch 989/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.8404 - total_train_reward: -747.5322\n",
      "Epoch 990/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.1893 - total_train_reward: -969.9599\n",
      "Epoch 991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6188 - total_train_reward: -1170.1795\n",
      "Epoch 992/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9089 - total_train_reward: -1179.3540\n",
      "Epoch 993/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.2787 - total_train_reward: -1094.3152\n",
      "Epoch 994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5223 - total_train_reward: -747.5607\n",
      "Epoch 995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8576 - total_train_reward: -1289.2468\n",
      "Epoch 996/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.9919 - total_train_reward: -1158.5713\n",
      "Epoch 997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1282 - total_train_reward: -1461.4879\n",
      "Epoch 998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6324 - total_train_reward: -1138.4060\n",
      "Epoch 999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1301 - total_train_reward: -1169.3344\n",
      "Epoch 1000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2996 - total_train_reward: -902.7626\n",
      "Epoch 1001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9056 - total_train_reward: -965.2829\n",
      "Epoch 1002/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.0032 - total_train_reward: -1455.9018\n",
      "Epoch 1003/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3063 - total_train_reward: -1245.9078\n",
      "Epoch 1004/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8729 - total_train_reward: -1689.4041\n",
      "Epoch 1005/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.6470 - total_train_reward: -1325.5406\n",
      "Epoch 1006/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.2545 - total_train_reward: -895.5822\n",
      "Epoch 1007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7097 - total_train_reward: -1568.8418\n",
      "Epoch 1008/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.7494 - total_train_reward: -1338.1265\n",
      "Epoch 1009/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.5900 - total_train_reward: -1184.4482\n",
      "Epoch 1010/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.7702 - total_train_reward: -1444.7522\n",
      "Epoch 1011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0472 - total_train_reward: -1491.5074\n",
      "Epoch 1012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9703 - total_train_reward: -1294.9733\n",
      "Epoch 1013/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.4734 - total_train_reward: -1072.3623\n",
      "Epoch 1014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4239 - total_train_reward: -1471.0821\n",
      "Epoch 1015/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5602 - total_train_reward: -869.4353\n",
      "Epoch 1016/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 10.1654 - total_train_reward: -770.5218\n",
      "Epoch 1017/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.0062 - total_train_reward: -1172.5087\n",
      "Epoch 1018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8910 - total_train_reward: -1803.1036\n",
      "Epoch 1019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0569 - total_train_reward: -1182.3820\n",
      "Epoch 1020/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4091 - total_train_reward: -985.1729\n",
      "Epoch 1021/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.0671 - total_train_reward: -626.7071\n",
      "Epoch 1022/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.3205 - total_train_reward: -1816.3329\n",
      "Epoch 1023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1697 - total_train_reward: -1480.1087\n",
      "Epoch 1024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7425 - total_train_reward: -1299.4192\n",
      "Epoch 1025/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.9067 - total_train_reward: -1069.3105\n",
      "Epoch 1026/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.3671 - total_train_reward: -718.7229\n",
      "Epoch 1027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3649 - total_train_reward: -1279.7136\n",
      "Epoch 1028/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.6551 - total_train_reward: -982.6157\n",
      "Epoch 1029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5775 - total_train_reward: -881.7048\n",
      "Epoch 1030/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.4755 - total_train_reward: -1172.1969\n",
      "Epoch 1031/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.4493 - total_train_reward: -1638.7784\n",
      "Epoch 1032/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.0031 - total_train_reward: -1227.4995\n",
      "Epoch 1033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7653 - total_train_reward: -1487.8439\n",
      "Epoch 1034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7022 - total_train_reward: -1171.4543\n",
      "Epoch 1035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1739 - total_train_reward: -1693.0828\n",
      "Epoch 1036/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.8151 - total_train_reward: -747.3032\n",
      "Epoch 1037/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.0993 - total_train_reward: -1472.5855\n",
      "Epoch 1038/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8320 - total_train_reward: -1760.7335\n",
      "Epoch 1039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7877 - total_train_reward: -1750.4912\n",
      "Epoch 1040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1072 - total_train_reward: -625.5752\n",
      "Epoch 1041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6161 - total_train_reward: -1116.3635\n",
      "Epoch 1042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5730 - total_train_reward: -1214.9378\n",
      "Epoch 1043/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0875 - total_train_reward: -626.7532\n",
      "Epoch 1044/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.4506 - total_train_reward: -1287.7899\n",
      "Epoch 1045/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.2952 - total_train_reward: -1478.5765\n",
      "Epoch 1046/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8316 - total_train_reward: -1069.9559\n",
      "Epoch 1047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3053 - total_train_reward: -1164.2814\n",
      "Epoch 1048/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.7120 - total_train_reward: -765.2954\n",
      "Epoch 1049/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.6826 - total_train_reward: -1774.8775\n",
      "Epoch 1050/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.1033 - total_train_reward: -1530.4231\n",
      "Epoch 1051/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.2887 - total_train_reward: -1285.3791\n",
      "Epoch 1052/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.2302 - total_train_reward: -1799.8054\n",
      "Epoch 1053/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9719 - total_train_reward: -1601.7132\n",
      "Epoch 1054/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8670 - total_train_reward: -864.0721\n",
      "Epoch 1055/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.0949 - total_train_reward: -1197.0566\n",
      "Epoch 1056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7629 - total_train_reward: -1055.9219\n",
      "Epoch 1057/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.2078 - total_train_reward: -1009.3776\n",
      "Epoch 1058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3405 - total_train_reward: -968.5907\n",
      "Epoch 1059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3404 - total_train_reward: -1743.4506\n",
      "Epoch 1060/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.9609 - total_train_reward: -627.5470\n",
      "Epoch 1061/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.9969 - total_train_reward: -651.8576\n",
      "Epoch 1062/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.4475 - total_train_reward: -1291.1423\n",
      "Epoch 1063/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2323 - total_train_reward: -1206.9423\n",
      "Epoch 1064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4252 - total_train_reward: -1073.4163\n",
      "Epoch 1065/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.0688 - total_train_reward: -1670.4857\n",
      "Epoch 1066/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5879 - total_train_reward: -1262.7245\n",
      "Epoch 1067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8321 - total_train_reward: -1379.5687\n",
      "Epoch 1068/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1345 - total_train_reward: -1631.7221\n",
      "Epoch 1069/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.4775 - total_train_reward: -1634.5394\n",
      "Epoch 1070/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.1357 - total_train_reward: -970.3106\n",
      "Epoch 1071/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.3686 - total_train_reward: -970.2186\n",
      "Epoch 1072/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1618 - total_train_reward: -1094.7287\n",
      "Epoch 1073/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.4911 - total_train_reward: -1838.7761\n",
      "Epoch 1074/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.0570 - total_train_reward: -1160.2585\n",
      "Epoch 1075/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.9831 - total_train_reward: -1763.3253\n",
      "Epoch 1076/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.1248 - total_train_reward: -728.8879\n",
      "Epoch 1077/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.8749 - total_train_reward: -747.6600\n",
      "Epoch 1078/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.5952 - total_train_reward: -1525.1795\n",
      "Epoch 1079/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9265 - total_train_reward: -1327.8662\n",
      "Epoch 1080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7871 - total_train_reward: -862.7518\n",
      "Epoch 1081/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3441 - total_train_reward: -765.3042\n",
      "Epoch 1082/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8117 - total_train_reward: -1287.3780\n",
      "Epoch 1083/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.7571 - total_train_reward: -867.6504\n",
      "Epoch 1084/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.7279 - total_train_reward: -632.9546\n",
      "Epoch 1085/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.8324 - total_train_reward: -979.3849\n",
      "Epoch 1086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1408 - total_train_reward: -1591.5039\n",
      "Epoch 1087/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.4611 - total_train_reward: -1703.4332\n",
      "Epoch 1088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7443 - total_train_reward: -1567.6314\n",
      "Epoch 1089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5855 - total_train_reward: -1069.7017\n",
      "Epoch 1090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4464 - total_train_reward: -1487.7950\n",
      "Epoch 1091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8279 - total_train_reward: -1830.2308\n",
      "Epoch 1092/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.7883 - total_train_reward: -1873.7119\n",
      "Epoch 1093/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.1450 - total_train_reward: -1121.4854\n",
      "Epoch 1094/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6232 - total_train_reward: -1556.2905\n",
      "Epoch 1095/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.3794 - total_train_reward: -1162.1472\n",
      "Epoch 1096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.7825 - total_train_reward: -1663.1833\n",
      "Epoch 1097/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0749 - total_train_reward: -861.2503\n",
      "Epoch 1098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1280 - total_train_reward: -1549.4814\n",
      "Epoch 1099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5118 - total_train_reward: -839.1009\n",
      "Epoch 1100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2736 - total_train_reward: -1185.3980\n",
      "Epoch 1101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5355 - total_train_reward: -860.2651\n",
      "Epoch 1102/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.2340 - total_train_reward: -1436.0854\n",
      "Epoch 1103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8691 - total_train_reward: -1674.4104\n",
      "Epoch 1104/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1948 - total_train_reward: -1192.3446\n",
      "Epoch 1105/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4815 - total_train_reward: -1075.3540\n",
      "Epoch 1106/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.4812 - total_train_reward: -867.8151\n",
      "Epoch 1107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9486 - total_train_reward: -1175.2419\n",
      "Epoch 1108/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.5800 - total_train_reward: -1791.3936\n",
      "Epoch 1109/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.0892 - total_train_reward: -1281.9560\n",
      "Epoch 1110/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8817 - total_train_reward: -1095.3384\n",
      "Epoch 1111/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.2592 - total_train_reward: -1849.3152\n",
      "Epoch 1112/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2077 - total_train_reward: -1063.8450\n",
      "Epoch 1113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9175 - total_train_reward: -1361.3096\n",
      "Epoch 1114/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.7981 - total_train_reward: -1766.6048\n",
      "Epoch 1115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8061 - total_train_reward: -747.2047\n",
      "Epoch 1116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6860 - total_train_reward: -872.8346\n",
      "Epoch 1117/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.8658 - total_train_reward: -1773.3437\n",
      "Epoch 1118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8929 - total_train_reward: -1163.6412\n",
      "Epoch 1119/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.2991 - total_train_reward: -1309.3866\n",
      "Epoch 1120/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.1876 - total_train_reward: -1204.7956\n",
      "Epoch 1121/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6573 - total_train_reward: -1481.6969\n",
      "Epoch 1122/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.1878 - total_train_reward: -1229.4202\n",
      "Epoch 1123/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.9534 - total_train_reward: -1860.5604\n",
      "Epoch 1124/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4521 - total_train_reward: -1744.6868\n",
      "Epoch 1125/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.4290 - total_train_reward: -1484.9471\n",
      "Epoch 1126/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8701 - total_train_reward: -1294.4229\n",
      "Epoch 1127/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.3093 - total_train_reward: -897.1226\n",
      "Epoch 1128/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.3586 - total_train_reward: -758.3291\n",
      "Epoch 1129/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.8476 - total_train_reward: -973.4509\n",
      "Epoch 1130/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.2973 - total_train_reward: -700.5356\n",
      "Epoch 1131/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5518 - total_train_reward: -763.4376\n",
      "Epoch 1132/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.2837 - total_train_reward: -1068.0934\n",
      "Epoch 1133/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1908 - total_train_reward: -864.9801\n",
      "Epoch 1134/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.4259 - total_train_reward: -1304.8976\n",
      "Epoch 1135/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.2136 - total_train_reward: -782.2583\n",
      "Epoch 1136/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9058 - total_train_reward: -622.5540\n",
      "Epoch 1137/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3784 - total_train_reward: -745.3764\n",
      "Epoch 1138/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.6285 - total_train_reward: -747.6014\n",
      "Epoch 1139/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0677 - total_train_reward: -651.4227\n",
      "Epoch 1140/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.4913 - total_train_reward: -1514.1166\n",
      "Epoch 1141/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3712 - total_train_reward: -837.3512\n",
      "Epoch 1142/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5464 - total_train_reward: -970.3485\n",
      "Epoch 1143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0707 - total_train_reward: -1183.4840\n",
      "Epoch 1144/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7438 - total_train_reward: -768.6207\n",
      "Epoch 1145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1651 - total_train_reward: -1162.1173\n",
      "Epoch 1146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5952 - total_train_reward: -1305.1637\n",
      "Epoch 1147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4102 - total_train_reward: -1045.4891\n",
      "Epoch 1148/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.9019e-04 - total_train_reward: -1910.2902\n",
      "Epoch 1149/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4176 - total_train_reward: -1813.2890\n",
      "Epoch 1150/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.7700 - total_train_reward: -1160.3275\n",
      "Epoch 1151/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.6469 - total_train_reward: -1382.9119\n",
      "Epoch 1152/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6440 - total_train_reward: -1375.4476\n",
      "Epoch 1153/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8795 - total_train_reward: -1505.0837\n",
      "Epoch 1154/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.9655 - total_train_reward: -1179.7346\n",
      "Epoch 1155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5243 - total_train_reward: -1415.9384\n",
      "Epoch 1156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9456 - total_train_reward: -1368.4102\n",
      "Epoch 1157/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.8749 - total_train_reward: -1333.2151\n",
      "Epoch 1158/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8969 - total_train_reward: -732.5323\n",
      "Epoch 1159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2985 - total_train_reward: -501.2980\n",
      "Epoch 1160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1502 - total_train_reward: -1520.8002\n",
      "Epoch 1161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6707 - total_train_reward: -1378.7241\n",
      "Epoch 1162/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.1396 - total_train_reward: -1859.1986\n",
      "Epoch 1163/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9609 - total_train_reward: -1293.9684\n",
      "Epoch 1164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9748 - total_train_reward: -749.0893\n",
      "Epoch 1165/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.4707 - total_train_reward: -749.0514\n",
      "Epoch 1166/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9795 - total_train_reward: -743.6580\n",
      "Epoch 1167/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -17.2048 - total_train_reward: -1679.4361\n",
      "Epoch 1168/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.0987 - total_train_reward: -630.1904\n",
      "Epoch 1169/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.7970 - total_train_reward: -1535.1351\n",
      "Epoch 1170/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.6278 - total_train_reward: -846.2281\n",
      "Epoch 1171/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.2100 - total_train_reward: -763.5846\n",
      "Epoch 1172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7035 - total_train_reward: -1444.1066\n",
      "Epoch 1173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4791 - total_train_reward: -903.5210\n",
      "Epoch 1174/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5623 - total_train_reward: -1154.1950\n",
      "Epoch 1175/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6012 - total_train_reward: -1383.8444\n",
      "Epoch 1176/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.4118 - total_train_reward: -821.9537\n",
      "Epoch 1177/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.2160 - total_train_reward: -1829.3744\n",
      "Epoch 1178/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.7470 - total_train_reward: -1278.6081\n",
      "Epoch 1179/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9922 - total_train_reward: -763.4015\n",
      "Epoch 1180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5576 - total_train_reward: -1163.4231\n",
      "Epoch 1181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0060 - total_train_reward: -1188.9460\n",
      "Epoch 1182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1164 - total_train_reward: -752.0521\n",
      "Epoch 1183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6385 - total_train_reward: -1305.2930\n",
      "Epoch 1184/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.1991 - total_train_reward: -1220.8249\n",
      "Epoch 1185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7573 - total_train_reward: -1077.8477\n",
      "Epoch 1186/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.0692 - total_train_reward: -1613.1843\n",
      "Epoch 1187/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.4875 - total_train_reward: -1176.2420\n",
      "Epoch 1188/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.5814 - total_train_reward: -894.8806\n",
      "Epoch 1189/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9425 - total_train_reward: -1070.9709\n",
      "Epoch 1190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2828 - total_train_reward: -1371.1566\n",
      "Epoch 1191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1242 - total_train_reward: -1313.1008\n",
      "Epoch 1192/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.4644 - total_train_reward: -1310.1813\n",
      "Epoch 1193/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7197 - total_train_reward: -1065.6618\n",
      "Epoch 1194/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 8.2077 - total_train_reward: -631.7760\n",
      "Epoch 1195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9612 - total_train_reward: -1276.5668\n",
      "Epoch 1196/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.7640 - total_train_reward: -1231.1893\n",
      "Epoch 1197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3648 - total_train_reward: -1576.8245\n",
      "Epoch 1198/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7409 - total_train_reward: -1057.5494\n",
      "Epoch 1199/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2238 - total_train_reward: -1289.6299\n",
      "Epoch 1200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1774 - total_train_reward: -1785.8985\n",
      "Epoch 1201/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.1354 - total_train_reward: -1284.1065\n",
      "Epoch 1202/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.1139 - total_train_reward: -1203.6257\n",
      "Epoch 1203/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.2006 - total_train_reward: -1614.7598\n",
      "Epoch 1204/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.5925 - total_train_reward: -969.9242\n",
      "Epoch 1205/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0701 - total_train_reward: -1279.3414\n",
      "Epoch 1206/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8944 - total_train_reward: -1167.1401\n",
      "Epoch 1207/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.9998 - total_train_reward: -1675.2463\n",
      "Epoch 1208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1992 - total_train_reward: -1148.4488\n",
      "Epoch 1209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4152 - total_train_reward: -744.2220\n",
      "Epoch 1210/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.6433 - total_train_reward: -1494.0199\n",
      "Epoch 1211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8110 - total_train_reward: -1873.6730\n",
      "Epoch 1212/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.0968 - total_train_reward: -945.7432\n",
      "Epoch 1213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9714 - total_train_reward: -1879.1753\n",
      "Epoch 1214/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0596 - total_train_reward: -624.9600\n",
      "Epoch 1215/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8446 - total_train_reward: -748.4945\n",
      "Epoch 1216/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.0502 - total_train_reward: -1822.7626\n",
      "Epoch 1217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.9035 - total_train_reward: -1638.3417\n",
      "Epoch 1218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0275 - total_train_reward: -1679.0391\n",
      "Epoch 1219/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5266 - total_train_reward: -627.1413\n",
      "Epoch 1220/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.5697 - total_train_reward: -925.6464\n",
      "Epoch 1221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7908 - total_train_reward: -968.4954\n",
      "Epoch 1222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0265 - total_train_reward: -1168.5486\n",
      "Epoch 1223/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3633 - total_train_reward: -981.1280\n",
      "Epoch 1224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3571 - total_train_reward: -1115.2059\n",
      "Epoch 1225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5232 - total_train_reward: -630.1506\n",
      "Epoch 1226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3130 - total_train_reward: -969.8052\n",
      "Epoch 1227/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5754 - total_train_reward: -1744.6404\n",
      "Epoch 1228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1101 - total_train_reward: -1317.0889\n",
      "Epoch 1229/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.2678 - total_train_reward: -1166.6153\n",
      "Epoch 1230/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0688 - total_train_reward: -1478.6921\n",
      "Epoch 1231/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8005 - total_train_reward: -737.9270\n",
      "Epoch 1232/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.0131 - total_train_reward: -710.9816\n",
      "Epoch 1233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5316 - total_train_reward: -1496.0671\n",
      "Epoch 1234/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8114 - total_train_reward: -979.9999\n",
      "Epoch 1235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1890 - total_train_reward: -1569.6747\n",
      "Epoch 1236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1672 - total_train_reward: -633.6907\n",
      "Epoch 1237/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.4992 - total_train_reward: -970.1068\n",
      "Epoch 1238/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.4854 - total_train_reward: -1746.3111\n",
      "Epoch 1239/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0607 - total_train_reward: -1078.8304\n",
      "Epoch 1240/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9314 - total_train_reward: -1195.6911\n",
      "Epoch 1241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2843 - total_train_reward: -1643.2392\n",
      "Epoch 1242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4822 - total_train_reward: -619.9178\n",
      "Epoch 1243/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.5283 - total_train_reward: -1278.9432\n",
      "Epoch 1244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8214 - total_train_reward: -1618.1235\n",
      "Epoch 1245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9823 - total_train_reward: -862.7418\n",
      "Epoch 1246/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.6551 - total_train_reward: -1368.2657\n",
      "Epoch 1247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1753 - total_train_reward: -965.8372\n",
      "Epoch 1248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5956 - total_train_reward: -1800.1847\n",
      "Epoch 1249/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7854 - total_train_reward: -1779.6778\n",
      "Epoch 1250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6073 - total_train_reward: -970.2593\n",
      "Epoch 1251/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.2643 - total_train_reward: -694.2050\n",
      "Epoch 1252/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.4260 - total_train_reward: -1285.5943\n",
      "Epoch 1253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0144 - total_train_reward: -763.7871\n",
      "Epoch 1254/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3694 - total_train_reward: -1286.8858\n",
      "Epoch 1255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6049 - total_train_reward: -1024.9675\n",
      "Epoch 1256/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.6766 - total_train_reward: -981.6806\n",
      "Epoch 1257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9403 - total_train_reward: -1310.0567\n",
      "Epoch 1258/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9037 - total_train_reward: -1389.9424\n",
      "Epoch 1259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6330 - total_train_reward: -628.5547\n",
      "Epoch 1260/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9385 - total_train_reward: -1170.9364\n",
      "Epoch 1261/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.8855 - total_train_reward: -748.2759\n",
      "Epoch 1262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4268 - total_train_reward: -641.8630\n",
      "Epoch 1263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3111 - total_train_reward: -1306.3883\n",
      "Epoch 1264/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6394 - total_train_reward: -1300.8189\n",
      "Epoch 1265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2093 - total_train_reward: -1890.5350\n",
      "Epoch 1266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9295 - total_train_reward: -626.5106\n",
      "Epoch 1267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7552 - total_train_reward: -1177.2534\n",
      "Epoch 1268/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.9434 - total_train_reward: -1070.1308\n",
      "Epoch 1269/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.0246 - total_train_reward: -1007.1693\n",
      "Epoch 1270/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.2367 - total_train_reward: -1542.1738\n",
      "Epoch 1271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6203 - total_train_reward: -1774.4169\n",
      "Epoch 1272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9003 - total_train_reward: -725.0525\n",
      "Epoch 1273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5923 - total_train_reward: -1837.8096\n",
      "Epoch 1274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1692 - total_train_reward: -966.6339\n",
      "Epoch 1275/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6625 - total_train_reward: -1179.6269\n",
      "Epoch 1276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4853 - total_train_reward: -751.0298\n",
      "Epoch 1277/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.3600 - total_train_reward: -508.9241\n",
      "Epoch 1278/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.0497 - total_train_reward: -1477.7788\n",
      "Epoch 1279/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3822 - total_train_reward: -1816.5283\n",
      "Epoch 1280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8754 - total_train_reward: -1562.2995\n",
      "Epoch 1281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6731 - total_train_reward: -641.2769\n",
      "Epoch 1282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5609 - total_train_reward: -1703.1989\n",
      "Epoch 1283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8324 - total_train_reward: -1196.0501\n",
      "Epoch 1284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3046 - total_train_reward: -1222.2966\n",
      "Epoch 1285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4173 - total_train_reward: -1706.2401\n",
      "Epoch 1286/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.2644 - total_train_reward: -1286.9392\n",
      "Epoch 1287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8955 - total_train_reward: -802.6538\n",
      "Epoch 1288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8387 - total_train_reward: -1195.7511\n",
      "Epoch 1289/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5173 - total_train_reward: -511.3320\n",
      "Epoch 1290/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.1625 - total_train_reward: -1646.9144\n",
      "Epoch 1291/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.3748 - total_train_reward: -634.5577\n",
      "Epoch 1292/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.9046 - total_train_reward: -663.1510\n",
      "Epoch 1293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7073 - total_train_reward: -1170.7358\n",
      "Epoch 1294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7525 - total_train_reward: -1094.3945\n",
      "Epoch 1295/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3523 - total_train_reward: -1882.9260\n",
      "Epoch 1296/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.6404 - total_train_reward: -1175.3776\n",
      "Epoch 1297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7782 - total_train_reward: -1074.2912\n",
      "Epoch 1298/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.0916 - total_train_reward: -878.1134\n",
      "Epoch 1299/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -16.1293 - total_train_reward: -1703.5000\n",
      "Epoch 1300/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.3730 - total_train_reward: -987.8791\n",
      "Epoch 1301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8404 - total_train_reward: -1744.5280\n",
      "Epoch 1302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1101 - total_train_reward: -840.6166\n",
      "Epoch 1303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.0199 - total_train_reward: -1433.5695\n",
      "Epoch 1304/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.7030 - total_train_reward: -1167.6251\n",
      "Epoch 1305/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.5386 - total_train_reward: -1160.4582\n",
      "Epoch 1306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2340 - total_train_reward: -947.4826\n",
      "Epoch 1307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9131 - total_train_reward: -1172.9011\n",
      "Epoch 1308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8273 - total_train_reward: -1235.4727\n",
      "Epoch 1309/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.6244 - total_train_reward: -1142.2363\n",
      "Epoch 1310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6183 - total_train_reward: -1782.5544\n",
      "Epoch 1311/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.5687 - total_train_reward: -1427.2363\n",
      "Epoch 1312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9089 - total_train_reward: -1278.9703\n",
      "Epoch 1313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3828 - total_train_reward: -632.1815\n",
      "Epoch 1314/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9778 - total_train_reward: -1694.8348\n",
      "Epoch 1315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8228 - total_train_reward: -1217.4314\n",
      "Epoch 1316/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.3568 - total_train_reward: -1355.4431\n",
      "Epoch 1317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7652 - total_train_reward: -1764.5618\n",
      "Epoch 1318/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.2936 - total_train_reward: -1708.7014\n",
      "Epoch 1319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1488 - total_train_reward: -1466.2993\n",
      "Epoch 1320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5086 - total_train_reward: -855.4652\n",
      "Epoch 1321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1886 - total_train_reward: -1899.1248\n",
      "Epoch 1322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1029 - total_train_reward: -1391.2576\n",
      "Epoch 1323/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.4652 - total_train_reward: -1647.7960\n",
      "Epoch 1324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9459 - total_train_reward: -1612.1218\n",
      "Epoch 1325/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4994 - total_train_reward: -1679.3535\n",
      "Epoch 1326/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.7336 - total_train_reward: -737.0408\n",
      "Epoch 1327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6240 - total_train_reward: -735.0555\n",
      "Epoch 1328/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.7142 - total_train_reward: -1680.8217\n",
      "Epoch 1329/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.0787 - total_train_reward: -1577.2206\n",
      "Epoch 1330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8855 - total_train_reward: -966.6910\n",
      "Epoch 1331/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.9106 - total_train_reward: -970.5099\n",
      "Epoch 1332/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.2793 - total_train_reward: -857.9879\n",
      "Epoch 1333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5034 - total_train_reward: -1411.6183\n",
      "Epoch 1334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1833 - total_train_reward: -1479.8526\n",
      "Epoch 1335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2633 - total_train_reward: -1194.9954\n",
      "Epoch 1336/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.7757 - total_train_reward: -1030.3172\n",
      "Epoch 1337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5635 - total_train_reward: -1457.2943\n",
      "Epoch 1338/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0832 - total_train_reward: -1222.7093\n",
      "Epoch 1339/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.9799 - total_train_reward: -1088.6262\n",
      "Epoch 1340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0285 - total_train_reward: -1197.1546\n",
      "Epoch 1341/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.6668 - total_train_reward: -907.7979\n",
      "Epoch 1342/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.2816 - total_train_reward: -1288.2376\n",
      "Epoch 1343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7727 - total_train_reward: -1266.3701\n",
      "Epoch 1344/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.2440 - total_train_reward: -746.5139\n",
      "Epoch 1345/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.4715 - total_train_reward: -1829.9279\n",
      "Epoch 1346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4024 - total_train_reward: -1188.4816\n",
      "Epoch 1347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3622 - total_train_reward: -885.2750\n",
      "Epoch 1348/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8363 - total_train_reward: -1279.7905\n",
      "Epoch 1349/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.0419 - total_train_reward: -747.3721\n",
      "Epoch 1350/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.3642 - total_train_reward: -1320.5267\n",
      "Epoch 1351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8272 - total_train_reward: -1294.8977\n",
      "Epoch 1352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2767 - total_train_reward: -627.3211\n",
      "Epoch 1353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0582 - total_train_reward: -1847.4108\n",
      "Epoch 1354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.6110 - total_train_reward: -1578.3593\n",
      "Epoch 1355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3510 - total_train_reward: -1166.4316\n",
      "Epoch 1356/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.9806 - total_train_reward: -831.6421\n",
      "Epoch 1357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1014 - total_train_reward: -874.9426\n",
      "Epoch 1358/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.1998 - total_train_reward: -1290.4866\n",
      "Epoch 1359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4126 - total_train_reward: -1495.0166\n",
      "Epoch 1360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6550 - total_train_reward: -1060.8211\n",
      "Epoch 1361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0040 - total_train_reward: -862.7267\n",
      "Epoch 1362/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4610 - total_train_reward: -1140.3897\n",
      "Epoch 1363/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.9792 - total_train_reward: -1773.7632\n",
      "Epoch 1364/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.7818 - total_train_reward: -1256.6876\n",
      "Epoch 1365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4631 - total_train_reward: -1634.2960\n",
      "Epoch 1366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7607 - total_train_reward: -801.7778\n",
      "Epoch 1367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3623 - total_train_reward: -750.8650\n",
      "Epoch 1368/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.5010 - total_train_reward: -958.7153\n",
      "Epoch 1369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8043 - total_train_reward: -1104.0054\n",
      "Epoch 1370/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3629 - total_train_reward: -837.2576\n",
      "Epoch 1371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5521 - total_train_reward: -1679.6300\n",
      "Epoch 1372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1525 - total_train_reward: -1560.0476\n",
      "Epoch 1373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5170 - total_train_reward: -1665.5610\n",
      "Epoch 1374/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.5618 - total_train_reward: -1727.9343\n",
      "Epoch 1375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2969 - total_train_reward: -1159.1985\n",
      "Epoch 1376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7288 - total_train_reward: -749.0598\n",
      "Epoch 1377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3629 - total_train_reward: -1299.4872\n",
      "Epoch 1378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1968 - total_train_reward: -862.6509\n",
      "Epoch 1379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7374 - total_train_reward: -1591.0039\n",
      "Epoch 1380/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.8885 - total_train_reward: -1189.2869\n",
      "Epoch 1381/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.3685 - total_train_reward: -970.0156\n",
      "Epoch 1382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0943 - total_train_reward: -975.1834\n",
      "Epoch 1383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0284 - total_train_reward: -737.0435\n",
      "Epoch 1384/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.7655 - total_train_reward: -1461.0740\n",
      "Epoch 1385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0416 - total_train_reward: -1478.1968\n",
      "Epoch 1386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8932 - total_train_reward: -1861.6718\n",
      "Epoch 1387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7292 - total_train_reward: -1161.7034\n",
      "Epoch 1388/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.4603 - total_train_reward: -1291.4329\n",
      "Epoch 1389/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.0693 - total_train_reward: -1186.2490\n",
      "Epoch 1390/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.9124 - total_train_reward: -1494.9714\n",
      "Epoch 1391/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.1654 - total_train_reward: -945.3149\n",
      "Epoch 1392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5415 - total_train_reward: -865.8893\n",
      "Epoch 1393/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -13.8986 - total_train_reward: -1564.8667\n",
      "Epoch 1394/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.5061 - total_train_reward: -1100.7119\n",
      "Epoch 1395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6155 - total_train_reward: -1294.8224\n",
      "Epoch 1396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3235 - total_train_reward: -694.9314\n",
      "Epoch 1397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3001 - total_train_reward: -1420.4277\n",
      "Epoch 1398/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.1214 - total_train_reward: -631.2318\n",
      "Epoch 1399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2740 - total_train_reward: -642.1648\n",
      "Epoch 1400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0576 - total_train_reward: -1075.8104\n",
      "Epoch 1401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4925 - total_train_reward: -1604.3175\n",
      "Epoch 1402/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.8582 - total_train_reward: -1752.9616\n",
      "Epoch 1403/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.8430 - total_train_reward: -1517.3020\n",
      "Epoch 1404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9050 - total_train_reward: -504.1297\n",
      "Epoch 1405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7735 - total_train_reward: -1570.8582\n",
      "Epoch 1406/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -1.5249 - total_train_reward: -1310.9532\n",
      "Epoch 1407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8437 - total_train_reward: -1959.4404\n",
      "Epoch 1408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6083 - total_train_reward: -759.1591\n",
      "Epoch 1409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2411 - total_train_reward: -1499.3020\n",
      "Epoch 1410/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6719 - total_train_reward: -1071.2065\n",
      "Epoch 1411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1802 - total_train_reward: -1826.1459\n",
      "Epoch 1412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8312 - total_train_reward: -1586.2848\n",
      "Epoch 1413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8440 - total_train_reward: -1826.4899\n",
      "Epoch 1414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5474 - total_train_reward: -1064.2681\n",
      "Epoch 1415/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.7038 - total_train_reward: -862.7699\n",
      "Epoch 1416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9462 - total_train_reward: -872.5437\n",
      "Epoch 1417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.5301 - total_train_reward: -1787.0055\n",
      "Epoch 1418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2758 - total_train_reward: -752.3784\n",
      "Epoch 1419/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.3828 - total_train_reward: -1547.8953\n",
      "Epoch 1420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8572 - total_train_reward: -1255.6259\n",
      "Epoch 1421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6723 - total_train_reward: -1189.6385\n",
      "Epoch 1422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5769 - total_train_reward: -1799.6727\n",
      "Epoch 1423/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.6023 - total_train_reward: -1409.6598\n",
      "Epoch 1424/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.6113 - total_train_reward: -1548.0764\n",
      "Epoch 1425/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.1268 - total_train_reward: -970.0116\n",
      "Epoch 1426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1251 - total_train_reward: -1510.9014\n",
      "Epoch 1427/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.3731 - total_train_reward: -863.1239\n",
      "Epoch 1428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9016 - total_train_reward: -966.5704\n",
      "Epoch 1429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5450 - total_train_reward: -747.5518\n",
      "Epoch 1430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9743 - total_train_reward: -1081.5117\n",
      "Epoch 1431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4540 - total_train_reward: -1289.1271\n",
      "Epoch 1432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3007 - total_train_reward: -1865.9972\n",
      "Epoch 1433/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -14.6714 - total_train_reward: -1707.8460\n",
      "Epoch 1434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4630 - total_train_reward: -1190.0614\n",
      "Epoch 1435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8208 - total_train_reward: -1597.7021\n",
      "Epoch 1436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4380 - total_train_reward: -746.8134\n",
      "Epoch 1437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7800 - total_train_reward: -1410.3915\n",
      "Epoch 1438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0751 - total_train_reward: -1268.5532\n",
      "Epoch 1439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8956 - total_train_reward: -780.7867\n",
      "Epoch 1440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7338 - total_train_reward: -728.6911\n",
      "Epoch 1441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7986 - total_train_reward: -1314.9684\n",
      "Epoch 1442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0265 - total_train_reward: -965.3983\n",
      "Epoch 1443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4155 - total_train_reward: -1167.2641\n",
      "Epoch 1444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7903 - total_train_reward: -1427.8114\n",
      "Epoch 1445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8547 - total_train_reward: -1625.6390\n",
      "Epoch 1446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6508 - total_train_reward: -1171.8587\n",
      "Epoch 1447/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2940 - total_train_reward: -1191.6667\n",
      "Epoch 1448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0049 - total_train_reward: -1051.1231\n",
      "Epoch 1449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3994 - total_train_reward: -883.2664\n",
      "Epoch 1450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1781 - total_train_reward: -748.9214\n",
      "Epoch 1451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3993 - total_train_reward: -810.6872\n",
      "Epoch 1452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0722 - total_train_reward: -860.5192\n",
      "Epoch 1453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6828 - total_train_reward: -1156.3852\n",
      "Epoch 1454/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.0328 - total_train_reward: -1883.5427\n",
      "Epoch 1455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0200 - total_train_reward: -1829.7976\n",
      "Epoch 1456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9317 - total_train_reward: -1171.4726\n",
      "Epoch 1457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9619 - total_train_reward: -1156.1287\n",
      "Epoch 1458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2806 - total_train_reward: -1349.6669\n",
      "Epoch 1459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5486 - total_train_reward: -1214.9819\n",
      "Epoch 1460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9230 - total_train_reward: -1595.1519\n",
      "Epoch 1461/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.7223 - total_train_reward: -879.1590\n",
      "Epoch 1462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2979 - total_train_reward: -1070.1712\n",
      "Epoch 1463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5950 - total_train_reward: -789.1774\n",
      "Epoch 1464/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.8424 - total_train_reward: -1258.9612\n",
      "Epoch 1465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0132 - total_train_reward: -1189.9023\n",
      "Epoch 1466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5086 - total_train_reward: -1735.7974\n",
      "Epoch 1467/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.4667 - total_train_reward: -970.1336\n",
      "Epoch 1468/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.9119 - total_train_reward: -1203.9832\n",
      "Epoch 1469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7685 - total_train_reward: -1168.2986\n",
      "Epoch 1470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3539 - total_train_reward: -837.6394\n",
      "Epoch 1471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7806 - total_train_reward: -1157.9488\n",
      "Epoch 1472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1251 - total_train_reward: -972.3725\n",
      "Epoch 1473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3702 - total_train_reward: -1108.4590\n",
      "Epoch 1474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6617 - total_train_reward: -1165.4543\n",
      "Epoch 1475/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.3656 - total_train_reward: -969.8714\n",
      "Epoch 1476/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.5712 - total_train_reward: -1374.4942\n",
      "Epoch 1477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5403 - total_train_reward: -1849.1311\n",
      "Epoch 1478/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.1573 - total_train_reward: -1602.3521\n",
      "Epoch 1479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0939 - total_train_reward: -1737.7652\n",
      "Epoch 1480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4412 - total_train_reward: -762.7560\n",
      "Epoch 1481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2875 - total_train_reward: -1307.4916\n",
      "Epoch 1482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4393 - total_train_reward: -1402.6228\n",
      "Epoch 1483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4251 - total_train_reward: -1100.7646\n",
      "Epoch 1484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6185 - total_train_reward: -1069.3865\n",
      "Epoch 1485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1093 - total_train_reward: -1275.5100\n",
      "Epoch 1486/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.1051 - total_train_reward: -1085.4038\n",
      "Epoch 1487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2061 - total_train_reward: -1727.3007\n",
      "Epoch 1488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4286 - total_train_reward: -1269.9775\n",
      "Epoch 1489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7984 - total_train_reward: -968.9167\n",
      "Epoch 1490/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.2466 - total_train_reward: -1659.6981\n",
      "Epoch 1491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3252 - total_train_reward: -717.4975\n",
      "Epoch 1492/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 6.0118 - total_train_reward: -636.5598\n",
      "Epoch 1493/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.8689 - total_train_reward: -803.8940\n",
      "Epoch 1494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2959 - total_train_reward: -1058.3446\n",
      "Epoch 1495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3450 - total_train_reward: -993.8773\n",
      "Epoch 1496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7976 - total_train_reward: -622.3119\n",
      "Epoch 1497/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 12.9137 - total_train_reward: -968.9339\n",
      "Epoch 1498/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1593 - total_train_reward: -1092.6445\n",
      "Epoch 1499/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.2784 - total_train_reward: -1529.0696\n",
      "Epoch 1500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3593 - total_train_reward: -1157.8271\n",
      "Epoch 1501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2125 - total_train_reward: -1757.3407\n",
      "Epoch 1502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2874 - total_train_reward: -1352.9302\n",
      "Epoch 1503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5077 - total_train_reward: -1683.0933\n",
      "Epoch 1504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8346 - total_train_reward: -1667.4998\n",
      "Epoch 1505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9893 - total_train_reward: -1257.0763\n",
      "Epoch 1506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9636 - total_train_reward: -1163.6936\n",
      "Epoch 1507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7197 - total_train_reward: -967.0845\n",
      "Epoch 1508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8648 - total_train_reward: -1337.4380\n",
      "Epoch 1509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1733 - total_train_reward: -1072.7059\n",
      "Epoch 1510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1690 - total_train_reward: -1582.2880\n",
      "Epoch 1511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3037 - total_train_reward: -1068.9952\n",
      "Epoch 1512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0816 - total_train_reward: -741.5924\n",
      "Epoch 1513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0287 - total_train_reward: -503.1271\n",
      "Epoch 1514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6483 - total_train_reward: -1294.4754\n",
      "Epoch 1515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3652 - total_train_reward: -1743.7382\n",
      "Epoch 1516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0750 - total_train_reward: -1445.4585\n",
      "Epoch 1517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0091 - total_train_reward: -1928.9534\n",
      "Epoch 1518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6315 - total_train_reward: -1055.4296\n",
      "Epoch 1519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2398 - total_train_reward: -776.9262\n",
      "Epoch 1520/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.0248 - total_train_reward: -758.6906\n",
      "Epoch 1521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4309 - total_train_reward: -862.9807\n",
      "Epoch 1522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3279 - total_train_reward: -1315.1054\n",
      "Epoch 1523/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -12.7668 - total_train_reward: -1463.6794\n",
      "Epoch 1524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2649 - total_train_reward: -1700.3136\n",
      "Epoch 1525/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -0.9205 - total_train_reward: -622.7772\n",
      "Epoch 1526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1822 - total_train_reward: -627.2091\n",
      "Epoch 1527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8165 - total_train_reward: -1260.2450\n",
      "Epoch 1528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4868 - total_train_reward: -1710.2124\n",
      "Epoch 1529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8761 - total_train_reward: -862.5015\n",
      "Epoch 1530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9173 - total_train_reward: -1497.4103\n",
      "Epoch 1531/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.2351 - total_train_reward: -917.2158\n",
      "Epoch 1532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0088 - total_train_reward: -1366.4496\n",
      "Epoch 1533/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.6452 - total_train_reward: -860.2717\n",
      "Epoch 1534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9688 - total_train_reward: -1062.0592\n",
      "Epoch 1535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0187 - total_train_reward: -1552.7241\n",
      "Epoch 1536/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.0871 - total_train_reward: -1074.0489\n",
      "Epoch 1537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3397 - total_train_reward: -1558.0952\n",
      "Epoch 1538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2560 - total_train_reward: -1519.8117\n",
      "Epoch 1539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4123 - total_train_reward: -640.3264\n",
      "Epoch 1540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5482 - total_train_reward: -1797.2529\n",
      "Epoch 1541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1307 - total_train_reward: -1074.3521\n",
      "Epoch 1542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1366 - total_train_reward: -747.9593\n",
      "Epoch 1543/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -9.4312 - total_train_reward: -1264.6487\n",
      "Epoch 1544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1191 - total_train_reward: -748.5868\n",
      "Epoch 1545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6415 - total_train_reward: -971.3417\n",
      "Epoch 1546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7555 - total_train_reward: -1402.8546\n",
      "Epoch 1547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7303 - total_train_reward: -1164.5533\n",
      "Epoch 1548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0442 - total_train_reward: -1655.8544\n",
      "Epoch 1549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3596 - total_train_reward: -969.8198\n",
      "Epoch 1550/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.8995 - total_train_reward: -1862.7725\n",
      "Epoch 1551/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -15.3848 - total_train_reward: -1738.8171\n",
      "Epoch 1552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5889 - total_train_reward: -1450.4185\n",
      "Epoch 1553/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.1749 - total_train_reward: -1319.6227\n",
      "Epoch 1554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6890 - total_train_reward: -1126.8084\n",
      "Epoch 1555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7331 - total_train_reward: -969.4524\n",
      "Epoch 1556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6981 - total_train_reward: -748.1267\n",
      "Epoch 1557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7107 - total_train_reward: -1306.2713\n",
      "Epoch 1558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8146 - total_train_reward: -1877.5841\n",
      "Epoch 1559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2692 - total_train_reward: -1175.1782\n",
      "Epoch 1560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3592 - total_train_reward: -1713.1353\n",
      "Epoch 1561/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.7613 - total_train_reward: -1372.3327\n",
      "Epoch 1562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0497 - total_train_reward: -1639.2806\n",
      "Epoch 1563/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.9441 - total_train_reward: -1491.2747\n",
      "Epoch 1564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4917 - total_train_reward: -1262.9930\n",
      "Epoch 1565/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -8.2087 - total_train_reward: -1433.6752\n",
      "Epoch 1566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9469 - total_train_reward: -1554.8179\n",
      "Epoch 1567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1165 - total_train_reward: -1364.2745\n",
      "Epoch 1568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8608 - total_train_reward: -749.7111\n",
      "Epoch 1569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3207 - total_train_reward: -756.5610\n",
      "Epoch 1570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4979 - total_train_reward: -1049.0945\n",
      "Epoch 1571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8123 - total_train_reward: -1337.3429\n",
      "Epoch 1572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6269 - total_train_reward: -1470.7083\n",
      "Epoch 1573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8635 - total_train_reward: -1881.4452\n",
      "Epoch 1574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6402 - total_train_reward: -897.5182\n",
      "Epoch 1575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8242 - total_train_reward: -867.3178\n",
      "Epoch 1576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7094 - total_train_reward: -1367.5082\n",
      "Epoch 1577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1438 - total_train_reward: -1070.0395\n",
      "Epoch 1578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1600 - total_train_reward: -1110.1646\n",
      "Epoch 1579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9516 - total_train_reward: -1226.7214\n",
      "Epoch 1580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2190 - total_train_reward: -1165.7285\n",
      "Epoch 1581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.2766 - total_train_reward: -1467.7984\n",
      "Epoch 1582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6901 - total_train_reward: -1069.6968\n",
      "Epoch 1583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5530 - total_train_reward: -1858.2090\n",
      "Epoch 1584/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.5592 - total_train_reward: -1614.8392\n",
      "Epoch 1585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3619 - total_train_reward: -1341.5222\n",
      "Epoch 1586/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.7866 - total_train_reward: -1069.8650\n",
      "Epoch 1587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3597 - total_train_reward: -969.9408\n",
      "Epoch 1588/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.3738 - total_train_reward: -1032.4756\n",
      "Epoch 1589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3443 - total_train_reward: -1483.2414\n",
      "Epoch 1590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0451 - total_train_reward: -1617.6439\n",
      "Epoch 1591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6702 - total_train_reward: -1425.3079\n",
      "Epoch 1592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6403 - total_train_reward: -1168.2007\n",
      "Epoch 1593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2789 - total_train_reward: -1318.7815\n",
      "Epoch 1594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6329 - total_train_reward: -862.4891\n",
      "Epoch 1595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1782 - total_train_reward: -599.5120\n",
      "Epoch 1596/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 7.3329 - total_train_reward: -1158.6701\n",
      "Epoch 1597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0136 - total_train_reward: -1166.2476\n",
      "Epoch 1598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5193 - total_train_reward: -1771.3337\n",
      "Epoch 1599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2787 - total_train_reward: -1772.9964\n",
      "Epoch 1600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -17.6039 - total_train_reward: -1702.3826\n",
      "Epoch 1601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5922 - total_train_reward: -1456.2480\n",
      "Epoch 1602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7934 - total_train_reward: -634.8735\n",
      "Epoch 1603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7035 - total_train_reward: -631.9881\n",
      "Epoch 1604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7345 - total_train_reward: -1161.4042\n",
      "Epoch 1605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1050 - total_train_reward: -1068.9554\n",
      "Epoch 1606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0448 - total_train_reward: -684.3130\n",
      "Epoch 1607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1467 - total_train_reward: -1759.9389\n",
      "Epoch 1608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2243 - total_train_reward: -1174.6746\n",
      "Epoch 1609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2283 - total_train_reward: -1402.0913\n",
      "Epoch 1610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7741 - total_train_reward: -1172.5031\n",
      "Epoch 1611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7299 - total_train_reward: -1511.2273\n",
      "Epoch 1612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1756 - total_train_reward: -1626.0911\n",
      "Epoch 1613/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.8046 - total_train_reward: -1617.9216\n",
      "Epoch 1614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8434 - total_train_reward: -712.4594\n",
      "Epoch 1615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4906 - total_train_reward: -1140.2786\n",
      "Epoch 1616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0865 - total_train_reward: -1196.7283\n",
      "Epoch 1617/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.6624 - total_train_reward: -1417.3455\n",
      "Epoch 1618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3724 - total_train_reward: -1836.8756\n",
      "Epoch 1619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4337 - total_train_reward: -1452.7289\n",
      "Epoch 1620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5717 - total_train_reward: -656.1254\n",
      "Epoch 1621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2895 - total_train_reward: -617.8904\n",
      "Epoch 1622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9655 - total_train_reward: -861.7096\n",
      "Epoch 1623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9569 - total_train_reward: -1215.1096\n",
      "Epoch 1624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3107 - total_train_reward: -1460.3106\n",
      "Epoch 1625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5318 - total_train_reward: -1158.3883\n",
      "Epoch 1626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6587 - total_train_reward: -950.4354\n",
      "Epoch 1627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6582 - total_train_reward: -862.7596\n",
      "Epoch 1628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6390 - total_train_reward: -626.7543\n",
      "Epoch 1629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9088 - total_train_reward: -1678.9128\n",
      "Epoch 1630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2504 - total_train_reward: -1161.3573\n",
      "Epoch 1631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9307 - total_train_reward: -1166.6917\n",
      "Epoch 1632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7255 - total_train_reward: -969.5506\n",
      "Epoch 1633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8549 - total_train_reward: -1121.5190\n",
      "Epoch 1634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5951 - total_train_reward: -868.4815\n",
      "Epoch 1635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4664 - total_train_reward: -1663.4676\n",
      "Epoch 1636/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.4139 - total_train_reward: -944.0248\n",
      "Epoch 1637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8381 - total_train_reward: -1522.1818\n",
      "Epoch 1638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8910 - total_train_reward: -1147.8355\n",
      "Epoch 1639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8290 - total_train_reward: -959.3489\n",
      "Epoch 1640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0620 - total_train_reward: -862.5316\n",
      "Epoch 1641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4833 - total_train_reward: -1577.3534\n",
      "Epoch 1642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3328 - total_train_reward: -1703.4833\n",
      "Epoch 1643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6711 - total_train_reward: -970.8261\n",
      "Epoch 1644/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -10.7141 - total_train_reward: -1526.6234\n",
      "Epoch 1645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0556 - total_train_reward: -626.6223\n",
      "Epoch 1646/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5631 - total_train_reward: -1240.6195\n",
      "Epoch 1647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3450 - total_train_reward: -1498.9996\n",
      "Epoch 1648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4012 - total_train_reward: -1463.5436\n",
      "Epoch 1649/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.2623 - total_train_reward: -1171.6871\n",
      "Epoch 1650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9182 - total_train_reward: -1295.8520\n",
      "Epoch 1651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2933 - total_train_reward: -1738.1337\n",
      "Epoch 1652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3970 - total_train_reward: -1496.3730\n",
      "Epoch 1653/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8374 - total_train_reward: -1151.5700\n",
      "Epoch 1654/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.6061 - total_train_reward: -1177.6915\n",
      "Epoch 1655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3669 - total_train_reward: -1165.6419\n",
      "Epoch 1656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1395 - total_train_reward: -1192.8631\n",
      "Epoch 1657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3118 - total_train_reward: -1428.7070\n",
      "Epoch 1658/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5585 - total_train_reward: -1757.7175\n",
      "Epoch 1659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7879 - total_train_reward: -887.1961\n",
      "Epoch 1660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7588 - total_train_reward: -1075.4700\n",
      "Epoch 1661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.7045 - total_train_reward: -759.5479\n",
      "Epoch 1662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0664 - total_train_reward: -1597.0032\n",
      "Epoch 1663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8168 - total_train_reward: -1435.7787\n",
      "Epoch 1664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3554 - total_train_reward: -1326.0111\n",
      "Epoch 1665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6301 - total_train_reward: -1151.9562\n",
      "Epoch 1666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1651 - total_train_reward: -1371.9896\n",
      "Epoch 1667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8109 - total_train_reward: -1409.3823\n",
      "Epoch 1668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9570 - total_train_reward: -1070.0115\n",
      "Epoch 1669/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 4.1149 - total_train_reward: -1178.0790\n",
      "Epoch 1670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3943 - total_train_reward: -1461.4835\n",
      "Epoch 1671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8981 - total_train_reward: -851.7800\n",
      "Epoch 1672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9761 - total_train_reward: -748.2964\n",
      "Epoch 1673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1217 - total_train_reward: -868.5460\n",
      "Epoch 1674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1208 - total_train_reward: -1674.6102\n",
      "Epoch 1675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3515 - total_train_reward: -1402.9465\n",
      "Epoch 1676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2667 - total_train_reward: -1253.4290\n",
      "Epoch 1677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3888 - total_train_reward: -1100.7942\n",
      "Epoch 1678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8822 - total_train_reward: -967.8690\n",
      "Epoch 1679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7966 - total_train_reward: -969.7119\n",
      "Epoch 1680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6375 - total_train_reward: -766.1099\n",
      "Epoch 1681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6439 - total_train_reward: -862.6023\n",
      "Epoch 1682/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 10.7562 - total_train_reward: -1896.2080\n",
      "Epoch 1683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1580 - total_train_reward: -758.8528\n",
      "Epoch 1684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9384 - total_train_reward: -526.1359\n",
      "Epoch 1685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3370 - total_train_reward: -1233.0255\n",
      "Epoch 1686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8470 - total_train_reward: -1057.9861\n",
      "Epoch 1687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2336 - total_train_reward: -1697.2700\n",
      "Epoch 1688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8275 - total_train_reward: -1069.5466\n",
      "Epoch 1689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1334 - total_train_reward: -1816.1190\n",
      "Epoch 1690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2486 - total_train_reward: -1193.2314\n",
      "Epoch 1691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.7617 - total_train_reward: -1580.2102\n",
      "Epoch 1692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4518 - total_train_reward: -747.2414\n",
      "Epoch 1693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9293 - total_train_reward: -1545.8255\n",
      "Epoch 1694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2968 - total_train_reward: -765.8650\n",
      "Epoch 1695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4582 - total_train_reward: -1510.4349\n",
      "Epoch 1696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6285 - total_train_reward: -1166.6777\n",
      "Epoch 1697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9735 - total_train_reward: -897.6961\n",
      "Epoch 1698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5831 - total_train_reward: -1797.4138\n",
      "Epoch 1699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5403 - total_train_reward: -1260.7544\n",
      "Epoch 1700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6210 - total_train_reward: -1398.6070\n",
      "Epoch 1701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7449 - total_train_reward: -1069.9709\n",
      "Epoch 1702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7098 - total_train_reward: -900.7606\n",
      "Epoch 1703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3476 - total_train_reward: -1072.0112\n",
      "Epoch 1704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9568 - total_train_reward: -1282.0681\n",
      "Epoch 1705/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.6183 - total_train_reward: -969.8566\n",
      "Epoch 1706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7495 - total_train_reward: -1122.8613\n",
      "Epoch 1707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0171 - total_train_reward: -1825.7369\n",
      "Epoch 1708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9450 - total_train_reward: -764.1630\n",
      "Epoch 1709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2240 - total_train_reward: -1392.9972\n",
      "Epoch 1710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8252 - total_train_reward: -1324.8844\n",
      "Epoch 1711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7183 - total_train_reward: -892.2568\n",
      "Epoch 1712/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.4039 - total_train_reward: -1273.5012\n",
      "Epoch 1713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4219 - total_train_reward: -641.8467\n",
      "Epoch 1714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9346 - total_train_reward: -795.2889\n",
      "Epoch 1715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1313 - total_train_reward: -1404.7110\n",
      "Epoch 1716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7649 - total_train_reward: -1321.4326\n",
      "Epoch 1717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7597 - total_train_reward: -1055.7938\n",
      "Epoch 1718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8640 - total_train_reward: -1284.4786\n",
      "Epoch 1719/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.7106 - total_train_reward: -1488.7118\n",
      "Epoch 1720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5447 - total_train_reward: -1162.0648\n",
      "Epoch 1721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5294 - total_train_reward: -1298.8679\n",
      "Epoch 1722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.2404 - total_train_reward: -780.0490\n",
      "Epoch 1723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5440 - total_train_reward: -1396.6572\n",
      "Epoch 1724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1026 - total_train_reward: -1634.5740\n",
      "Epoch 1725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9603 - total_train_reward: -1767.4916\n",
      "Epoch 1726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4390 - total_train_reward: -1060.9706\n",
      "Epoch 1727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0494 - total_train_reward: -1504.4620\n",
      "Epoch 1728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0445 - total_train_reward: -1687.8305\n",
      "Epoch 1729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8977 - total_train_reward: -1641.8477\n",
      "Epoch 1730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4705 - total_train_reward: -1357.2270\n",
      "Epoch 1731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8181 - total_train_reward: -984.0067\n",
      "Epoch 1732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5982 - total_train_reward: -1179.5589\n",
      "Epoch 1733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7711 - total_train_reward: -1195.0584\n",
      "Epoch 1734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6880 - total_train_reward: -1665.7997\n",
      "Epoch 1735/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 9.1637 - total_train_reward: -903.7651\n",
      "Epoch 1736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9290 - total_train_reward: -1182.1859\n",
      "Epoch 1737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9731 - total_train_reward: -1924.1718\n",
      "Epoch 1738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.1095 - total_train_reward: -1787.1765\n",
      "Epoch 1739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0289 - total_train_reward: -1456.9084\n",
      "Epoch 1740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4208 - total_train_reward: -1169.1052\n",
      "Epoch 1741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9611 - total_train_reward: -1688.8300\n",
      "Epoch 1742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9489 - total_train_reward: -1123.5417\n",
      "Epoch 1743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1580 - total_train_reward: -1082.8521\n",
      "Epoch 1744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5034 - total_train_reward: -1613.0520\n",
      "Epoch 1745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1296 - total_train_reward: -1624.1990\n",
      "Epoch 1746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7399 - total_train_reward: -1674.8640\n",
      "Epoch 1747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3514 - total_train_reward: -1544.2359\n",
      "Epoch 1748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3003 - total_train_reward: -1193.0675\n",
      "Epoch 1749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4808 - total_train_reward: -968.7597\n",
      "Epoch 1750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6014 - total_train_reward: -1069.7722\n",
      "Epoch 1751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8183 - total_train_reward: -1280.7589\n",
      "Epoch 1752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0225 - total_train_reward: -746.5681\n",
      "Epoch 1753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6086 - total_train_reward: -1061.4599\n",
      "Epoch 1754/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 3.5198 - total_train_reward: -868.5242\n",
      "Epoch 1755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8212 - total_train_reward: -928.8354\n",
      "Epoch 1756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8110 - total_train_reward: -1675.8106\n",
      "Epoch 1757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5239 - total_train_reward: -1153.7379\n",
      "Epoch 1758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2974 - total_train_reward: -1381.2528\n",
      "Epoch 1759/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.9094 - total_train_reward: -816.0408\n",
      "Epoch 1760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6463 - total_train_reward: -1256.9734\n",
      "Epoch 1761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1339 - total_train_reward: -1285.2389\n",
      "Epoch 1762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2558 - total_train_reward: -807.0980\n",
      "Epoch 1763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5249 - total_train_reward: -970.5325\n",
      "Epoch 1764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8542 - total_train_reward: -1072.5840\n",
      "Epoch 1765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5558 - total_train_reward: -793.7626\n",
      "Epoch 1766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2490 - total_train_reward: -1067.1683\n",
      "Epoch 1767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5207 - total_train_reward: -1066.3135\n",
      "Epoch 1768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8843 - total_train_reward: -913.5169\n",
      "Epoch 1769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6548 - total_train_reward: -1866.1143\n",
      "Epoch 1770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8995 - total_train_reward: -863.1716\n",
      "Epoch 1771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2339 - total_train_reward: -861.7121\n",
      "Epoch 1772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3875 - total_train_reward: -1167.3274\n",
      "Epoch 1773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9481 - total_train_reward: -1472.4535\n",
      "Epoch 1774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7071 - total_train_reward: -1172.6566\n",
      "Epoch 1775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6812 - total_train_reward: -1086.8214\n",
      "Epoch 1776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4817 - total_train_reward: -1078.8008\n",
      "Epoch 1777/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6770 - total_train_reward: -1490.4447\n",
      "Epoch 1778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0399 - total_train_reward: -1346.1818\n",
      "Epoch 1779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2441 - total_train_reward: -1271.1648\n",
      "Epoch 1780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1841 - total_train_reward: -1570.3902\n",
      "Epoch 1781/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.8515 - total_train_reward: -967.0344\n",
      "Epoch 1782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8551 - total_train_reward: -1259.3877\n",
      "Epoch 1783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4117 - total_train_reward: -1049.6417\n",
      "Epoch 1784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4796 - total_train_reward: -1188.2867\n",
      "Epoch 1785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2913 - total_train_reward: -1458.9820\n",
      "Epoch 1786/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7291 - total_train_reward: -906.8659\n",
      "Epoch 1787/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0331 - total_train_reward: -1197.0221\n",
      "Epoch 1788/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6557 - total_train_reward: -1142.0556\n",
      "Epoch 1789/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1919 - total_train_reward: -1674.1607\n",
      "Epoch 1790/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2489 - total_train_reward: -1072.5669\n",
      "Epoch 1791/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1558 - total_train_reward: -1100.4905\n",
      "Epoch 1792/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1009 - total_train_reward: -624.8529\n",
      "Epoch 1793/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2987 - total_train_reward: -1151.7924\n",
      "Epoch 1794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0564 - total_train_reward: -1296.7700\n",
      "Epoch 1795/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2841 - total_train_reward: -1075.0681\n",
      "Epoch 1796/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4410 - total_train_reward: -1195.0861\n",
      "Epoch 1797/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8565 - total_train_reward: -1506.4175\n",
      "Epoch 1798/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2157 - total_train_reward: -1570.3805\n",
      "Epoch 1799/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -7.4747 - total_train_reward: -1287.7263\n",
      "Epoch 1800/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6535 - total_train_reward: -1282.6108\n",
      "Epoch 1801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5385 - total_train_reward: -904.1281\n",
      "Epoch 1802/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0833 - total_train_reward: -1755.0850\n",
      "Epoch 1803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8677 - total_train_reward: -1789.1162\n",
      "Epoch 1804/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9697 - total_train_reward: -1251.1589\n",
      "Epoch 1805/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9809 - total_train_reward: -972.5028\n",
      "Epoch 1806/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3740 - total_train_reward: -1549.5842\n",
      "Epoch 1807/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1967 - total_train_reward: -863.8557\n",
      "Epoch 1808/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4308 - total_train_reward: -1075.6480\n",
      "Epoch 1809/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9915 - total_train_reward: -1685.3266\n",
      "Epoch 1810/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2334 - total_train_reward: -903.7825\n",
      "Epoch 1811/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2412 - total_train_reward: -1177.6676\n",
      "Epoch 1812/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7634 - total_train_reward: -1275.1294\n",
      "Epoch 1813/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3381 - total_train_reward: -1073.4825\n",
      "Epoch 1814/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2868 - total_train_reward: -1547.7845\n",
      "Epoch 1815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6028 - total_train_reward: -1070.3995\n",
      "Epoch 1816/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1117 - total_train_reward: -906.0247\n",
      "Epoch 1817/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8354 - total_train_reward: -1169.3154\n",
      "Epoch 1818/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1229 - total_train_reward: -902.2663\n",
      "Epoch 1819/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7962 - total_train_reward: -1231.6630\n",
      "Epoch 1820/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5715 - total_train_reward: -1368.5546\n",
      "Epoch 1821/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6688 - total_train_reward: -1426.8230\n",
      "Epoch 1822/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5363 - total_train_reward: -1649.9320\n",
      "Epoch 1823/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8687 - total_train_reward: -1329.0016\n",
      "Epoch 1824/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0704 - total_train_reward: -1607.3148\n",
      "Epoch 1825/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6726 - total_train_reward: -1321.6437\n",
      "Epoch 1826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5198 - total_train_reward: -1028.2836\n",
      "Epoch 1827/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4042 - total_train_reward: -932.5556\n",
      "Epoch 1828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9227 - total_train_reward: -1801.1354\n",
      "Epoch 1829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3440 - total_train_reward: -1356.9873\n",
      "Epoch 1830/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1077 - total_train_reward: -1212.7397\n",
      "Epoch 1831/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5559 - total_train_reward: -1331.4325\n",
      "Epoch 1832/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4739 - total_train_reward: -1002.6473\n",
      "Epoch 1833/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6075 - total_train_reward: -1184.3635\n",
      "Epoch 1834/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4706 - total_train_reward: -1862.5250\n",
      "Epoch 1835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7979 - total_train_reward: -1592.3021\n",
      "Epoch 1836/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 1.7498 - total_train_reward: -1353.4721\n",
      "Epoch 1837/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5013 - total_train_reward: -913.5050\n",
      "Epoch 1838/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7941 - total_train_reward: -1058.8556\n",
      "Epoch 1839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4645 - total_train_reward: -1925.6626\n",
      "Epoch 1840/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3006 - total_train_reward: -1901.3056\n",
      "Epoch 1841/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6325 - total_train_reward: -907.2322\n",
      "Epoch 1842/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.0399 - total_train_reward: -1754.7242\n",
      "Epoch 1843/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4579 - total_train_reward: -1001.8090\n",
      "Epoch 1844/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8183 - total_train_reward: -1796.8920\n",
      "Epoch 1845/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2049 - total_train_reward: -1825.4289\n",
      "Epoch 1846/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1790 - total_train_reward: -1173.3353\n",
      "Epoch 1847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1841 - total_train_reward: -1210.4447\n",
      "Epoch 1848/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3222 - total_train_reward: -1404.3428\n",
      "Epoch 1849/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2310 - total_train_reward: -1329.6115\n",
      "Epoch 1850/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.9028 - total_train_reward: -968.8622\n",
      "Epoch 1851/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9759 - total_train_reward: -1013.5906\n",
      "Epoch 1852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6304 - total_train_reward: -1070.6243\n",
      "Epoch 1853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3307 - total_train_reward: -1289.9685\n",
      "Epoch 1854/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6438 - total_train_reward: -966.7771\n",
      "Epoch 1855/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1983 - total_train_reward: -1162.1666\n",
      "Epoch 1856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0380 - total_train_reward: -1156.4279\n",
      "Epoch 1857/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.4501 - total_train_reward: -859.9099\n",
      "Epoch 1858/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7479 - total_train_reward: -1358.9028\n",
      "Epoch 1859/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7626 - total_train_reward: -1236.7670\n",
      "Epoch 1860/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9758 - total_train_reward: -1484.7614\n",
      "Epoch 1861/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.1728 - total_train_reward: -971.2424\n",
      "Epoch 1862/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4019 - total_train_reward: -968.0552\n",
      "Epoch 1863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5796 - total_train_reward: -1365.0865\n",
      "Epoch 1864/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0424 - total_train_reward: -1270.6745\n",
      "Epoch 1865/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1110 - total_train_reward: -1324.6694\n",
      "Epoch 1866/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.6447 - total_train_reward: -743.3158\n",
      "Epoch 1867/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8479 - total_train_reward: -1560.3347\n",
      "Epoch 1868/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9836 - total_train_reward: -1743.5721\n",
      "Epoch 1869/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.5686 - total_train_reward: -907.7408\n",
      "Epoch 1870/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -3.3365 - total_train_reward: -1438.2811\n",
      "Epoch 1871/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4326 - total_train_reward: -1204.1610\n",
      "Epoch 1872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6059 - total_train_reward: -1677.8134\n",
      "Epoch 1873/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.5606 - total_train_reward: -996.6944\n",
      "Epoch 1874/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8804 - total_train_reward: -899.8846\n",
      "Epoch 1875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1637 - total_train_reward: -1548.5147\n",
      "Epoch 1876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8495 - total_train_reward: -1552.0533\n",
      "Epoch 1877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2553 - total_train_reward: -1391.5406\n",
      "Epoch 1878/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9855 - total_train_reward: -1089.3030\n",
      "Epoch 1879/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1795 - total_train_reward: -1801.6529\n",
      "Epoch 1880/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4600 - total_train_reward: -1634.7803\n",
      "Epoch 1881/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9000 - total_train_reward: -943.7975\n",
      "Epoch 1882/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6904 - total_train_reward: -1395.5164\n",
      "Epoch 1883/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9584 - total_train_reward: -743.9290\n",
      "Epoch 1884/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7341 - total_train_reward: -1200.0062\n",
      "Epoch 1885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3634 - total_train_reward: -1474.2394\n",
      "Epoch 1886/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2917 - total_train_reward: -1135.0584\n",
      "Epoch 1887/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2485 - total_train_reward: -1041.6622\n",
      "Epoch 1888/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8076 - total_train_reward: -1869.6556\n",
      "Epoch 1889/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9736 - total_train_reward: -1068.6682\n",
      "Epoch 1890/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.8006 - total_train_reward: -1250.2837\n",
      "Epoch 1891/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7265 - total_train_reward: -1194.8270\n",
      "Epoch 1892/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8214 - total_train_reward: -1470.5798\n",
      "Epoch 1893/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3668 - total_train_reward: -1565.0346\n",
      "Epoch 1894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8246 - total_train_reward: -1169.8375\n",
      "Epoch 1895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5394 - total_train_reward: -1099.6629\n",
      "Epoch 1896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4273 - total_train_reward: -1920.9207\n",
      "Epoch 1897/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7092 - total_train_reward: -1555.3335\n",
      "Epoch 1898/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0430 - total_train_reward: -1602.3228\n",
      "Epoch 1899/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7404 - total_train_reward: -1779.8535\n",
      "Epoch 1900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2480 - total_train_reward: -962.5544\n",
      "Epoch 1901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2028 - total_train_reward: -931.4913\n",
      "Epoch 1902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2592 - total_train_reward: -896.6498\n",
      "Epoch 1903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7560 - total_train_reward: -1696.7156\n",
      "Epoch 1904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7818 - total_train_reward: -1379.5885\n",
      "Epoch 1905/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0190 - total_train_reward: -1694.1546\n",
      "Epoch 1906/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9518 - total_train_reward: -1088.7317\n",
      "Epoch 1907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1869 - total_train_reward: -1336.8721\n",
      "Epoch 1908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7273 - total_train_reward: -996.6805\n",
      "Epoch 1909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5148 - total_train_reward: -1306.3037\n",
      "Epoch 1910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1016 - total_train_reward: -1171.8778\n",
      "Epoch 1911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9568 - total_train_reward: -769.7015\n",
      "Epoch 1912/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6530 - total_train_reward: -1735.6714\n",
      "Epoch 1913/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3221 - total_train_reward: -860.4934\n",
      "Epoch 1914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.8940 - total_train_reward: -1030.7074\n",
      "Epoch 1915/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6584 - total_train_reward: -1068.9787\n",
      "Epoch 1916/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6702 - total_train_reward: -1436.6353\n",
      "Epoch 1917/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4436 - total_train_reward: -1204.5493\n",
      "Epoch 1918/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 5.7507 - total_train_reward: -1862.3668\n",
      "Epoch 1919/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8473 - total_train_reward: -1511.4274\n",
      "Epoch 1920/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9019 - total_train_reward: -1037.6128\n",
      "Epoch 1921/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8949 - total_train_reward: -1256.0771\n",
      "Epoch 1922/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6472 - total_train_reward: -1433.7525\n",
      "Epoch 1923/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4077 - total_train_reward: -779.1001\n",
      "Epoch 1924/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2827 - total_train_reward: -1547.7556\n",
      "Epoch 1925/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.9251 - total_train_reward: -859.8906\n",
      "Epoch 1926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.2088 - total_train_reward: -1034.5576\n",
      "Epoch 1927/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9505 - total_train_reward: -854.8070\n",
      "Epoch 1928/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9483 - total_train_reward: -1173.5492\n",
      "Epoch 1929/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7354 - total_train_reward: -1166.0513\n",
      "Epoch 1930/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3120 - total_train_reward: -1290.1007\n",
      "Epoch 1931/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.6396 - total_train_reward: -968.1418\n",
      "Epoch 1932/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1035 - total_train_reward: -1589.2122\n",
      "Epoch 1933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0060 - total_train_reward: -1274.4457\n",
      "Epoch 1934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8792 - total_train_reward: -1156.5278\n",
      "Epoch 1935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7904 - total_train_reward: -1635.8874\n",
      "Epoch 1936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5892 - total_train_reward: -1609.0165\n",
      "Epoch 1937/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7062 - total_train_reward: -1293.3880\n",
      "Epoch 1938/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7489 - total_train_reward: -1034.0842\n",
      "Epoch 1939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0026 - total_train_reward: -1289.0947\n",
      "Epoch 1940/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4808 - total_train_reward: -1278.0526\n",
      "Epoch 1941/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -11.1636 - total_train_reward: -1493.7963\n",
      "Epoch 1942/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0790 - total_train_reward: -1398.9681\n",
      "Epoch 1943/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7924 - total_train_reward: -851.4808\n",
      "Epoch 1944/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1851 - total_train_reward: -1721.6061\n",
      "Epoch 1945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3287 - total_train_reward: -1568.8006\n",
      "Epoch 1946/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6424 - total_train_reward: -1192.4371\n",
      "Epoch 1947/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3679 - total_train_reward: -1643.4450\n",
      "Epoch 1948/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3932 - total_train_reward: -1719.7905\n",
      "Epoch 1949/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8621 - total_train_reward: -1332.7147\n",
      "Epoch 1950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3882 - total_train_reward: -1292.0114\n",
      "Epoch 1951/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9365 - total_train_reward: -1280.3637\n",
      "Epoch 1952/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9263 - total_train_reward: -1151.3668\n",
      "Epoch 1953/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3867 - total_train_reward: -1570.6801\n",
      "Epoch 1954/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5617 - total_train_reward: -893.5660\n",
      "Epoch 1955/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9282 - total_train_reward: -947.0608\n",
      "Epoch 1956/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7914 - total_train_reward: -1056.2001\n",
      "Epoch 1957/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3450 - total_train_reward: -1487.5766\n",
      "Epoch 1958/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4379 - total_train_reward: -1170.7853\n",
      "Epoch 1959/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -6.5778 - total_train_reward: -966.2999\n",
      "Epoch 1960/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1002 - total_train_reward: -620.8203\n",
      "Epoch 1961/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0756 - total_train_reward: -1068.3923\n",
      "Epoch 1962/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4274 - total_train_reward: -1000.4432\n",
      "Epoch 1963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3840 - total_train_reward: -1023.2083\n",
      "Epoch 1964/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2923 - total_train_reward: -988.3220\n",
      "Epoch 1965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1924 - total_train_reward: -1042.9492\n",
      "Epoch 1966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0920 - total_train_reward: -1365.3341\n",
      "Epoch 1967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3056 - total_train_reward: -1449.6517\n",
      "Epoch 1968/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7178 - total_train_reward: -1168.3270\n",
      "Epoch 1969/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.4548 - total_train_reward: -1079.5736\n",
      "Epoch 1970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3827 - total_train_reward: -1437.8257\n",
      "Epoch 1971/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.0711 - total_train_reward: -1422.1491\n",
      "Epoch 1972/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6965 - total_train_reward: -1815.3746\n",
      "Epoch 1973/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5671 - total_train_reward: -1088.6719\n",
      "Epoch 1974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8082 - total_train_reward: -1504.8214\n",
      "Epoch 1975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8299 - total_train_reward: -742.1787\n",
      "Epoch 1976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1741 - total_train_reward: -970.6463\n",
      "Epoch 1977/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4173 - total_train_reward: -968.4558\n",
      "Epoch 1978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1420 - total_train_reward: -1333.7180\n",
      "Epoch 1979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1097 - total_train_reward: -1592.8436\n",
      "Epoch 1980/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3959 - total_train_reward: -1182.6013\n",
      "Epoch 1981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5326 - total_train_reward: -1715.2882\n",
      "Epoch 1982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7461 - total_train_reward: -1162.4290\n",
      "Epoch 1983/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2231 - total_train_reward: -1590.3001\n",
      "Epoch 1984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7239 - total_train_reward: -1343.6602\n",
      "Epoch 1985/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1300 - total_train_reward: -1067.1529\n",
      "Epoch 1986/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4480 - total_train_reward: -1067.3572\n",
      "Epoch 1987/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3426 - total_train_reward: -1323.2232\n",
      "Epoch 1988/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9027 - total_train_reward: -1066.5342\n",
      "Epoch 1989/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0113 - total_train_reward: -1022.6236\n",
      "Epoch 1990/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7844 - total_train_reward: -1688.2023\n",
      "Epoch 1991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3557 - total_train_reward: -1074.6341\n",
      "Epoch 1992/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8259 - total_train_reward: -1424.7778\n",
      "Epoch 1993/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2660 - total_train_reward: -1524.7128\n",
      "Epoch 1994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3461 - total_train_reward: -1240.8962\n",
      "Epoch 1995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1342 - total_train_reward: -1768.9033\n",
      "Epoch 1996/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1944 - total_train_reward: -1611.8173\n",
      "Epoch 1997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1549 - total_train_reward: -1159.7434\n",
      "Epoch 1998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5410 - total_train_reward: -943.2545\n",
      "Epoch 1999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8687 - total_train_reward: -1394.2016\n",
      "Epoch 2000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6363 - total_train_reward: -865.7442\n",
      "Epoch 2001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2614 - total_train_reward: -1028.5778\n",
      "Epoch 2002/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6693 - total_train_reward: -925.8334\n",
      "Epoch 2003/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4469 - total_train_reward: -1034.4733\n",
      "Epoch 2004/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2710 - total_train_reward: -1721.5361\n",
      "Epoch 2005/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6644 - total_train_reward: -1819.9834\n",
      "Epoch 2006/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8913 - total_train_reward: -1375.3312\n",
      "Epoch 2007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1784 - total_train_reward: -1571.9908\n",
      "Epoch 2008/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6487 - total_train_reward: -1130.6863\n",
      "Epoch 2009/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5398 - total_train_reward: -1145.6379\n",
      "Epoch 2010/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0416 - total_train_reward: -1358.9722\n",
      "Epoch 2011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4444 - total_train_reward: -1258.6705\n",
      "Epoch 2012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7651 - total_train_reward: -1182.1760\n",
      "Epoch 2013/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6645 - total_train_reward: -1796.7900\n",
      "Epoch 2014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.8569 - total_train_reward: -916.1910\n",
      "Epoch 2015/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0348 - total_train_reward: -1056.7866\n",
      "Epoch 2016/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4173 - total_train_reward: -1634.4250\n",
      "Epoch 2017/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7340 - total_train_reward: -1034.9487\n",
      "Epoch 2018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7432 - total_train_reward: -1467.2577\n",
      "Epoch 2019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6225 - total_train_reward: -1662.1895\n",
      "Epoch 2020/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 0.3051 - total_train_reward: -1000.0592\n",
      "Epoch 2021/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1512 - total_train_reward: -1841.9258\n",
      "Epoch 2022/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2555 - total_train_reward: -1031.1064\n",
      "Epoch 2023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7848 - total_train_reward: -1172.2894\n",
      "Epoch 2024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7946 - total_train_reward: -955.6995\n",
      "Epoch 2025/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1460 - total_train_reward: -1231.4392\n",
      "Epoch 2026/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1408 - total_train_reward: -1283.0676\n",
      "Epoch 2027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2284 - total_train_reward: -1415.0699\n",
      "Epoch 2028/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1216 - total_train_reward: -1732.0359\n",
      "Epoch 2029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8409 - total_train_reward: -860.3686\n",
      "Epoch 2030/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3383 - total_train_reward: -1024.6229\n",
      "Epoch 2031/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: 2.5012 - total_train_reward: -1358.8320\n",
      "Epoch 2032/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9177 - total_train_reward: -1028.0111\n",
      "Epoch 2033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4963 - total_train_reward: -1387.9967\n",
      "Epoch 2034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2791 - total_train_reward: -969.0374\n",
      "Epoch 2035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2275 - total_train_reward: -1157.4177\n",
      "Epoch 2036/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9648 - total_train_reward: -1761.9576\n",
      "Epoch 2037/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5629 - total_train_reward: -1282.9646\n",
      "Epoch 2038/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8705 - total_train_reward: -1294.8564\n",
      "Epoch 2039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6966 - total_train_reward: -1156.6561\n",
      "Epoch 2040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0333 - total_train_reward: -1012.1381\n",
      "Epoch 2041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7507 - total_train_reward: -1607.1772\n",
      "Epoch 2042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2057 - total_train_reward: -860.9022\n",
      "Epoch 2043/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9925 - total_train_reward: -982.1702\n",
      "Epoch 2044/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5598 - total_train_reward: -1192.1144\n",
      "Epoch 2045/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7311 - total_train_reward: -1007.5756\n",
      "Epoch 2046/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.5970 - total_train_reward: -1272.5066\n",
      "Epoch 2047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8950 - total_train_reward: -1171.7141\n",
      "Epoch 2048/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9915 - total_train_reward: -1489.0688\n",
      "Epoch 2049/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7007 - total_train_reward: -1169.1550\n",
      "Epoch 2050/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5714 - total_train_reward: -1881.5472\n",
      "Epoch 2051/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5734 - total_train_reward: -1256.6975\n",
      "Epoch 2052/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2880 - total_train_reward: -1611.0996\n",
      "Epoch 2053/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2184 - total_train_reward: -1640.2970\n",
      "Epoch 2054/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9990 - total_train_reward: -976.5500\n",
      "Epoch 2055/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2685 - total_train_reward: -1551.4367\n",
      "Epoch 2056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3918 - total_train_reward: -1308.9631\n",
      "Epoch 2057/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3321 - total_train_reward: -1588.6196\n",
      "Epoch 2058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1281 - total_train_reward: -811.8463\n",
      "Epoch 2059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4866 - total_train_reward: -1742.2521\n",
      "Epoch 2060/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7357 - total_train_reward: -862.7116\n",
      "Epoch 2061/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -5.0039 - total_train_reward: -646.3477\n",
      "Epoch 2062/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5008 - total_train_reward: -1013.4717\n",
      "Epoch 2063/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8095 - total_train_reward: -880.5826\n",
      "Epoch 2064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8745 - total_train_reward: -1172.4876\n",
      "Epoch 2065/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2385 - total_train_reward: -863.5139\n",
      "Epoch 2066/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8467 - total_train_reward: -875.7106\n",
      "Epoch 2067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9380 - total_train_reward: -1333.4840\n",
      "Epoch 2068/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.9912 - total_train_reward: -1563.7610\n",
      "Epoch 2069/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3193 - total_train_reward: -1811.6522\n",
      "Epoch 2070/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8871 - total_train_reward: -971.7598\n",
      "Epoch 2071/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8091 - total_train_reward: -1345.6560\n",
      "Epoch 2072/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3604 - total_train_reward: -947.7280\n",
      "Epoch 2073/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3640 - total_train_reward: -1807.3716\n",
      "Epoch 2074/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7319 - total_train_reward: -1208.4751\n",
      "Epoch 2075/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4966 - total_train_reward: -1640.4905\n",
      "Epoch 2076/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1710 - total_train_reward: -1021.6198\n",
      "Epoch 2077/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5502 - total_train_reward: -1824.3957\n",
      "Epoch 2078/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2634 - total_train_reward: -1448.5705\n",
      "Epoch 2079/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0019 - total_train_reward: -1168.6330\n",
      "Epoch 2080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3457 - total_train_reward: -1355.4968\n",
      "Epoch 2081/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5455 - total_train_reward: -898.2707\n",
      "Epoch 2082/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6807 - total_train_reward: -1623.5447\n",
      "Epoch 2083/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9215 - total_train_reward: -1170.2644\n",
      "Epoch 2084/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8743 - total_train_reward: -1167.6690\n",
      "Epoch 2085/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1867 - total_train_reward: -1368.7701\n",
      "Epoch 2086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7914 - total_train_reward: -1001.5556\n",
      "Epoch 2087/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3684 - total_train_reward: -1775.0633\n",
      "Epoch 2088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9599 - total_train_reward: -1557.8973\n",
      "Epoch 2089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9389 - total_train_reward: -1290.2602\n",
      "Epoch 2090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9673 - total_train_reward: -1218.1122\n",
      "Epoch 2091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8378 - total_train_reward: -969.3486\n",
      "Epoch 2092/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9581 - total_train_reward: -897.0148\n",
      "Epoch 2093/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0849 - total_train_reward: -1076.2209\n",
      "Epoch 2094/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6356 - total_train_reward: -1443.2072\n",
      "Epoch 2095/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8842 - total_train_reward: -1442.6274\n",
      "Epoch 2096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7878 - total_train_reward: -1082.2703\n",
      "Epoch 2097/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0036 - total_train_reward: -1631.1485\n",
      "Epoch 2098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4433 - total_train_reward: -1629.3815\n",
      "Epoch 2099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2888 - total_train_reward: -1450.0082\n",
      "Epoch 2100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4756 - total_train_reward: -894.5217\n",
      "Epoch 2101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9252 - total_train_reward: -1071.0796\n",
      "Epoch 2102/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9140 - total_train_reward: -1175.9795\n",
      "Epoch 2103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5568 - total_train_reward: -1171.9430\n",
      "Epoch 2104/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2832 - total_train_reward: -1800.1851\n",
      "Epoch 2105/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8311 - total_train_reward: -996.1327\n",
      "Epoch 2106/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2109 - total_train_reward: -1816.9565\n",
      "Epoch 2107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1108 - total_train_reward: -1181.1659\n",
      "Epoch 2108/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1556 - total_train_reward: -1391.4772\n",
      "Epoch 2109/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1919 - total_train_reward: -1820.4194\n",
      "Epoch 2110/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6692 - total_train_reward: -1599.9882\n",
      "Epoch 2111/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8478 - total_train_reward: -968.9696\n",
      "Epoch 2112/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0682 - total_train_reward: -992.9640\n",
      "Epoch 2113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9075 - total_train_reward: -1179.4599\n",
      "Epoch 2114/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2566 - total_train_reward: -1039.2370\n",
      "Epoch 2115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9416 - total_train_reward: -1021.7511\n",
      "Epoch 2116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.0410 - total_train_reward: -1541.0937\n",
      "Epoch 2117/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0350 - total_train_reward: -1067.7617\n",
      "Epoch 2118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7857 - total_train_reward: -1248.0090\n",
      "Epoch 2119/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6225 - total_train_reward: -1809.8092\n",
      "Epoch 2120/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4890 - total_train_reward: -1806.3076\n",
      "Epoch 2121/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3256 - total_train_reward: -1164.4427\n",
      "Epoch 2122/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5545 - total_train_reward: -1075.4272\n",
      "Epoch 2123/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2056 - total_train_reward: -1042.5741\n",
      "Epoch 2124/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2098 - total_train_reward: -1292.3014\n",
      "Epoch 2125/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9783 - total_train_reward: -1155.3201\n",
      "Epoch 2126/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8440 - total_train_reward: -1420.6789\n",
      "Epoch 2127/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6572 - total_train_reward: -846.3722\n",
      "Epoch 2128/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3874 - total_train_reward: -966.2623\n",
      "Epoch 2129/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6004 - total_train_reward: -1265.6980\n",
      "Epoch 2130/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3874 - total_train_reward: -1887.0125\n",
      "Epoch 2131/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0015 - total_train_reward: -1472.9074\n",
      "Epoch 2132/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8892 - total_train_reward: -1502.4476\n",
      "Epoch 2133/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6785 - total_train_reward: -961.8550\n",
      "Epoch 2134/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8139 - total_train_reward: -1064.0770\n",
      "Epoch 2135/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6368 - total_train_reward: -1165.5359\n",
      "Epoch 2136/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8218 - total_train_reward: -1025.6580\n",
      "Epoch 2137/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7074 - total_train_reward: -1815.1162\n",
      "Epoch 2138/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.4255 - total_train_reward: -1434.6147\n",
      "Epoch 2139/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6336 - total_train_reward: -1658.8237\n",
      "Epoch 2140/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.8841 - total_train_reward: -1935.9026\n",
      "Epoch 2141/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9710 - total_train_reward: -1934.3135\n",
      "Epoch 2142/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1490 - total_train_reward: -1122.8538\n",
      "Epoch 2143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8900 - total_train_reward: -1202.7949\n",
      "Epoch 2144/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6776 - total_train_reward: -744.3326\n",
      "Epoch 2145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8280 - total_train_reward: -898.3546\n",
      "Epoch 2146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7388 - total_train_reward: -1021.5998\n",
      "Epoch 2147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0930 - total_train_reward: -1113.2155\n",
      "Epoch 2148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4395 - total_train_reward: -1643.3445\n",
      "Epoch 2149/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4413 - total_train_reward: -1411.0809\n",
      "Epoch 2150/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1995 - total_train_reward: -1175.6817\n",
      "Epoch 2151/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5468 - total_train_reward: -1173.6011\n",
      "Epoch 2152/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1770 - total_train_reward: -1334.4404\n",
      "Epoch 2153/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5053 - total_train_reward: -1306.0078\n",
      "Epoch 2154/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6716 - total_train_reward: -1392.9610\n",
      "Epoch 2155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7197 - total_train_reward: -1755.7013\n",
      "Epoch 2156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8577 - total_train_reward: -1170.7785\n",
      "Epoch 2157/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2543 - total_train_reward: -1374.3848\n",
      "Epoch 2158/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9085 - total_train_reward: -886.2324\n",
      "Epoch 2159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6589 - total_train_reward: -856.3413\n",
      "Epoch 2160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3870 - total_train_reward: -904.9257\n",
      "Epoch 2161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0903 - total_train_reward: -937.8644\n",
      "Epoch 2162/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2715 - total_train_reward: -1765.8676\n",
      "Epoch 2163/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7168 - total_train_reward: -1712.9400\n",
      "Epoch 2164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.1403 - total_train_reward: -1655.1431\n",
      "Epoch 2165/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9120 - total_train_reward: -795.5619\n",
      "Epoch 2166/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5162 - total_train_reward: -1175.8508\n",
      "Epoch 2167/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7873 - total_train_reward: -1895.8729\n",
      "Epoch 2168/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -16.9773 - total_train_reward: -1785.0994\n",
      "Epoch 2169/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7802 - total_train_reward: -927.7466\n",
      "Epoch 2170/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2084 - total_train_reward: -1421.5217\n",
      "Epoch 2171/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2484 - total_train_reward: -872.6150\n",
      "Epoch 2172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5922 - total_train_reward: -984.1482\n",
      "Epoch 2173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4185 - total_train_reward: -1653.7668\n",
      "Epoch 2174/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3033 - total_train_reward: -1410.8685\n",
      "Epoch 2175/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6925 - total_train_reward: -1041.6783\n",
      "Epoch 2176/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.3640 - total_train_reward: -1670.2132\n",
      "Epoch 2177/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2214 - total_train_reward: -1794.6477\n",
      "Epoch 2178/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0652 - total_train_reward: -1270.2764\n",
      "Epoch 2179/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2553 - total_train_reward: -1339.2193\n",
      "Epoch 2180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4298 - total_train_reward: -1728.7742\n",
      "Epoch 2181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3965 - total_train_reward: -1330.5546\n",
      "Epoch 2182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9526 - total_train_reward: -1835.1657\n",
      "Epoch 2183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8694 - total_train_reward: -1115.0631\n",
      "Epoch 2184/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -2.8526 - total_train_reward: -1403.9717\n",
      "Epoch 2185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3696 - total_train_reward: -1069.0745\n",
      "Epoch 2186/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2586 - total_train_reward: -1308.3627\n",
      "Epoch 2187/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2266 - total_train_reward: -1405.9595\n",
      "Epoch 2188/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7465 - total_train_reward: -1615.4730\n",
      "Epoch 2189/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0928 - total_train_reward: -1112.7319\n",
      "Epoch 2190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1145 - total_train_reward: -742.9885\n",
      "Epoch 2191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1770 - total_train_reward: -1022.1775\n",
      "Epoch 2192/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2223 - total_train_reward: -1177.9263\n",
      "Epoch 2193/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0038 - total_train_reward: -1746.2434\n",
      "Epoch 2194/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1888 - total_train_reward: -1881.9688\n",
      "Epoch 2195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -16.4455 - total_train_reward: -1692.6947\n",
      "Epoch 2196/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3194 - total_train_reward: -1774.8157\n",
      "Epoch 2197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7657 - total_train_reward: -1312.7470\n",
      "Epoch 2198/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7241 - total_train_reward: -1068.5361\n",
      "Epoch 2199/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8878 - total_train_reward: -1092.9696\n",
      "Epoch 2200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9138 - total_train_reward: -1269.3569\n",
      "Epoch 2201/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6287 - total_train_reward: -1264.0283\n",
      "Epoch 2202/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0964 - total_train_reward: -1765.9164\n",
      "Epoch 2203/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9136 - total_train_reward: -1257.8011\n",
      "Epoch 2204/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9894 - total_train_reward: -1765.4139\n",
      "Epoch 2205/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8944 - total_train_reward: -1035.3780\n",
      "Epoch 2206/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1410 - total_train_reward: -898.3907\n",
      "Epoch 2207/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1527 - total_train_reward: -1131.1359\n",
      "Epoch 2208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6238 - total_train_reward: -1853.9737\n",
      "Epoch 2209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4736 - total_train_reward: -1178.1177\n",
      "Epoch 2210/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1280 - total_train_reward: -1070.4401\n",
      "Epoch 2211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4766 - total_train_reward: -851.4986\n",
      "Epoch 2212/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -17.2009 - total_train_reward: -1641.2548\n",
      "Epoch 2213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0220 - total_train_reward: -1604.7793\n",
      "Epoch 2214/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2528 - total_train_reward: -1167.7462\n",
      "Epoch 2215/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9513 - total_train_reward: -1267.1012\n",
      "Epoch 2216/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5700 - total_train_reward: -1167.8824\n",
      "Epoch 2217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2839 - total_train_reward: -869.5456\n",
      "Epoch 2218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6638 - total_train_reward: -968.9640\n",
      "Epoch 2219/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5323 - total_train_reward: -1486.4013\n",
      "Epoch 2220/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8471 - total_train_reward: -964.8219\n",
      "Epoch 2221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8447 - total_train_reward: -1073.0777\n",
      "Epoch 2222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.5022 - total_train_reward: -1029.0430\n",
      "Epoch 2223/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0666 - total_train_reward: -867.8978\n",
      "Epoch 2224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6215 - total_train_reward: -1162.2640\n",
      "Epoch 2225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6626 - total_train_reward: -1039.2599\n",
      "Epoch 2226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7210 - total_train_reward: -1033.3305\n",
      "Epoch 2227/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4719 - total_train_reward: -901.9556\n",
      "Epoch 2228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7882 - total_train_reward: -1811.5418\n",
      "Epoch 2229/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -17.3774 - total_train_reward: -1568.1964\n",
      "Epoch 2230/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3876 - total_train_reward: -1698.8886\n",
      "Epoch 2231/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8160 - total_train_reward: -1060.6421\n",
      "Epoch 2232/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5476 - total_train_reward: -1562.1726\n",
      "Epoch 2233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6637 - total_train_reward: -1177.8459\n",
      "Epoch 2234/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0093 - total_train_reward: -1169.7310\n",
      "Epoch 2235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4690 - total_train_reward: -1170.2465\n",
      "Epoch 2236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5808 - total_train_reward: -1706.8855\n",
      "Epoch 2237/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8772 - total_train_reward: -1331.0624\n",
      "Epoch 2238/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0392 - total_train_reward: -1240.7559\n",
      "Epoch 2239/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0084 - total_train_reward: -970.7354\n",
      "Epoch 2240/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7095 - total_train_reward: -1340.2763\n",
      "Epoch 2241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6616 - total_train_reward: -1063.2112\n",
      "Epoch 2242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3986 - total_train_reward: -973.0591\n",
      "Epoch 2243/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2419 - total_train_reward: -1194.8615\n",
      "Epoch 2244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4899 - total_train_reward: -1021.1235\n",
      "Epoch 2245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0078 - total_train_reward: -958.9971\n",
      "Epoch 2246/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9728 - total_train_reward: -1176.9768\n",
      "Epoch 2247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8023 - total_train_reward: -1078.1157\n",
      "Epoch 2248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0797 - total_train_reward: -1234.7323\n",
      "Epoch 2249/10000\n",
      "1/1 [==============================] - 1s 1s/step - actor_loss: -4.7776 - total_train_reward: -1672.1091\n",
      "Epoch 2250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7999 - total_train_reward: -1512.9000\n",
      "Epoch 2251/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8444 - total_train_reward: -1420.8473\n",
      "Epoch 2252/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1064 - total_train_reward: -1033.1874\n",
      "Epoch 2253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9237 - total_train_reward: -1166.9450\n",
      "Epoch 2254/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4870 - total_train_reward: -1243.9843\n",
      "Epoch 2255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2021 - total_train_reward: -1371.3592\n",
      "Epoch 2256/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9729 - total_train_reward: -1683.0854\n",
      "Epoch 2257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2472 - total_train_reward: -1337.2368\n",
      "Epoch 2258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5440 - total_train_reward: -1069.5754\n",
      "Epoch 2259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8649 - total_train_reward: -1078.8326\n",
      "Epoch 2260/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2151 - total_train_reward: -1332.6174\n",
      "Epoch 2261/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6165 - total_train_reward: -1068.0588\n",
      "Epoch 2262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8097 - total_train_reward: -1417.2251\n",
      "Epoch 2263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7718 - total_train_reward: -936.5744\n",
      "Epoch 2264/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4148 - total_train_reward: -1368.6085\n",
      "Epoch 2265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9202 - total_train_reward: -1175.4902\n",
      "Epoch 2266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9036 - total_train_reward: -1169.1416\n",
      "Epoch 2267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0724 - total_train_reward: -1245.2107\n",
      "Epoch 2268/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9877 - total_train_reward: -1779.0490\n",
      "Epoch 2269/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8932 - total_train_reward: -850.2000\n",
      "Epoch 2270/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2900 - total_train_reward: -904.6318\n",
      "Epoch 2271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9506 - total_train_reward: -1472.2553\n",
      "Epoch 2272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7283 - total_train_reward: -1070.2715\n",
      "Epoch 2273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4493 - total_train_reward: -906.2281\n",
      "Epoch 2274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3528 - total_train_reward: -1373.9709\n",
      "Epoch 2275/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6473 - total_train_reward: -989.5564\n",
      "Epoch 2276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7133 - total_train_reward: -1093.0475\n",
      "Epoch 2277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4167 - total_train_reward: -741.6787\n",
      "Epoch 2278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5640 - total_train_reward: -1185.1529\n",
      "Epoch 2279/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3728 - total_train_reward: -1562.4349\n",
      "Epoch 2280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8524 - total_train_reward: -963.5780\n",
      "Epoch 2281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9181 - total_train_reward: -1612.0124\n",
      "Epoch 2282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2229 - total_train_reward: -1440.4162\n",
      "Epoch 2283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6940 - total_train_reward: -1250.4340\n",
      "Epoch 2284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5744 - total_train_reward: -1306.3993\n",
      "Epoch 2285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6033 - total_train_reward: -1406.3412\n",
      "Epoch 2286/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7315 - total_train_reward: -1432.9080\n",
      "Epoch 2287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1755 - total_train_reward: -957.3328\n",
      "Epoch 2288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8756 - total_train_reward: -1559.1460\n",
      "Epoch 2289/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4987 - total_train_reward: -1821.4845\n",
      "Epoch 2290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9783 - total_train_reward: -1083.0450\n",
      "Epoch 2291/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5421 - total_train_reward: -1199.5370\n",
      "Epoch 2292/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2813 - total_train_reward: -1923.5737\n",
      "Epoch 2293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4639 - total_train_reward: -770.5386\n",
      "Epoch 2294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3562 - total_train_reward: -1403.2291\n",
      "Epoch 2295/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0137 - total_train_reward: -696.0765\n",
      "Epoch 2296/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5467 - total_train_reward: -1310.4378\n",
      "Epoch 2297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2428 - total_train_reward: -859.2219\n",
      "Epoch 2298/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2092 - total_train_reward: -1269.2057\n",
      "Epoch 2299/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2358 - total_train_reward: -1570.1961\n",
      "Epoch 2300/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8863 - total_train_reward: -1156.2283\n",
      "Epoch 2301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3310 - total_train_reward: -1067.9328\n",
      "Epoch 2302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6963 - total_train_reward: -1384.3722\n",
      "Epoch 2303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5337 - total_train_reward: -850.0287\n",
      "Epoch 2304/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4012 - total_train_reward: -1273.9560\n",
      "Epoch 2305/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0849 - total_train_reward: -1312.7460\n",
      "Epoch 2306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8512 - total_train_reward: -1066.4617\n",
      "Epoch 2307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6904 - total_train_reward: -1881.0576\n",
      "Epoch 2308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7203 - total_train_reward: -1834.6254\n",
      "Epoch 2309/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9437 - total_train_reward: -1456.0846\n",
      "Epoch 2310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9991 - total_train_reward: -1338.1288\n",
      "Epoch 2311/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2313 - total_train_reward: -1041.1141\n",
      "Epoch 2312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2162 - total_train_reward: -1643.3311\n",
      "Epoch 2313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4884 - total_train_reward: -1044.6548\n",
      "Epoch 2314/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9064 - total_train_reward: -1857.1800\n",
      "Epoch 2315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4191 - total_train_reward: -1264.5041\n",
      "Epoch 2316/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2361 - total_train_reward: -1168.0058\n",
      "Epoch 2317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5833 - total_train_reward: -1068.2699\n",
      "Epoch 2318/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4597 - total_train_reward: -1050.3846\n",
      "Epoch 2319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5187 - total_train_reward: -1095.1569\n",
      "Epoch 2320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6879 - total_train_reward: -1677.4642\n",
      "Epoch 2321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1413 - total_train_reward: -1598.6235\n",
      "Epoch 2322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3748 - total_train_reward: -876.2679\n",
      "Epoch 2323/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0147 - total_train_reward: -863.4292\n",
      "Epoch 2324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9811 - total_train_reward: -1230.7651\n",
      "Epoch 2325/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4723 - total_train_reward: -1306.0086\n",
      "Epoch 2326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2295 - total_train_reward: -858.0793\n",
      "Epoch 2327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2895 - total_train_reward: -1173.2963\n",
      "Epoch 2328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1533 - total_train_reward: -1165.8680\n",
      "Epoch 2329/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4638 - total_train_reward: -1020.4118\n",
      "Epoch 2330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8686 - total_train_reward: -1793.7083\n",
      "Epoch 2331/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9114 - total_train_reward: -1453.1070\n",
      "Epoch 2332/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5983 - total_train_reward: -965.7281\n",
      "Epoch 2333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9198 - total_train_reward: -856.0750\n",
      "Epoch 2334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 19.5429 - total_train_reward: -1053.5190\n",
      "Epoch 2335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9577 - total_train_reward: -1562.0745\n",
      "Epoch 2336/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2859 - total_train_reward: -1669.1758\n",
      "Epoch 2337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3364 - total_train_reward: -1685.4139\n",
      "Epoch 2338/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2789 - total_train_reward: -1145.6234\n",
      "Epoch 2339/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2998 - total_train_reward: -1794.0352\n",
      "Epoch 2340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8639 - total_train_reward: -1363.4993\n",
      "Epoch 2341/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8238 - total_train_reward: -1693.0297\n",
      "Epoch 2342/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0168 - total_train_reward: -1068.8230\n",
      "Epoch 2343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7161 - total_train_reward: -1167.1614\n",
      "Epoch 2344/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.6149 - total_train_reward: -1057.1286\n",
      "Epoch 2345/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2400 - total_train_reward: -1068.5271\n",
      "Epoch 2346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0246 - total_train_reward: -1126.8758\n",
      "Epoch 2347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8173 - total_train_reward: -1089.0205\n",
      "Epoch 2348/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5288 - total_train_reward: -1615.1042\n",
      "Epoch 2349/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9081 - total_train_reward: -1416.8737\n",
      "Epoch 2350/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2335 - total_train_reward: -840.2504\n",
      "Epoch 2351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6262 - total_train_reward: -1244.4048\n",
      "Epoch 2352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6276 - total_train_reward: -1294.8750\n",
      "Epoch 2353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6452 - total_train_reward: -1592.2412\n",
      "Epoch 2354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.8116 - total_train_reward: -1004.4040\n",
      "Epoch 2355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2517 - total_train_reward: -958.4788\n",
      "Epoch 2356/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5774 - total_train_reward: -859.4289\n",
      "Epoch 2357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1027 - total_train_reward: -1734.8886\n",
      "Epoch 2358/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5500 - total_train_reward: -1580.4761\n",
      "Epoch 2359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0236 - total_train_reward: -965.4891\n",
      "Epoch 2360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8600 - total_train_reward: -1838.3327\n",
      "Epoch 2361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7929 - total_train_reward: -1068.2944\n",
      "Epoch 2362/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2777 - total_train_reward: -1896.7111\n",
      "Epoch 2363/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3837 - total_train_reward: -1771.3693\n",
      "Epoch 2364/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.5195 - total_train_reward: -1081.0439\n",
      "Epoch 2365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7916 - total_train_reward: -1378.2927\n",
      "Epoch 2366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9865 - total_train_reward: -970.6781\n",
      "Epoch 2367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3299 - total_train_reward: -1094.5681\n",
      "Epoch 2368/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0708 - total_train_reward: -1515.8920\n",
      "Epoch 2369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8317 - total_train_reward: -1020.4257\n",
      "Epoch 2370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3528 - total_train_reward: -1754.4592\n",
      "Epoch 2371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7641 - total_train_reward: -1847.6729\n",
      "Epoch 2372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5977 - total_train_reward: -1173.1699\n",
      "Epoch 2373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0668 - total_train_reward: -1075.5440\n",
      "Epoch 2374/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6531 - total_train_reward: -1329.8497\n",
      "Epoch 2375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5802 - total_train_reward: -1323.7377\n",
      "Epoch 2376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9747 - total_train_reward: -1777.4776\n",
      "Epoch 2377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4721 - total_train_reward: -1269.0545\n",
      "Epoch 2378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9517 - total_train_reward: -1055.9376\n",
      "Epoch 2379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1779 - total_train_reward: -1192.6027\n",
      "Epoch 2380/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4610 - total_train_reward: -980.8280\n",
      "Epoch 2381/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8858 - total_train_reward: -979.3041\n",
      "Epoch 2382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7277 - total_train_reward: -960.4326\n",
      "Epoch 2383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5845 - total_train_reward: -1788.4995\n",
      "Epoch 2384/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9890 - total_train_reward: -1098.5424\n",
      "Epoch 2385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8997 - total_train_reward: -1165.5485\n",
      "Epoch 2386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4285 - total_train_reward: -1499.7399\n",
      "Epoch 2387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9004 - total_train_reward: -1220.5800\n",
      "Epoch 2388/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9062 - total_train_reward: -1060.5863\n",
      "Epoch 2389/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1288 - total_train_reward: -1128.7130\n",
      "Epoch 2390/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8011 - total_train_reward: -1061.3884\n",
      "Epoch 2391/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6894 - total_train_reward: -1890.8495\n",
      "Epoch 2392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6095 - total_train_reward: -1576.1701\n",
      "Epoch 2393/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4881 - total_train_reward: -1058.2315\n",
      "Epoch 2394/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8234 - total_train_reward: -1661.3386\n",
      "Epoch 2395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4442 - total_train_reward: -1078.4688\n",
      "Epoch 2396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7024 - total_train_reward: -1133.1174\n",
      "Epoch 2397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1698 - total_train_reward: -1209.1261\n",
      "Epoch 2398/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0658 - total_train_reward: -1067.5807\n",
      "Epoch 2399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9908 - total_train_reward: -1813.0268\n",
      "Epoch 2400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3525 - total_train_reward: -1246.4272\n",
      "Epoch 2401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1732 - total_train_reward: -1336.8900\n",
      "Epoch 2402/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3799 - total_train_reward: -965.5002\n",
      "Epoch 2403/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1504 - total_train_reward: -1749.2427\n",
      "Epoch 2404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0068 - total_train_reward: -1402.7800\n",
      "Epoch 2405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4064 - total_train_reward: -1824.8798\n",
      "Epoch 2406/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5343 - total_train_reward: -961.8412\n",
      "Epoch 2407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1695 - total_train_reward: -1069.0279\n",
      "Epoch 2408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8843 - total_train_reward: -1652.8498\n",
      "Epoch 2409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1358 - total_train_reward: -858.9103\n",
      "Epoch 2410/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.7949 - total_train_reward: -1112.3777\n",
      "Epoch 2411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.8472 - total_train_reward: -1523.4337\n",
      "Epoch 2412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1427 - total_train_reward: -1258.5113\n",
      "Epoch 2413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1090 - total_train_reward: -1648.1463\n",
      "Epoch 2414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2922 - total_train_reward: -965.3355\n",
      "Epoch 2415/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2961 - total_train_reward: -1323.2152\n",
      "Epoch 2416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0815 - total_train_reward: -1668.0120\n",
      "Epoch 2417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4345 - total_train_reward: -1067.3458\n",
      "Epoch 2418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.9436 - total_train_reward: -1878.6011\n",
      "Epoch 2419/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6972 - total_train_reward: -1271.1795\n",
      "Epoch 2420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5650 - total_train_reward: -1344.2805\n",
      "Epoch 2421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1573 - total_train_reward: -1078.9149\n",
      "Epoch 2422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5862 - total_train_reward: -1314.8976\n",
      "Epoch 2423/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8763 - total_train_reward: -1170.6999\n",
      "Epoch 2424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3471 - total_train_reward: -1568.4087\n",
      "Epoch 2425/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0995 - total_train_reward: -1182.3036\n",
      "Epoch 2426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5301 - total_train_reward: -1506.8558\n",
      "Epoch 2427/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0770 - total_train_reward: -1682.2294\n",
      "Epoch 2428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5301 - total_train_reward: -1317.1044\n",
      "Epoch 2429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3022 - total_train_reward: -1170.2965\n",
      "Epoch 2430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4233 - total_train_reward: -1384.9706\n",
      "Epoch 2431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9272 - total_train_reward: -1169.7427\n",
      "Epoch 2432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1791 - total_train_reward: -1067.4865\n",
      "Epoch 2433/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7991 - total_train_reward: -1064.3676\n",
      "Epoch 2434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0193 - total_train_reward: -1033.7574\n",
      "Epoch 2435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9289 - total_train_reward: -1352.3638\n",
      "Epoch 2436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7391 - total_train_reward: -1619.9095\n",
      "Epoch 2437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8761 - total_train_reward: -1273.1930\n",
      "Epoch 2438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1707 - total_train_reward: -964.6374\n",
      "Epoch 2439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8403 - total_train_reward: -982.4449\n",
      "Epoch 2440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4917 - total_train_reward: -1283.2594\n",
      "Epoch 2441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6078 - total_train_reward: -1212.6092\n",
      "Epoch 2442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8560 - total_train_reward: -1431.6975\n",
      "Epoch 2443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7531 - total_train_reward: -1652.4973\n",
      "Epoch 2444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4336 - total_train_reward: -1542.3125\n",
      "Epoch 2445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1008 - total_train_reward: -1081.2809\n",
      "Epoch 2446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1733 - total_train_reward: -1479.9636\n",
      "Epoch 2447/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8283 - total_train_reward: -1161.1918\n",
      "Epoch 2448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0705 - total_train_reward: -1139.9990\n",
      "Epoch 2449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4874 - total_train_reward: -1059.6173\n",
      "Epoch 2450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2979 - total_train_reward: -1083.6846\n",
      "Epoch 2451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1545 - total_train_reward: -1201.1538\n",
      "Epoch 2452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.6553 - total_train_reward: -1589.5842\n",
      "Epoch 2453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6623 - total_train_reward: -1613.2777\n",
      "Epoch 2454/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7871 - total_train_reward: -984.0246\n",
      "Epoch 2455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9133 - total_train_reward: -739.2437\n",
      "Epoch 2456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9501 - total_train_reward: -1257.6505\n",
      "Epoch 2457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3800 - total_train_reward: -925.9814\n",
      "Epoch 2458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3109 - total_train_reward: -1486.1108\n",
      "Epoch 2459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1390 - total_train_reward: -848.2208\n",
      "Epoch 2460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1356 - total_train_reward: -1313.3593\n",
      "Epoch 2461/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.4886 - total_train_reward: -1702.2607\n",
      "Epoch 2462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7744 - total_train_reward: -1670.2820\n",
      "Epoch 2463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5265 - total_train_reward: -1614.7426\n",
      "Epoch 2464/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 22.7838 - total_train_reward: -1115.3934\n",
      "Epoch 2465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5652 - total_train_reward: -1325.2755\n",
      "Epoch 2466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4252 - total_train_reward: -1829.7847\n",
      "Epoch 2467/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1454 - total_train_reward: -1168.5609\n",
      "Epoch 2468/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7673 - total_train_reward: -1168.4061\n",
      "Epoch 2469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5179 - total_train_reward: -1529.5281\n",
      "Epoch 2470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2448 - total_train_reward: -1130.6293\n",
      "Epoch 2471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7151 - total_train_reward: -1174.4470\n",
      "Epoch 2472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5824 - total_train_reward: -1073.1832\n",
      "Epoch 2473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9540 - total_train_reward: -1544.9941\n",
      "Epoch 2474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4177 - total_train_reward: -906.0368\n",
      "Epoch 2475/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.5801 - total_train_reward: -1819.9303\n",
      "Epoch 2476/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8517 - total_train_reward: -1179.7631\n",
      "Epoch 2477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2039 - total_train_reward: -1870.8851\n",
      "Epoch 2478/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1502 - total_train_reward: -1080.9180\n",
      "Epoch 2479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5973 - total_train_reward: -1051.6957\n",
      "Epoch 2480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3980 - total_train_reward: -1286.2092\n",
      "Epoch 2481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3706 - total_train_reward: -1863.2537\n",
      "Epoch 2482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8325 - total_train_reward: -1169.5000\n",
      "Epoch 2483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7931 - total_train_reward: -1415.5522\n",
      "Epoch 2484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4789 - total_train_reward: -1112.5982\n",
      "Epoch 2485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1708 - total_train_reward: -1070.8319\n",
      "Epoch 2486/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5366 - total_train_reward: -1063.6591\n",
      "Epoch 2487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1917 - total_train_reward: -1765.8022\n",
      "Epoch 2488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5841 - total_train_reward: -1356.2541\n",
      "Epoch 2489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6997 - total_train_reward: -849.8094\n",
      "Epoch 2490/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7805 - total_train_reward: -1818.3544\n",
      "Epoch 2491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4143 - total_train_reward: -958.8716\n",
      "Epoch 2492/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.4165 - total_train_reward: -1844.4248\n",
      "Epoch 2493/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5430 - total_train_reward: -965.3818\n",
      "Epoch 2494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.8615 - total_train_reward: -1021.6631\n",
      "Epoch 2495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8521 - total_train_reward: -1304.6793\n",
      "Epoch 2496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5028 - total_train_reward: -1466.5234\n",
      "Epoch 2497/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0010 - total_train_reward: -1063.0591\n",
      "Epoch 2498/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7395 - total_train_reward: -981.0941\n",
      "Epoch 2499/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1814 - total_train_reward: -1530.4417\n",
      "Epoch 2500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3488 - total_train_reward: -1753.4986\n",
      "Epoch 2501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0384 - total_train_reward: -1186.7306\n",
      "Epoch 2502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1608 - total_train_reward: -1129.9342\n",
      "Epoch 2503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6935 - total_train_reward: -1331.6529\n",
      "Epoch 2504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5107 - total_train_reward: -1036.1879\n",
      "Epoch 2505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7072 - total_train_reward: -1085.5031\n",
      "Epoch 2506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9734 - total_train_reward: -1435.3108\n",
      "Epoch 2507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2706 - total_train_reward: -1026.3936\n",
      "Epoch 2508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4481 - total_train_reward: -1056.2116\n",
      "Epoch 2509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1817 - total_train_reward: -923.4790\n",
      "Epoch 2510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1854 - total_train_reward: -1083.3794\n",
      "Epoch 2511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6641 - total_train_reward: -1295.6621\n",
      "Epoch 2512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2563 - total_train_reward: -1067.6506\n",
      "Epoch 2513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9378 - total_train_reward: -1382.1100\n",
      "Epoch 2514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1642 - total_train_reward: -1209.2805\n",
      "Epoch 2515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7669 - total_train_reward: -1554.0092\n",
      "Epoch 2516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6051 - total_train_reward: -965.0905\n",
      "Epoch 2517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8716 - total_train_reward: -1168.6266\n",
      "Epoch 2518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7488 - total_train_reward: -839.9486\n",
      "Epoch 2519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0515 - total_train_reward: -856.9601\n",
      "Epoch 2520/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2900 - total_train_reward: -1086.8631\n",
      "Epoch 2521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.5720 - total_train_reward: -1094.2338\n",
      "Epoch 2522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1372 - total_train_reward: -1067.9803\n",
      "Epoch 2523/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4368 - total_train_reward: -1502.8126\n",
      "Epoch 2524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5512 - total_train_reward: -1311.5866\n",
      "Epoch 2525/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9119 - total_train_reward: -1710.2263\n",
      "Epoch 2526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0771 - total_train_reward: -1858.9906\n",
      "Epoch 2527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1988 - total_train_reward: -1107.1544\n",
      "Epoch 2528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3487 - total_train_reward: -969.7031\n",
      "Epoch 2529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1270 - total_train_reward: -1071.3072\n",
      "Epoch 2530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7005 - total_train_reward: -977.5939\n",
      "Epoch 2531/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6256 - total_train_reward: -1100.2283\n",
      "Epoch 2532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5096 - total_train_reward: -1167.5463\n",
      "Epoch 2533/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1372 - total_train_reward: -1004.7394\n",
      "Epoch 2534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4445 - total_train_reward: -1195.4558\n",
      "Epoch 2535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5121 - total_train_reward: -1319.2476\n",
      "Epoch 2536/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1776 - total_train_reward: -1701.5598\n",
      "Epoch 2537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6208 - total_train_reward: -1792.2500\n",
      "Epoch 2538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6988 - total_train_reward: -736.4939\n",
      "Epoch 2539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.2406 - total_train_reward: -1154.9492\n",
      "Epoch 2540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5448 - total_train_reward: -1348.3235\n",
      "Epoch 2541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6934 - total_train_reward: -1076.3122\n",
      "Epoch 2542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.6700 - total_train_reward: -608.5377\n",
      "Epoch 2543/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 23.7860 - total_train_reward: -872.1071\n",
      "Epoch 2544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4882 - total_train_reward: -735.0289\n",
      "Epoch 2545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0761 - total_train_reward: -1639.6779\n",
      "Epoch 2546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2034 - total_train_reward: -964.4574\n",
      "Epoch 2547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3475 - total_train_reward: -1012.3128\n",
      "Epoch 2548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7744 - total_train_reward: -914.9728\n",
      "Epoch 2549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0020 - total_train_reward: -1171.2043\n",
      "Epoch 2550/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5243 - total_train_reward: -1513.8195\n",
      "Epoch 2551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5132 - total_train_reward: -1632.8500\n",
      "Epoch 2552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1285 - total_train_reward: -1356.8445\n",
      "Epoch 2553/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6344 - total_train_reward: -1373.7152\n",
      "Epoch 2554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3688 - total_train_reward: -1541.1303\n",
      "Epoch 2555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5125 - total_train_reward: -1147.4995\n",
      "Epoch 2556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3100 - total_train_reward: -1240.0693\n",
      "Epoch 2557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1699 - total_train_reward: -1275.9954\n",
      "Epoch 2558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1601 - total_train_reward: -1409.3910\n",
      "Epoch 2559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7284 - total_train_reward: -1319.7385\n",
      "Epoch 2560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6791 - total_train_reward: -1540.3549\n",
      "Epoch 2561/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3732 - total_train_reward: -1400.9195\n",
      "Epoch 2562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7080 - total_train_reward: -1167.5575\n",
      "Epoch 2563/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3082 - total_train_reward: -962.1147\n",
      "Epoch 2564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8361 - total_train_reward: -1105.2752\n",
      "Epoch 2565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0002 - total_train_reward: -1847.6544\n",
      "Epoch 2566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8879 - total_train_reward: -1498.4084\n",
      "Epoch 2567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9418 - total_train_reward: -1167.3321\n",
      "Epoch 2568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3469 - total_train_reward: -1514.3187\n",
      "Epoch 2569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7749 - total_train_reward: -1001.1924\n",
      "Epoch 2570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5871 - total_train_reward: -1334.9012\n",
      "Epoch 2571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8774 - total_train_reward: -1303.4628\n",
      "Epoch 2572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6867 - total_train_reward: -1788.7392\n",
      "Epoch 2573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 18.7987 - total_train_reward: -1080.7046\n",
      "Epoch 2574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1296 - total_train_reward: -1380.8726\n",
      "Epoch 2575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2319 - total_train_reward: -1491.4065\n",
      "Epoch 2576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1247 - total_train_reward: -1143.9082\n",
      "Epoch 2577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6215 - total_train_reward: -1146.1938\n",
      "Epoch 2578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8949 - total_train_reward: -1155.9306\n",
      "Epoch 2579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1386 - total_train_reward: -1540.4707\n",
      "Epoch 2580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3491 - total_train_reward: -1617.3208\n",
      "Epoch 2581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6753 - total_train_reward: -1171.6529\n",
      "Epoch 2582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9929 - total_train_reward: -965.5303\n",
      "Epoch 2583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9017 - total_train_reward: -1365.2816\n",
      "Epoch 2584/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9988 - total_train_reward: -1102.7145\n",
      "Epoch 2585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6265 - total_train_reward: -965.9282\n",
      "Epoch 2586/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5832 - total_train_reward: -1278.7247\n",
      "Epoch 2587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0797 - total_train_reward: -1292.8621\n",
      "Epoch 2588/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9974 - total_train_reward: -967.4923\n",
      "Epoch 2589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1043 - total_train_reward: -1181.0357\n",
      "Epoch 2590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.3789 - total_train_reward: -1067.2282\n",
      "Epoch 2591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9091 - total_train_reward: -1081.4237\n",
      "Epoch 2592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1517 - total_train_reward: -1567.2290\n",
      "Epoch 2593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1170 - total_train_reward: -1543.4146\n",
      "Epoch 2594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2524 - total_train_reward: -1296.6005\n",
      "Epoch 2595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2020 - total_train_reward: -935.8375\n",
      "Epoch 2596/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2961 - total_train_reward: -1067.6102\n",
      "Epoch 2597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6058 - total_train_reward: -1371.5097\n",
      "Epoch 2598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1156 - total_train_reward: -1622.8679\n",
      "Epoch 2599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5310 - total_train_reward: -1219.8794\n",
      "Epoch 2600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9894 - total_train_reward: -1440.7969\n",
      "Epoch 2601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5002 - total_train_reward: -1502.9474\n",
      "Epoch 2602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6794 - total_train_reward: -757.6371\n",
      "Epoch 2603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.0534 - total_train_reward: -1061.1526\n",
      "Epoch 2604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0195 - total_train_reward: -1171.4891\n",
      "Epoch 2605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8142 - total_train_reward: -1072.1996\n",
      "Epoch 2606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3710 - total_train_reward: -1267.7809\n",
      "Epoch 2607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9049 - total_train_reward: -1191.2099\n",
      "Epoch 2608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0114 - total_train_reward: -1124.2783\n",
      "Epoch 2609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2482 - total_train_reward: -1343.0033\n",
      "Epoch 2610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3498 - total_train_reward: -1557.4336\n",
      "Epoch 2611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0355 - total_train_reward: -1606.5275\n",
      "Epoch 2612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7910 - total_train_reward: -1192.2395\n",
      "Epoch 2613/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2853 - total_train_reward: -1168.4618\n",
      "Epoch 2614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2265 - total_train_reward: -1224.5208\n",
      "Epoch 2615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3140 - total_train_reward: -1170.9389\n",
      "Epoch 2616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6129 - total_train_reward: -1785.7718\n",
      "Epoch 2617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3500 - total_train_reward: -1648.6804\n",
      "Epoch 2618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6060 - total_train_reward: -1736.5136\n",
      "Epoch 2619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2092 - total_train_reward: -1367.5908\n",
      "Epoch 2620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4694 - total_train_reward: -1041.1781\n",
      "Epoch 2621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8523 - total_train_reward: -1110.6690\n",
      "Epoch 2622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1782 - total_train_reward: -1080.4859\n",
      "Epoch 2623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7693 - total_train_reward: -849.1455\n",
      "Epoch 2624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3664 - total_train_reward: -972.1936\n",
      "Epoch 2625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9526 - total_train_reward: -1519.9850\n",
      "Epoch 2626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1531 - total_train_reward: -1300.2586\n",
      "Epoch 2627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7038 - total_train_reward: -1674.3085\n",
      "Epoch 2628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8426 - total_train_reward: -1707.6097\n",
      "Epoch 2629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3417 - total_train_reward: -1875.2782\n",
      "Epoch 2630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6939 - total_train_reward: -1422.6881\n",
      "Epoch 2631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5744 - total_train_reward: -1520.2300\n",
      "Epoch 2632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6770 - total_train_reward: -1202.0822\n",
      "Epoch 2633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6098 - total_train_reward: -952.3010\n",
      "Epoch 2634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5425 - total_train_reward: -1348.3414\n",
      "Epoch 2635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9772 - total_train_reward: -1557.3826\n",
      "Epoch 2636/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4672 - total_train_reward: -1481.0275\n",
      "Epoch 2637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7779 - total_train_reward: -1462.7052\n",
      "Epoch 2638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1636 - total_train_reward: -1658.3675\n",
      "Epoch 2639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9977 - total_train_reward: -1276.9895\n",
      "Epoch 2640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5347 - total_train_reward: -1073.3314\n",
      "Epoch 2641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5117 - total_train_reward: -1492.7675\n",
      "Epoch 2642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.6920 - total_train_reward: -1086.9938\n",
      "Epoch 2643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8109 - total_train_reward: -1538.2544\n",
      "Epoch 2644/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6238 - total_train_reward: -1168.2856\n",
      "Epoch 2645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9490 - total_train_reward: -839.7276\n",
      "Epoch 2646/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8866 - total_train_reward: -1054.7733\n",
      "Epoch 2647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2633 - total_train_reward: -1865.1376\n",
      "Epoch 2648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3548 - total_train_reward: -1779.2609\n",
      "Epoch 2649/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6305 - total_train_reward: -1049.5068\n",
      "Epoch 2650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2167 - total_train_reward: -1712.3083\n",
      "Epoch 2651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2678 - total_train_reward: -1170.4189\n",
      "Epoch 2652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9771 - total_train_reward: -938.1039\n",
      "Epoch 2653/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6543 - total_train_reward: -1044.2990\n",
      "Epoch 2654/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3517 - total_train_reward: -1054.4724\n",
      "Epoch 2655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3857 - total_train_reward: -1099.2574\n",
      "Epoch 2656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1203 - total_train_reward: -1349.1917\n",
      "Epoch 2657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6855 - total_train_reward: -1481.7718\n",
      "Epoch 2658/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6017 - total_train_reward: -1477.2300\n",
      "Epoch 2659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1391 - total_train_reward: -1050.8234\n",
      "Epoch 2660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6309 - total_train_reward: -1063.8917\n",
      "Epoch 2661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3395 - total_train_reward: -1384.2056\n",
      "Epoch 2662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2810 - total_train_reward: -1510.4376\n",
      "Epoch 2663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1048 - total_train_reward: -1890.2866\n",
      "Epoch 2664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5765 - total_train_reward: -1131.7608\n",
      "Epoch 2665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1377 - total_train_reward: -1130.1624\n",
      "Epoch 2666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3287 - total_train_reward: -1376.2646\n",
      "Epoch 2667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4348 - total_train_reward: -1768.5780\n",
      "Epoch 2668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2028 - total_train_reward: -1329.3641\n",
      "Epoch 2669/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4338 - total_train_reward: -1175.1371\n",
      "Epoch 2670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4251 - total_train_reward: -854.1064\n",
      "Epoch 2671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3083 - total_train_reward: -1676.5364\n",
      "Epoch 2672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8886 - total_train_reward: -1662.8177\n",
      "Epoch 2673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8458 - total_train_reward: -1178.6094\n",
      "Epoch 2674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4374 - total_train_reward: -963.8083\n",
      "Epoch 2675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4186 - total_train_reward: -1066.8301\n",
      "Epoch 2676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5607 - total_train_reward: -1300.7306\n",
      "Epoch 2677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3009 - total_train_reward: -1057.2198\n",
      "Epoch 2678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8167 - total_train_reward: -1174.3998\n",
      "Epoch 2679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1130 - total_train_reward: -1606.3020\n",
      "Epoch 2680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2491 - total_train_reward: -1222.8088\n",
      "Epoch 2681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2901 - total_train_reward: -1063.6557\n",
      "Epoch 2682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0500 - total_train_reward: -1154.1831\n",
      "Epoch 2683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5081 - total_train_reward: -1175.1623\n",
      "Epoch 2684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.8880 - total_train_reward: -1105.3962\n",
      "Epoch 2685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9982 - total_train_reward: -1441.5552\n",
      "Epoch 2686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9915 - total_train_reward: -826.6107\n",
      "Epoch 2687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3543 - total_train_reward: -1778.0684\n",
      "Epoch 2688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8048 - total_train_reward: -1219.1394\n",
      "Epoch 2689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7729 - total_train_reward: -856.4132\n",
      "Epoch 2690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3200 - total_train_reward: -1117.3287\n",
      "Epoch 2691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1283 - total_train_reward: -1442.2843\n",
      "Epoch 2692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0951 - total_train_reward: -1074.1264\n",
      "Epoch 2693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4990 - total_train_reward: -1069.6271\n",
      "Epoch 2694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1887 - total_train_reward: -1162.8309\n",
      "Epoch 2695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3585 - total_train_reward: -979.6678\n",
      "Epoch 2696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7660 - total_train_reward: -1197.8040\n",
      "Epoch 2697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8641 - total_train_reward: -1617.1575\n",
      "Epoch 2698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6156 - total_train_reward: -1168.6700\n",
      "Epoch 2699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9869 - total_train_reward: -1156.9218\n",
      "Epoch 2700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7020 - total_train_reward: -1222.6787\n",
      "Epoch 2701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7096 - total_train_reward: -1154.5960\n",
      "Epoch 2702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8709 - total_train_reward: -1059.7903\n",
      "Epoch 2703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3617 - total_train_reward: -1067.7243\n",
      "Epoch 2704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3657 - total_train_reward: -1153.8895\n",
      "Epoch 2705/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1500 - total_train_reward: -1159.3521\n",
      "Epoch 2706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6209 - total_train_reward: -1372.9494\n",
      "Epoch 2707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5446 - total_train_reward: -1112.2208\n",
      "Epoch 2708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4205 - total_train_reward: -1173.5583\n",
      "Epoch 2709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0448 - total_train_reward: -963.9478\n",
      "Epoch 2710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5919 - total_train_reward: -1418.3575\n",
      "Epoch 2711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3063 - total_train_reward: -1683.5862\n",
      "Epoch 2712/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6238 - total_train_reward: -1172.3108\n",
      "Epoch 2713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4967 - total_train_reward: -1035.1434\n",
      "Epoch 2714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9798 - total_train_reward: -1718.5303\n",
      "Epoch 2715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5329 - total_train_reward: -1099.2091\n",
      "Epoch 2716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1319 - total_train_reward: -1321.9684\n",
      "Epoch 2717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2165 - total_train_reward: -1577.7973\n",
      "Epoch 2718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7168 - total_train_reward: -1521.9306\n",
      "Epoch 2719/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6336 - total_train_reward: -1407.2561\n",
      "Epoch 2720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8841 - total_train_reward: -758.3918\n",
      "Epoch 2721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3091 - total_train_reward: -1404.5805\n",
      "Epoch 2722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9896 - total_train_reward: -1068.2027\n",
      "Epoch 2723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5386 - total_train_reward: -1460.3127\n",
      "Epoch 2724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0766 - total_train_reward: -622.8533\n",
      "Epoch 2725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.4672 - total_train_reward: -1406.1723\n",
      "Epoch 2726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1758 - total_train_reward: -1425.6170\n",
      "Epoch 2727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.5739 - total_train_reward: -736.5223\n",
      "Epoch 2728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2131 - total_train_reward: -1262.3129\n",
      "Epoch 2729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1610 - total_train_reward: -971.5295\n",
      "Epoch 2730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1288 - total_train_reward: -1526.5358\n",
      "Epoch 2731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1007 - total_train_reward: -1069.1821\n",
      "Epoch 2732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7817 - total_train_reward: -960.3803\n",
      "Epoch 2733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2171 - total_train_reward: -852.1802\n",
      "Epoch 2734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2594 - total_train_reward: -1752.2432\n",
      "Epoch 2735/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8475 - total_train_reward: -1165.0780\n",
      "Epoch 2736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4049 - total_train_reward: -1048.3498\n",
      "Epoch 2737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.3891 - total_train_reward: -1079.5501\n",
      "Epoch 2738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7743 - total_train_reward: -954.5918\n",
      "Epoch 2739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3026 - total_train_reward: -960.2899\n",
      "Epoch 2740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8951 - total_train_reward: -1053.8605\n",
      "Epoch 2741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0804 - total_train_reward: -1344.3797\n",
      "Epoch 2742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5062 - total_train_reward: -1323.3797\n",
      "Epoch 2743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2700 - total_train_reward: -1087.7149\n",
      "Epoch 2744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0839 - total_train_reward: -1066.1376\n",
      "Epoch 2745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3292 - total_train_reward: -1155.9953\n",
      "Epoch 2746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3240 - total_train_reward: -1066.7425\n",
      "Epoch 2747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8276 - total_train_reward: -1629.7840\n",
      "Epoch 2748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3594 - total_train_reward: -1680.9250\n",
      "Epoch 2749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2124 - total_train_reward: -1188.1969\n",
      "Epoch 2750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.6537 - total_train_reward: -1102.5441\n",
      "Epoch 2751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3307 - total_train_reward: -733.9899\n",
      "Epoch 2752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6352 - total_train_reward: -1291.0369\n",
      "Epoch 2753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.6944 - total_train_reward: -1134.7005\n",
      "Epoch 2754/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7553 - total_train_reward: -1170.8354\n",
      "Epoch 2755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9260 - total_train_reward: -1152.4669\n",
      "Epoch 2756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0120 - total_train_reward: -1770.3330\n",
      "Epoch 2757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9976 - total_train_reward: -855.2485\n",
      "Epoch 2758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0723 - total_train_reward: -1069.3258\n",
      "Epoch 2759/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3746 - total_train_reward: -1219.8553\n",
      "Epoch 2760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4854 - total_train_reward: -958.8517\n",
      "Epoch 2761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9447 - total_train_reward: -1289.5121\n",
      "Epoch 2762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1487 - total_train_reward: -1089.3941\n",
      "Epoch 2763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.1725 - total_train_reward: -1133.2979\n",
      "Epoch 2764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0133 - total_train_reward: -1185.6966\n",
      "Epoch 2765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3710 - total_train_reward: -1137.8656\n",
      "Epoch 2766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0121 - total_train_reward: -1446.1162\n",
      "Epoch 2767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5539 - total_train_reward: -1141.0159\n",
      "Epoch 2768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5346 - total_train_reward: -964.6628\n",
      "Epoch 2769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4292 - total_train_reward: -1524.8917\n",
      "Epoch 2770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2546 - total_train_reward: -1026.7959\n",
      "Epoch 2771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0545 - total_train_reward: -1135.0555\n",
      "Epoch 2772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6018 - total_train_reward: -1101.6483\n",
      "Epoch 2773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3154 - total_train_reward: -1084.4003\n",
      "Epoch 2774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6627 - total_train_reward: -1063.6290\n",
      "Epoch 2775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5807 - total_train_reward: -1258.5334\n",
      "Epoch 2776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3266 - total_train_reward: -1054.9663\n",
      "Epoch 2777/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3414 - total_train_reward: -1620.5479\n",
      "Epoch 2778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6541 - total_train_reward: -1481.1336\n",
      "Epoch 2779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3457 - total_train_reward: -1326.3874\n",
      "Epoch 2780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8289 - total_train_reward: -1131.6710\n",
      "Epoch 2781/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0729 - total_train_reward: -1435.2956\n",
      "Epoch 2782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0036 - total_train_reward: -969.1154\n",
      "Epoch 2783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3002 - total_train_reward: -1124.0501\n",
      "Epoch 2784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6890 - total_train_reward: -1528.1179\n",
      "Epoch 2785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8010 - total_train_reward: -1641.5107\n",
      "Epoch 2786/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9406 - total_train_reward: -1290.0452\n",
      "Epoch 2787/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6478 - total_train_reward: -1154.7042\n",
      "Epoch 2788/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7815 - total_train_reward: -1517.4884\n",
      "Epoch 2789/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2775 - total_train_reward: -1102.8916\n",
      "Epoch 2790/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9314 - total_train_reward: -1309.9896\n",
      "Epoch 2791/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0896 - total_train_reward: -1058.6252\n",
      "Epoch 2792/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6904 - total_train_reward: -1692.0967\n",
      "Epoch 2793/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3507 - total_train_reward: -1314.8445\n",
      "Epoch 2794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8441 - total_train_reward: -1812.5925\n",
      "Epoch 2795/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3643 - total_train_reward: -975.1759\n",
      "Epoch 2796/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8124 - total_train_reward: -1321.5654\n",
      "Epoch 2797/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8454 - total_train_reward: -1322.5229\n",
      "Epoch 2798/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9940 - total_train_reward: -1722.3528\n",
      "Epoch 2799/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4485 - total_train_reward: -1189.0693\n",
      "Epoch 2800/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4260 - total_train_reward: -1661.8662\n",
      "Epoch 2801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5372 - total_train_reward: -1070.2305\n",
      "Epoch 2802/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6301 - total_train_reward: -1607.1927\n",
      "Epoch 2803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6341 - total_train_reward: -1072.7940\n",
      "Epoch 2804/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0036 - total_train_reward: -1260.3930\n",
      "Epoch 2805/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9683 - total_train_reward: -1260.4806\n",
      "Epoch 2806/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3683 - total_train_reward: -1292.9594\n",
      "Epoch 2807/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6177 - total_train_reward: -1674.6595\n",
      "Epoch 2808/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1590 - total_train_reward: -731.3922\n",
      "Epoch 2809/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.7063 - total_train_reward: -902.4818\n",
      "Epoch 2810/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2061 - total_train_reward: -1460.4186\n",
      "Epoch 2811/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.0771 - total_train_reward: -1066.2847\n",
      "Epoch 2812/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6112 - total_train_reward: -1070.5049\n",
      "Epoch 2813/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6385 - total_train_reward: -1460.5927\n",
      "Epoch 2814/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5569 - total_train_reward: -1513.7889\n",
      "Epoch 2815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8625 - total_train_reward: -1410.5695\n",
      "Epoch 2816/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1193 - total_train_reward: -1141.9071\n",
      "Epoch 2817/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4599 - total_train_reward: -1486.8836\n",
      "Epoch 2818/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1303 - total_train_reward: -1126.4143\n",
      "Epoch 2819/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1403 - total_train_reward: -1517.3578\n",
      "Epoch 2820/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5451 - total_train_reward: -1795.6657\n",
      "Epoch 2821/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4436 - total_train_reward: -904.6775\n",
      "Epoch 2822/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7148 - total_train_reward: -1468.3122\n",
      "Epoch 2823/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7889 - total_train_reward: -1049.5637\n",
      "Epoch 2824/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6853 - total_train_reward: -1739.3754\n",
      "Epoch 2825/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6616 - total_train_reward: -1209.2791\n",
      "Epoch 2826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8138 - total_train_reward: -1722.6984\n",
      "Epoch 2827/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0145 - total_train_reward: -735.3041\n",
      "Epoch 2828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8785 - total_train_reward: -1189.7574\n",
      "Epoch 2829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3417 - total_train_reward: -1144.6306\n",
      "Epoch 2830/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6317 - total_train_reward: -1049.0732\n",
      "Epoch 2831/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8071 - total_train_reward: -1191.9577\n",
      "Epoch 2832/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3907 - total_train_reward: -1614.0777\n",
      "Epoch 2833/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5391 - total_train_reward: -1862.4743\n",
      "Epoch 2834/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4623 - total_train_reward: -1730.7720\n",
      "Epoch 2835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5522 - total_train_reward: -1109.0398\n",
      "Epoch 2836/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7194 - total_train_reward: -1245.9817\n",
      "Epoch 2837/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2956 - total_train_reward: -1652.5500\n",
      "Epoch 2838/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6114 - total_train_reward: -1331.8679\n",
      "Epoch 2839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2749 - total_train_reward: -1166.8047\n",
      "Epoch 2840/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2570 - total_train_reward: -1132.0372\n",
      "Epoch 2841/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6293 - total_train_reward: -1052.7597\n",
      "Epoch 2842/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2787 - total_train_reward: -1288.7814\n",
      "Epoch 2843/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5805 - total_train_reward: -1145.2596\n",
      "Epoch 2844/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0330 - total_train_reward: -1130.7137\n",
      "Epoch 2845/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.8226 - total_train_reward: -1696.3232\n",
      "Epoch 2846/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9272 - total_train_reward: -1438.2343\n",
      "Epoch 2847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7861 - total_train_reward: -869.6155\n",
      "Epoch 2848/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1527 - total_train_reward: -1067.5532\n",
      "Epoch 2849/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.8863 - total_train_reward: -609.2040\n",
      "Epoch 2850/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 54.3994 - total_train_reward: -1079.3292\n",
      "Epoch 2851/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9189 - total_train_reward: -1188.0221\n",
      "Epoch 2852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7101 - total_train_reward: -1189.8634\n",
      "Epoch 2853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9150 - total_train_reward: -1483.2910\n",
      "Epoch 2854/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6628 - total_train_reward: -1294.1255\n",
      "Epoch 2855/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6630 - total_train_reward: -1579.1200\n",
      "Epoch 2856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9619 - total_train_reward: -1293.1987\n",
      "Epoch 2857/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4786 - total_train_reward: -1815.1487\n",
      "Epoch 2858/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3346 - total_train_reward: -1250.0291\n",
      "Epoch 2859/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7278 - total_train_reward: -1176.5475\n",
      "Epoch 2860/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.8546 - total_train_reward: -1072.0884\n",
      "Epoch 2861/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.8214 - total_train_reward: -1437.5935\n",
      "Epoch 2862/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8802 - total_train_reward: -1166.1637\n",
      "Epoch 2863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3528 - total_train_reward: -1179.0845\n",
      "Epoch 2864/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8603 - total_train_reward: -1134.0573\n",
      "Epoch 2865/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2133 - total_train_reward: -1261.9175\n",
      "Epoch 2866/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0783 - total_train_reward: -1793.3631\n",
      "Epoch 2867/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9553 - total_train_reward: -1697.8989\n",
      "Epoch 2868/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4126 - total_train_reward: -969.2726\n",
      "Epoch 2869/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4033 - total_train_reward: -1605.4192\n",
      "Epoch 2870/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4059 - total_train_reward: -1170.3743\n",
      "Epoch 2871/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6564 - total_train_reward: -1131.8263\n",
      "Epoch 2872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1394 - total_train_reward: -862.2588\n",
      "Epoch 2873/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.6472 - total_train_reward: -1834.5420\n",
      "Epoch 2874/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4419 - total_train_reward: -1328.1527\n",
      "Epoch 2875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4910 - total_train_reward: -1311.4153\n",
      "Epoch 2876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2082 - total_train_reward: -1433.2201\n",
      "Epoch 2877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4767 - total_train_reward: -1752.8085\n",
      "Epoch 2878/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3971 - total_train_reward: -1050.1310\n",
      "Epoch 2879/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8886 - total_train_reward: -1092.7715\n",
      "Epoch 2880/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6051 - total_train_reward: -1403.5620\n",
      "Epoch 2881/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7822 - total_train_reward: -1080.4420\n",
      "Epoch 2882/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6602 - total_train_reward: -1651.7882\n",
      "Epoch 2883/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0161 - total_train_reward: -1798.4972\n",
      "Epoch 2884/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1589 - total_train_reward: -780.0941\n",
      "Epoch 2885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2593 - total_train_reward: -921.4226\n",
      "Epoch 2886/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6262 - total_train_reward: -1044.3253\n",
      "Epoch 2887/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7300 - total_train_reward: -1052.1982\n",
      "Epoch 2888/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.7627 - total_train_reward: -1598.3839\n",
      "Epoch 2889/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3327 - total_train_reward: -1314.6659\n",
      "Epoch 2890/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2566 - total_train_reward: -1398.2973\n",
      "Epoch 2891/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4127 - total_train_reward: -900.3286\n",
      "Epoch 2892/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0656 - total_train_reward: -1860.8656\n",
      "Epoch 2893/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7159 - total_train_reward: -1540.3059\n",
      "Epoch 2894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.1123 - total_train_reward: -1053.0282\n",
      "Epoch 2895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0212 - total_train_reward: -1051.3599\n",
      "Epoch 2896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6997 - total_train_reward: -1255.7862\n",
      "Epoch 2897/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0285 - total_train_reward: -1305.0177\n",
      "Epoch 2898/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2158 - total_train_reward: -1164.5577\n",
      "Epoch 2899/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5071 - total_train_reward: -1861.7631\n",
      "Epoch 2900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5933 - total_train_reward: -1803.6974\n",
      "Epoch 2901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9470 - total_train_reward: -1557.7191\n",
      "Epoch 2902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2758 - total_train_reward: -1062.4000\n",
      "Epoch 2903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1807 - total_train_reward: -856.8238\n",
      "Epoch 2904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2864 - total_train_reward: -1050.6072\n",
      "Epoch 2905/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1508 - total_train_reward: -1068.6544\n",
      "Epoch 2906/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5711 - total_train_reward: -1388.5283\n",
      "Epoch 2907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8613 - total_train_reward: -1246.0377\n",
      "Epoch 2908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1522 - total_train_reward: -1192.3438\n",
      "Epoch 2909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5433 - total_train_reward: -1351.7237\n",
      "Epoch 2910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0822 - total_train_reward: -1049.3367\n",
      "Epoch 2911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1946 - total_train_reward: -1060.4826\n",
      "Epoch 2912/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0497 - total_train_reward: -1584.9255\n",
      "Epoch 2913/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4125 - total_train_reward: -1293.0220\n",
      "Epoch 2914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7931 - total_train_reward: -1072.5413\n",
      "Epoch 2915/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4494 - total_train_reward: -1523.9447\n",
      "Epoch 2916/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8319 - total_train_reward: -1467.4421\n",
      "Epoch 2917/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0214 - total_train_reward: -1786.1350\n",
      "Epoch 2918/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0322 - total_train_reward: -1043.0613\n",
      "Epoch 2919/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3866 - total_train_reward: -1046.2301\n",
      "Epoch 2920/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5777 - total_train_reward: -1791.1907\n",
      "Epoch 2921/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.6516 - total_train_reward: -1031.0703\n",
      "Epoch 2922/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7615 - total_train_reward: -885.2358\n",
      "Epoch 2923/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3609 - total_train_reward: -1179.3243\n",
      "Epoch 2924/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4831 - total_train_reward: -1318.1203\n",
      "Epoch 2925/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3686 - total_train_reward: -1726.2788\n",
      "Epoch 2926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2947 - total_train_reward: -1844.5121\n",
      "Epoch 2927/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5628 - total_train_reward: -1541.1029\n",
      "Epoch 2928/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7773 - total_train_reward: -1450.2098\n",
      "Epoch 2929/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9839 - total_train_reward: -1800.7101\n",
      "Epoch 2930/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6694 - total_train_reward: -1053.0012\n",
      "Epoch 2931/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8608 - total_train_reward: -1045.1601\n",
      "Epoch 2932/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5651 - total_train_reward: -961.6270\n",
      "Epoch 2933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6224 - total_train_reward: -1308.4207\n",
      "Epoch 2934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9607 - total_train_reward: -1276.4251\n",
      "Epoch 2935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4247 - total_train_reward: -1319.7240\n",
      "Epoch 2936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1886 - total_train_reward: -1627.9979\n",
      "Epoch 2937/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0541 - total_train_reward: -1065.7227\n",
      "Epoch 2938/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7131 - total_train_reward: -907.1008\n",
      "Epoch 2939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2082 - total_train_reward: -1638.0628\n",
      "Epoch 2940/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5070 - total_train_reward: -812.5564\n",
      "Epoch 2941/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8851 - total_train_reward: -1045.0976\n",
      "Epoch 2942/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.1843 - total_train_reward: -1588.5537\n",
      "Epoch 2943/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7504 - total_train_reward: -852.7778\n",
      "Epoch 2944/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1112 - total_train_reward: -1047.9913\n",
      "Epoch 2945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3657 - total_train_reward: -1784.0663\n",
      "Epoch 2946/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3856 - total_train_reward: -1466.7893\n",
      "Epoch 2947/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1245 - total_train_reward: -1434.8042\n",
      "Epoch 2948/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0027 - total_train_reward: -1158.0312\n",
      "Epoch 2949/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8411 - total_train_reward: -1174.7303\n",
      "Epoch 2950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0212 - total_train_reward: -1275.8368\n",
      "Epoch 2951/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7061 - total_train_reward: -1661.6791\n",
      "Epoch 2952/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1742 - total_train_reward: -1653.5197\n",
      "Epoch 2953/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7974 - total_train_reward: -861.0089\n",
      "Epoch 2954/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5916 - total_train_reward: -1369.2785\n",
      "Epoch 2955/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4504 - total_train_reward: -1045.1023\n",
      "Epoch 2956/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8910 - total_train_reward: -1294.3142\n",
      "Epoch 2957/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -18.0254 - total_train_reward: -730.3164\n",
      "Epoch 2958/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1669 - total_train_reward: -1072.5220\n",
      "Epoch 2959/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8512 - total_train_reward: -960.1263\n",
      "Epoch 2960/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.6798 - total_train_reward: -1046.6556\n",
      "Epoch 2961/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9086 - total_train_reward: -974.3829\n",
      "Epoch 2962/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.4301 - total_train_reward: -1793.2982\n",
      "Epoch 2963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5812 - total_train_reward: -1613.4198\n",
      "Epoch 2964/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6870 - total_train_reward: -1043.7272\n",
      "Epoch 2965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4899 - total_train_reward: -1187.4719\n",
      "Epoch 2966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2296 - total_train_reward: -1483.4936\n",
      "Epoch 2967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5997 - total_train_reward: -1393.3823\n",
      "Epoch 2968/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3168 - total_train_reward: -901.6596\n",
      "Epoch 2969/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0692 - total_train_reward: -1564.4294\n",
      "Epoch 2970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3643 - total_train_reward: -896.5108\n",
      "Epoch 2971/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2602 - total_train_reward: -1178.6857\n",
      "Epoch 2972/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9441 - total_train_reward: -1136.7802\n",
      "Epoch 2973/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4203 - total_train_reward: -1369.5378\n",
      "Epoch 2974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2539 - total_train_reward: -1169.9096\n",
      "Epoch 2975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9888 - total_train_reward: -1036.5722\n",
      "Epoch 2976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7039 - total_train_reward: -1262.4014\n",
      "Epoch 2977/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3774 - total_train_reward: -1001.8819\n",
      "Epoch 2978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5738 - total_train_reward: -1534.4719\n",
      "Epoch 2979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6094 - total_train_reward: -1302.1549\n",
      "Epoch 2980/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1181 - total_train_reward: -1038.6420\n",
      "Epoch 2981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4959 - total_train_reward: -1074.9982\n",
      "Epoch 2982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1900 - total_train_reward: -1171.7117\n",
      "Epoch 2983/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8297 - total_train_reward: -1478.8880\n",
      "Epoch 2984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2907 - total_train_reward: -988.3183\n",
      "Epoch 2985/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4988 - total_train_reward: -1477.3468\n",
      "Epoch 2986/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7586 - total_train_reward: -1729.2119\n",
      "Epoch 2987/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3100 - total_train_reward: -1899.0675\n",
      "Epoch 2988/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0183 - total_train_reward: -1471.0912\n",
      "Epoch 2989/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1998 - total_train_reward: -1012.1764\n",
      "Epoch 2990/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2200 - total_train_reward: -1043.7519\n",
      "Epoch 2991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6815 - total_train_reward: -1773.9057\n",
      "Epoch 2992/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8789 - total_train_reward: -1048.5356\n",
      "Epoch 2993/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6018 - total_train_reward: -1049.7278\n",
      "Epoch 2994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3573 - total_train_reward: -1455.2932\n",
      "Epoch 2995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0492 - total_train_reward: -1709.8279\n",
      "Epoch 2996/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4488 - total_train_reward: -1045.7848\n",
      "Epoch 2997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2762 - total_train_reward: -1219.6258\n",
      "Epoch 2998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5303 - total_train_reward: -1063.4448\n",
      "Epoch 2999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.3807 - total_train_reward: -1915.7457\n",
      "Epoch 3000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5413 - total_train_reward: -948.2528\n",
      "Epoch 3001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7411 - total_train_reward: -1472.4641\n",
      "Epoch 3002/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7751 - total_train_reward: -739.9081\n",
      "Epoch 3003/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5002 - total_train_reward: -1170.2623\n",
      "Epoch 3004/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1011 - total_train_reward: -930.7229\n",
      "Epoch 3005/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5949 - total_train_reward: -1819.6905\n",
      "Epoch 3006/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0270 - total_train_reward: -1648.9051\n",
      "Epoch 3007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7496 - total_train_reward: -856.8006\n",
      "Epoch 3008/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4590 - total_train_reward: -1617.8101\n",
      "Epoch 3009/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6892 - total_train_reward: -1183.4197\n",
      "Epoch 3010/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5141 - total_train_reward: -1070.7451\n",
      "Epoch 3011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2351 - total_train_reward: -1784.0799\n",
      "Epoch 3012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7704 - total_train_reward: -1171.4043\n",
      "Epoch 3013/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1156 - total_train_reward: -1541.9566\n",
      "Epoch 3014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1871 - total_train_reward: -1047.2908\n",
      "Epoch 3015/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8125 - total_train_reward: -1070.5311\n",
      "Epoch 3016/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7213 - total_train_reward: -1643.2102\n",
      "Epoch 3017/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3142 - total_train_reward: -1470.6961\n",
      "Epoch 3018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4082 - total_train_reward: -1479.5436\n",
      "Epoch 3019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8038 - total_train_reward: -1072.3089\n",
      "Epoch 3020/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4907 - total_train_reward: -1168.1149\n",
      "Epoch 3021/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4574 - total_train_reward: -966.6324\n",
      "Epoch 3022/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8816 - total_train_reward: -925.4155\n",
      "Epoch 3023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5627 - total_train_reward: -1684.2994\n",
      "Epoch 3024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6115 - total_train_reward: -868.7432\n",
      "Epoch 3025/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9074 - total_train_reward: -1286.0721\n",
      "Epoch 3026/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2644 - total_train_reward: -1151.5868\n",
      "Epoch 3027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2059 - total_train_reward: -1228.0594\n",
      "Epoch 3028/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3420 - total_train_reward: -737.6056\n",
      "Epoch 3029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7727 - total_train_reward: -1292.0143\n",
      "Epoch 3030/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9403 - total_train_reward: -1447.8175\n",
      "Epoch 3031/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7114 - total_train_reward: -1006.6270\n",
      "Epoch 3032/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2856 - total_train_reward: -1314.1756\n",
      "Epoch 3033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4328 - total_train_reward: -1071.5430\n",
      "Epoch 3034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5941 - total_train_reward: -1046.0298\n",
      "Epoch 3035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3535 - total_train_reward: -1280.2117\n",
      "Epoch 3036/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2127 - total_train_reward: -1033.0861\n",
      "Epoch 3037/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8282 - total_train_reward: -1129.2624\n",
      "Epoch 3038/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0616 - total_train_reward: -1895.4793\n",
      "Epoch 3039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6307 - total_train_reward: -856.8568\n",
      "Epoch 3040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7903 - total_train_reward: -1842.7738\n",
      "Epoch 3041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4361 - total_train_reward: -1044.4517\n",
      "Epoch 3042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7257 - total_train_reward: -1045.2237\n",
      "Epoch 3043/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9525 - total_train_reward: -1050.2091\n",
      "Epoch 3044/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9102 - total_train_reward: -1042.2988\n",
      "Epoch 3045/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5052 - total_train_reward: -1308.6824\n",
      "Epoch 3046/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9776 - total_train_reward: -855.8599\n",
      "Epoch 3047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3099 - total_train_reward: -1023.7472\n",
      "Epoch 3048/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2275 - total_train_reward: -1701.6460\n",
      "Epoch 3049/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2000 - total_train_reward: -1536.2062\n",
      "Epoch 3050/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3216 - total_train_reward: -1295.2301\n",
      "Epoch 3051/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4439 - total_train_reward: -1298.6235\n",
      "Epoch 3052/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3328 - total_train_reward: -1160.6109\n",
      "Epoch 3053/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1100 - total_train_reward: -1892.3415\n",
      "Epoch 3054/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4683 - total_train_reward: -1019.9950\n",
      "Epoch 3055/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1268 - total_train_reward: -1466.3316\n",
      "Epoch 3056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2801 - total_train_reward: -1709.2056\n",
      "Epoch 3057/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0004 - total_train_reward: -1179.3832\n",
      "Epoch 3058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8609 - total_train_reward: -1013.4797\n",
      "Epoch 3059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0070 - total_train_reward: -773.4971\n",
      "Epoch 3060/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9208 - total_train_reward: -968.0948\n",
      "Epoch 3061/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -16.1770 - total_train_reward: -1554.3963\n",
      "Epoch 3062/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7840 - total_train_reward: -1349.2320\n",
      "Epoch 3063/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.2041 - total_train_reward: -494.9843\n",
      "Epoch 3064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5431 - total_train_reward: -1369.0038\n",
      "Epoch 3065/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.9607 - total_train_reward: -861.6300\n",
      "Epoch 3066/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6642 - total_train_reward: -1589.4856\n",
      "Epoch 3067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0253 - total_train_reward: -1083.0502\n",
      "Epoch 3068/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6981 - total_train_reward: -1432.8194\n",
      "Epoch 3069/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9546 - total_train_reward: -1202.0065\n",
      "Epoch 3070/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1311 - total_train_reward: -1042.1080\n",
      "Epoch 3071/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5444 - total_train_reward: -1678.9588\n",
      "Epoch 3072/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6489 - total_train_reward: -1824.9989\n",
      "Epoch 3073/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2268 - total_train_reward: -1718.3774\n",
      "Epoch 3074/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.8667 - total_train_reward: -1316.0009\n",
      "Epoch 3075/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5055 - total_train_reward: -1035.1111\n",
      "Epoch 3076/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6161 - total_train_reward: -1164.6108\n",
      "Epoch 3077/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4421 - total_train_reward: -1729.7155\n",
      "Epoch 3078/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1481 - total_train_reward: -1500.1793\n",
      "Epoch 3079/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1477 - total_train_reward: -1356.1163\n",
      "Epoch 3080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8359 - total_train_reward: -1920.0137\n",
      "Epoch 3081/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5627 - total_train_reward: -1171.5862\n",
      "Epoch 3082/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.1675 - total_train_reward: -1845.7598\n",
      "Epoch 3083/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4300 - total_train_reward: -936.0984\n",
      "Epoch 3084/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7150 - total_train_reward: -1848.2734\n",
      "Epoch 3085/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7030 - total_train_reward: -967.0896\n",
      "Epoch 3086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5124 - total_train_reward: -1173.8506\n",
      "Epoch 3087/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3112 - total_train_reward: -1298.5149\n",
      "Epoch 3088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.0865 - total_train_reward: -1038.9709\n",
      "Epoch 3089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7474 - total_train_reward: -1046.7032\n",
      "Epoch 3090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1697 - total_train_reward: -1154.8457\n",
      "Epoch 3091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6442 - total_train_reward: -1157.7845\n",
      "Epoch 3092/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5054 - total_train_reward: -1214.1101\n",
      "Epoch 3093/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1406 - total_train_reward: -1176.6550\n",
      "Epoch 3094/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8960 - total_train_reward: -1141.6419\n",
      "Epoch 3095/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3419 - total_train_reward: -1049.7361\n",
      "Epoch 3096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9771 - total_train_reward: -1043.3663\n",
      "Epoch 3097/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3425 - total_train_reward: -1070.4131\n",
      "Epoch 3098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6569 - total_train_reward: -1762.2794\n",
      "Epoch 3099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3326 - total_train_reward: -1484.2433\n",
      "Epoch 3100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6245 - total_train_reward: -857.1172\n",
      "Epoch 3101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7395 - total_train_reward: -1560.8716\n",
      "Epoch 3102/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8114 - total_train_reward: -1531.5277\n",
      "Epoch 3103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8607 - total_train_reward: -1665.5384\n",
      "Epoch 3104/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1828 - total_train_reward: -1062.4880\n",
      "Epoch 3105/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7745 - total_train_reward: -1289.6396\n",
      "Epoch 3106/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7658 - total_train_reward: -971.4590\n",
      "Epoch 3107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8081 - total_train_reward: -974.2861\n",
      "Epoch 3108/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7441 - total_train_reward: -1762.5927\n",
      "Epoch 3109/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2494 - total_train_reward: -864.7357\n",
      "Epoch 3110/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0434 - total_train_reward: -1283.3648\n",
      "Epoch 3111/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2076 - total_train_reward: -1009.5202\n",
      "Epoch 3112/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6598 - total_train_reward: -1632.5859\n",
      "Epoch 3113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6694 - total_train_reward: -1226.4946\n",
      "Epoch 3114/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3055 - total_train_reward: -992.2428\n",
      "Epoch 3115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5658 - total_train_reward: -1595.3692\n",
      "Epoch 3116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3200 - total_train_reward: -1139.8822\n",
      "Epoch 3117/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7642 - total_train_reward: -1472.3527\n",
      "Epoch 3118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3339 - total_train_reward: -968.5238\n",
      "Epoch 3119/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2449 - total_train_reward: -1825.3584\n",
      "Epoch 3120/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4514 - total_train_reward: -763.4305\n",
      "Epoch 3121/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3666 - total_train_reward: -968.6173\n",
      "Epoch 3122/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7200 - total_train_reward: -961.5689\n",
      "Epoch 3123/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.6327 - total_train_reward: -1591.4871\n",
      "Epoch 3124/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8210 - total_train_reward: -1059.2243\n",
      "Epoch 3125/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9759 - total_train_reward: -1694.8276\n",
      "Epoch 3126/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4808 - total_train_reward: -1077.3308\n",
      "Epoch 3127/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5955 - total_train_reward: -1284.1061\n",
      "Epoch 3128/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5261 - total_train_reward: -1472.1198\n",
      "Epoch 3129/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1488 - total_train_reward: -1568.0174\n",
      "Epoch 3130/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2250 - total_train_reward: -1559.6007\n",
      "Epoch 3131/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8643 - total_train_reward: -1427.7366\n",
      "Epoch 3132/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6756 - total_train_reward: -1496.3060\n",
      "Epoch 3133/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7322 - total_train_reward: -1062.2404\n",
      "Epoch 3134/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2770 - total_train_reward: -970.7904\n",
      "Epoch 3135/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5520 - total_train_reward: -1321.5542\n",
      "Epoch 3136/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7965 - total_train_reward: -1750.4927\n",
      "Epoch 3137/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9935 - total_train_reward: -1161.9429\n",
      "Epoch 3138/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1042 - total_train_reward: -1729.6449\n",
      "Epoch 3139/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9013 - total_train_reward: -1182.7048\n",
      "Epoch 3140/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4462 - total_train_reward: -900.7555\n",
      "Epoch 3141/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3160 - total_train_reward: -1249.5417\n",
      "Epoch 3142/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4750 - total_train_reward: -1282.3251\n",
      "Epoch 3143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7916 - total_train_reward: -1049.6441\n",
      "Epoch 3144/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0414 - total_train_reward: -1069.3656\n",
      "Epoch 3145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5878 - total_train_reward: -1331.2879\n",
      "Epoch 3146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2201 - total_train_reward: -1610.2036\n",
      "Epoch 3147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0872 - total_train_reward: -955.5910\n",
      "Epoch 3148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5063 - total_train_reward: -1047.2015\n",
      "Epoch 3149/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0433 - total_train_reward: -1171.6034\n",
      "Epoch 3150/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4261 - total_train_reward: -1523.4939\n",
      "Epoch 3151/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3450 - total_train_reward: -1275.4776\n",
      "Epoch 3152/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1866 - total_train_reward: -771.1969\n",
      "Epoch 3153/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6556 - total_train_reward: -1168.6257\n",
      "Epoch 3154/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1887 - total_train_reward: -1318.1626\n",
      "Epoch 3155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9388 - total_train_reward: -1383.6531\n",
      "Epoch 3156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7096 - total_train_reward: -1524.7775\n",
      "Epoch 3157/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9405 - total_train_reward: -1467.9335\n",
      "Epoch 3158/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.4236 - total_train_reward: -935.2191\n",
      "Epoch 3159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6316 - total_train_reward: -969.5845\n",
      "Epoch 3160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7343 - total_train_reward: -1396.3195\n",
      "Epoch 3161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4013 - total_train_reward: -1802.2870\n",
      "Epoch 3162/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4594 - total_train_reward: -1645.4829\n",
      "Epoch 3163/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5692 - total_train_reward: -1298.3102\n",
      "Epoch 3164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2245 - total_train_reward: -1171.3882\n",
      "Epoch 3165/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6847 - total_train_reward: -1113.3854\n",
      "Epoch 3166/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6521 - total_train_reward: -1813.3807\n",
      "Epoch 3167/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2257 - total_train_reward: -1243.0775\n",
      "Epoch 3168/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4799 - total_train_reward: -1337.1400\n",
      "Epoch 3169/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5173 - total_train_reward: -1017.1945\n",
      "Epoch 3170/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7030 - total_train_reward: -818.4587\n",
      "Epoch 3171/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6528 - total_train_reward: -1295.7260\n",
      "Epoch 3172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7513 - total_train_reward: -1308.4479\n",
      "Epoch 3173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7097 - total_train_reward: -1500.9526\n",
      "Epoch 3174/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2507 - total_train_reward: -926.6622\n",
      "Epoch 3175/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4468 - total_train_reward: -1303.3093\n",
      "Epoch 3176/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2541 - total_train_reward: -1221.0525\n",
      "Epoch 3177/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4447 - total_train_reward: -873.3766\n",
      "Epoch 3178/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6374 - total_train_reward: -1624.5308\n",
      "Epoch 3179/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0383 - total_train_reward: -1187.8823\n",
      "Epoch 3180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2774 - total_train_reward: -1352.9433\n",
      "Epoch 3181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0934 - total_train_reward: -1698.9013\n",
      "Epoch 3182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2194 - total_train_reward: -1174.6496\n",
      "Epoch 3183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7531 - total_train_reward: -1364.6982\n",
      "Epoch 3184/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9283 - total_train_reward: -1274.8377\n",
      "Epoch 3185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6914 - total_train_reward: -1517.2773\n",
      "Epoch 3186/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5754 - total_train_reward: -1269.9691\n",
      "Epoch 3187/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8134 - total_train_reward: -1164.4656\n",
      "Epoch 3188/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2848 - total_train_reward: -1515.0803\n",
      "Epoch 3189/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9104 - total_train_reward: -1176.6467\n",
      "Epoch 3190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9800 - total_train_reward: -1322.2821\n",
      "Epoch 3191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9289 - total_train_reward: -1492.5349\n",
      "Epoch 3192/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1554 - total_train_reward: -907.8878\n",
      "Epoch 3193/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5492 - total_train_reward: -1681.1090\n",
      "Epoch 3194/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0002 - total_train_reward: -1160.4141\n",
      "Epoch 3195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8279 - total_train_reward: -1689.5729\n",
      "Epoch 3196/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 21.9911 - total_train_reward: -1051.1275\n",
      "Epoch 3197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9914 - total_train_reward: -1251.8468\n",
      "Epoch 3198/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8017 - total_train_reward: -1606.2401\n",
      "Epoch 3199/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6271 - total_train_reward: -1067.3898\n",
      "Epoch 3200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3980 - total_train_reward: -935.9192\n",
      "Epoch 3201/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3232 - total_train_reward: -1820.4056\n",
      "Epoch 3202/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3417 - total_train_reward: -1359.0004\n",
      "Epoch 3203/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1895 - total_train_reward: -988.3118\n",
      "Epoch 3204/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8413 - total_train_reward: -1808.8894\n",
      "Epoch 3205/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5144 - total_train_reward: -1053.0232\n",
      "Epoch 3206/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7156 - total_train_reward: -1316.5931\n",
      "Epoch 3207/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1160 - total_train_reward: -967.5613\n",
      "Epoch 3208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8837 - total_train_reward: -1648.6147\n",
      "Epoch 3209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4284 - total_train_reward: -969.8794\n",
      "Epoch 3210/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1602 - total_train_reward: -1043.2842\n",
      "Epoch 3211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7297 - total_train_reward: -1269.8898\n",
      "Epoch 3212/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2218 - total_train_reward: -1697.7710\n",
      "Epoch 3213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0991 - total_train_reward: -1027.8718\n",
      "Epoch 3214/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1254 - total_train_reward: -1203.9092\n",
      "Epoch 3215/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5567 - total_train_reward: -1737.3435\n",
      "Epoch 3216/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9914 - total_train_reward: -1512.1528\n",
      "Epoch 3217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7143 - total_train_reward: -758.1060\n",
      "Epoch 3218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2374 - total_train_reward: -1392.5280\n",
      "Epoch 3219/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4958 - total_train_reward: -1167.4859\n",
      "Epoch 3220/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8050 - total_train_reward: -1662.7629\n",
      "Epoch 3221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2091 - total_train_reward: -1196.6264\n",
      "Epoch 3222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8543 - total_train_reward: -1137.7783\n",
      "Epoch 3223/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4878 - total_train_reward: -751.1440\n",
      "Epoch 3224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1849 - total_train_reward: -858.1710\n",
      "Epoch 3225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3803 - total_train_reward: -1563.1174\n",
      "Epoch 3226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2508 - total_train_reward: -1412.8135\n",
      "Epoch 3227/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9405 - total_train_reward: -1776.4018\n",
      "Epoch 3228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6239 - total_train_reward: -973.3376\n",
      "Epoch 3229/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.7889 - total_train_reward: -1038.4116\n",
      "Epoch 3230/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9151 - total_train_reward: -1728.0662\n",
      "Epoch 3231/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3818 - total_train_reward: -903.5004\n",
      "Epoch 3232/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4686 - total_train_reward: -1536.6716\n",
      "Epoch 3233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5087 - total_train_reward: -1926.1879\n",
      "Epoch 3234/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1442 - total_train_reward: -1342.2865\n",
      "Epoch 3235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6544 - total_train_reward: -1279.3687\n",
      "Epoch 3236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9718 - total_train_reward: -1033.9940\n",
      "Epoch 3237/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6269 - total_train_reward: -1070.2218\n",
      "Epoch 3238/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1914 - total_train_reward: -1280.9054\n",
      "Epoch 3239/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2793 - total_train_reward: -1667.9142\n",
      "Epoch 3240/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5584 - total_train_reward: -1826.6578\n",
      "Epoch 3241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8048 - total_train_reward: -1011.5195\n",
      "Epoch 3242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8139 - total_train_reward: -992.9491\n",
      "Epoch 3243/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5163 - total_train_reward: -861.7832\n",
      "Epoch 3244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7145 - total_train_reward: -1177.9457\n",
      "Epoch 3245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2591 - total_train_reward: -853.7996\n",
      "Epoch 3246/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2516 - total_train_reward: -1906.7227\n",
      "Epoch 3247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6890 - total_train_reward: -1057.1920\n",
      "Epoch 3248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8413 - total_train_reward: -867.1608\n",
      "Epoch 3249/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3668 - total_train_reward: -1073.5358\n",
      "Epoch 3250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0645 - total_train_reward: -1616.7269\n",
      "Epoch 3251/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0293 - total_train_reward: -1032.2387\n",
      "Epoch 3252/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7996 - total_train_reward: -1173.4632\n",
      "Epoch 3253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7248 - total_train_reward: -829.6814\n",
      "Epoch 3254/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9883 - total_train_reward: -900.4817\n",
      "Epoch 3255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5131 - total_train_reward: -1228.8924\n",
      "Epoch 3256/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3751 - total_train_reward: -1327.0182\n",
      "Epoch 3257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5245 - total_train_reward: -1785.9894\n",
      "Epoch 3258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0048 - total_train_reward: -1703.7721\n",
      "Epoch 3259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0614 - total_train_reward: -890.2346\n",
      "Epoch 3260/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3936 - total_train_reward: -1400.3378\n",
      "Epoch 3261/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7342 - total_train_reward: -1311.3347\n",
      "Epoch 3262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9592 - total_train_reward: -1411.7693\n",
      "Epoch 3263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0521 - total_train_reward: -1615.8569\n",
      "Epoch 3264/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1729 - total_train_reward: -1841.0225\n",
      "Epoch 3265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9690 - total_train_reward: -908.4119\n",
      "Epoch 3266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8073 - total_train_reward: -862.8747\n",
      "Epoch 3267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7663 - total_train_reward: -1058.5662\n",
      "Epoch 3268/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0436 - total_train_reward: -1042.9475\n",
      "Epoch 3269/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7247 - total_train_reward: -967.9040\n",
      "Epoch 3270/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8344 - total_train_reward: -1445.3006\n",
      "Epoch 3271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3691 - total_train_reward: -1184.8325\n",
      "Epoch 3272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1314 - total_train_reward: -1300.8455\n",
      "Epoch 3273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5933 - total_train_reward: -1439.1617\n",
      "Epoch 3274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9051 - total_train_reward: -1900.4274\n",
      "Epoch 3275/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4541 - total_train_reward: -1370.9551\n",
      "Epoch 3276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7671 - total_train_reward: -979.2580\n",
      "Epoch 3277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2603 - total_train_reward: -1663.4955\n",
      "Epoch 3278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7350 - total_train_reward: -1346.9563\n",
      "Epoch 3279/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1015 - total_train_reward: -1664.9324\n",
      "Epoch 3280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7880 - total_train_reward: -1215.1334\n",
      "Epoch 3281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6207 - total_train_reward: -1366.8677\n",
      "Epoch 3282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9590 - total_train_reward: -1741.6127\n",
      "Epoch 3283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9784 - total_train_reward: -1580.6559\n",
      "Epoch 3284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0440 - total_train_reward: -951.3874\n",
      "Epoch 3285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6764 - total_train_reward: -1785.6402\n",
      "Epoch 3286/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1993 - total_train_reward: -1040.8562\n",
      "Epoch 3287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1158 - total_train_reward: -1845.7786\n",
      "Epoch 3288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1677 - total_train_reward: -1225.5896\n",
      "Epoch 3289/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1986 - total_train_reward: -1185.5213\n",
      "Epoch 3290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3502 - total_train_reward: -1455.4975\n",
      "Epoch 3291/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0225 - total_train_reward: -1253.0398\n",
      "Epoch 3292/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6552 - total_train_reward: -1172.4274\n",
      "Epoch 3293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4964 - total_train_reward: -1306.2125\n",
      "Epoch 3294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6718 - total_train_reward: -1554.1358\n",
      "Epoch 3295/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5934 - total_train_reward: -1231.1146\n",
      "Epoch 3296/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8865 - total_train_reward: -1893.1918\n",
      "Epoch 3297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9430 - total_train_reward: -1294.3270\n",
      "Epoch 3298/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2183 - total_train_reward: -1914.5268\n",
      "Epoch 3299/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7425 - total_train_reward: -1165.3523\n",
      "Epoch 3300/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2784 - total_train_reward: -1550.4094\n",
      "Epoch 3301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9317 - total_train_reward: -1408.4727\n",
      "Epoch 3302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5786 - total_train_reward: -1650.2958\n",
      "Epoch 3303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3630 - total_train_reward: -952.5095\n",
      "Epoch 3304/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1725 - total_train_reward: -897.1140\n",
      "Epoch 3305/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7355 - total_train_reward: -1282.6388\n",
      "Epoch 3306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9117 - total_train_reward: -1287.0375\n",
      "Epoch 3307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2544 - total_train_reward: -1332.1089\n",
      "Epoch 3308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6046 - total_train_reward: -1159.9015\n",
      "Epoch 3309/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0499 - total_train_reward: -1044.7341\n",
      "Epoch 3310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5756 - total_train_reward: -1052.9813\n",
      "Epoch 3311/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9156 - total_train_reward: -1651.2672\n",
      "Epoch 3312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5258 - total_train_reward: -1041.2368\n",
      "Epoch 3313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6608 - total_train_reward: -1736.6933\n",
      "Epoch 3314/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2319 - total_train_reward: -1358.9169\n",
      "Epoch 3315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5330 - total_train_reward: -1172.4172\n",
      "Epoch 3316/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9095 - total_train_reward: -1584.2196\n",
      "Epoch 3317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7856 - total_train_reward: -965.8591\n",
      "Epoch 3318/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9096 - total_train_reward: -1047.6551\n",
      "Epoch 3319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1862 - total_train_reward: -1050.4882\n",
      "Epoch 3320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3273 - total_train_reward: -1054.4840\n",
      "Epoch 3321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1444 - total_train_reward: -1402.3081\n",
      "Epoch 3322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8040 - total_train_reward: -1180.6249\n",
      "Epoch 3323/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7752 - total_train_reward: -1714.3248\n",
      "Epoch 3324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1573 - total_train_reward: -1587.0361\n",
      "Epoch 3325/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6732 - total_train_reward: -1597.1966\n",
      "Epoch 3326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8039 - total_train_reward: -1292.3134\n",
      "Epoch 3327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8198 - total_train_reward: -1165.5979\n",
      "Epoch 3328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9658 - total_train_reward: -1069.0540\n",
      "Epoch 3329/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8161 - total_train_reward: -1046.7440\n",
      "Epoch 3330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3807 - total_train_reward: -1047.2516\n",
      "Epoch 3331/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7039 - total_train_reward: -961.4860\n",
      "Epoch 3332/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0409 - total_train_reward: -1273.6093\n",
      "Epoch 3333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3660 - total_train_reward: -1846.8400\n",
      "Epoch 3334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1748 - total_train_reward: -1007.5629\n",
      "Epoch 3335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6127 - total_train_reward: -965.7124\n",
      "Epoch 3336/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2224 - total_train_reward: -1775.5487\n",
      "Epoch 3337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4744 - total_train_reward: -1577.3364\n",
      "Epoch 3338/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5341 - total_train_reward: -1746.3638\n",
      "Epoch 3339/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0147 - total_train_reward: -1062.9228\n",
      "Epoch 3340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0023 - total_train_reward: -1577.7505\n",
      "Epoch 3341/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6013 - total_train_reward: -1782.6310\n",
      "Epoch 3342/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7696 - total_train_reward: -1166.8307\n",
      "Epoch 3343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2922 - total_train_reward: -1054.2533\n",
      "Epoch 3344/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8387 - total_train_reward: -1419.2361\n",
      "Epoch 3345/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0006 - total_train_reward: -896.2020\n",
      "Epoch 3346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7981 - total_train_reward: -1283.7731\n",
      "Epoch 3347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1517 - total_train_reward: -1335.1821\n",
      "Epoch 3348/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8939 - total_train_reward: -1792.2947\n",
      "Epoch 3349/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3195 - total_train_reward: -968.9355\n",
      "Epoch 3350/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0768 - total_train_reward: -1055.7790\n",
      "Epoch 3351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.9449 - total_train_reward: -1059.1337\n",
      "Epoch 3352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9801 - total_train_reward: -1075.5400\n",
      "Epoch 3353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2728 - total_train_reward: -1070.4453\n",
      "Epoch 3354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7688 - total_train_reward: -995.3696\n",
      "Epoch 3355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1840 - total_train_reward: -856.5565\n",
      "Epoch 3356/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4230 - total_train_reward: -1512.0206\n",
      "Epoch 3357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1817 - total_train_reward: -1665.1102\n",
      "Epoch 3358/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6101 - total_train_reward: -1025.2957\n",
      "Epoch 3359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7033 - total_train_reward: -1847.9941\n",
      "Epoch 3360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0612 - total_train_reward: -1207.1646\n",
      "Epoch 3361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0428 - total_train_reward: -1067.9350\n",
      "Epoch 3362/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7615 - total_train_reward: -1653.2261\n",
      "Epoch 3363/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0245 - total_train_reward: -1069.7344\n",
      "Epoch 3364/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3377 - total_train_reward: -1075.6991\n",
      "Epoch 3365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8282 - total_train_reward: -1068.0623\n",
      "Epoch 3366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0098 - total_train_reward: -1229.0975\n",
      "Epoch 3367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1502 - total_train_reward: -965.6463\n",
      "Epoch 3368/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4426 - total_train_reward: -1194.9148\n",
      "Epoch 3369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5505 - total_train_reward: -1765.2619\n",
      "Epoch 3370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8987 - total_train_reward: -1851.6670\n",
      "Epoch 3371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5929 - total_train_reward: -954.2399\n",
      "Epoch 3372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2478 - total_train_reward: -1066.8370\n",
      "Epoch 3373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2735 - total_train_reward: -1561.0236\n",
      "Epoch 3374/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7801 - total_train_reward: -1281.2620\n",
      "Epoch 3375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2343 - total_train_reward: -1055.1638\n",
      "Epoch 3376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8949 - total_train_reward: -1067.0201\n",
      "Epoch 3377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.7215 - total_train_reward: -1700.6149\n",
      "Epoch 3378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9900 - total_train_reward: -1416.5605\n",
      "Epoch 3379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4983 - total_train_reward: -1773.0791\n",
      "Epoch 3380/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4698 - total_train_reward: -1050.6570\n",
      "Epoch 3381/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3988 - total_train_reward: -1085.9188\n",
      "Epoch 3382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6446 - total_train_reward: -1473.1092\n",
      "Epoch 3383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1495 - total_train_reward: -1050.3601\n",
      "Epoch 3384/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4211 - total_train_reward: -1719.5877\n",
      "Epoch 3385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0507 - total_train_reward: -1070.3702\n",
      "Epoch 3386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8371 - total_train_reward: -1261.5357\n",
      "Epoch 3387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9704 - total_train_reward: -1573.6267\n",
      "Epoch 3388/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8642 - total_train_reward: -1448.8450\n",
      "Epoch 3389/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1243 - total_train_reward: -888.6766\n",
      "Epoch 3390/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7213 - total_train_reward: -1092.7333\n",
      "Epoch 3391/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5900 - total_train_reward: -1287.2710\n",
      "Epoch 3392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4757 - total_train_reward: -1292.0648\n",
      "Epoch 3393/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1180 - total_train_reward: -1289.9468\n",
      "Epoch 3394/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0377 - total_train_reward: -1046.5323\n",
      "Epoch 3395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1342 - total_train_reward: -1497.8051\n",
      "Epoch 3396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9026 - total_train_reward: -1499.3758\n",
      "Epoch 3397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7316 - total_train_reward: -1052.2458\n",
      "Epoch 3398/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8126 - total_train_reward: -1762.0469\n",
      "Epoch 3399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3717 - total_train_reward: -1410.4802\n",
      "Epoch 3400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2802 - total_train_reward: -1469.6499\n",
      "Epoch 3401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1928 - total_train_reward: -1294.5793\n",
      "Epoch 3402/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7911 - total_train_reward: -1836.2551\n",
      "Epoch 3403/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1152 - total_train_reward: -966.8365\n",
      "Epoch 3404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9185 - total_train_reward: -1066.3489\n",
      "Epoch 3405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8845 - total_train_reward: -1563.2427\n",
      "Epoch 3406/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2136 - total_train_reward: -718.7715\n",
      "Epoch 3407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 24.4589 - total_train_reward: -1059.2973\n",
      "Epoch 3408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8871 - total_train_reward: -1181.2549\n",
      "Epoch 3409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7506 - total_train_reward: -1353.0464\n",
      "Epoch 3410/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5141 - total_train_reward: -1293.3095\n",
      "Epoch 3411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0920 - total_train_reward: -1054.3899\n",
      "Epoch 3412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1066 - total_train_reward: -886.2101\n",
      "Epoch 3413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2594 - total_train_reward: -1816.3056\n",
      "Epoch 3414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6507 - total_train_reward: -911.9587\n",
      "Epoch 3415/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2052 - total_train_reward: -1655.8619\n",
      "Epoch 3416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8822 - total_train_reward: -1311.0361\n",
      "Epoch 3417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9610 - total_train_reward: -1284.8841\n",
      "Epoch 3418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9810 - total_train_reward: -983.4310\n",
      "Epoch 3419/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3067 - total_train_reward: -1017.3519\n",
      "Epoch 3420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2010 - total_train_reward: -1170.7153\n",
      "Epoch 3421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1087 - total_train_reward: -1288.4613\n",
      "Epoch 3422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5932 - total_train_reward: -972.0388\n",
      "Epoch 3423/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0325 - total_train_reward: -1184.2677\n",
      "Epoch 3424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2419 - total_train_reward: -1842.1656\n",
      "Epoch 3425/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2020 - total_train_reward: -1438.4301\n",
      "Epoch 3426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3006 - total_train_reward: -1066.1637\n",
      "Epoch 3427/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4210 - total_train_reward: -1704.5272\n",
      "Epoch 3428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0868 - total_train_reward: -924.8417\n",
      "Epoch 3429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5885 - total_train_reward: -1506.8743\n",
      "Epoch 3430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9991 - total_train_reward: -735.5879\n",
      "Epoch 3431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3599 - total_train_reward: -1175.7122\n",
      "Epoch 3432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6613 - total_train_reward: -1239.5007\n",
      "Epoch 3433/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8499 - total_train_reward: -1635.4440\n",
      "Epoch 3434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9097 - total_train_reward: -1046.1170\n",
      "Epoch 3435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1709 - total_train_reward: -1064.9379\n",
      "Epoch 3436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.3369 - total_train_reward: -1056.3125\n",
      "Epoch 3437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6607 - total_train_reward: -1067.1097\n",
      "Epoch 3438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2565 - total_train_reward: -1520.2933\n",
      "Epoch 3439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9692 - total_train_reward: -1288.3852\n",
      "Epoch 3440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0382 - total_train_reward: -1142.3038\n",
      "Epoch 3441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7815 - total_train_reward: -1425.4531\n",
      "Epoch 3442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6523 - total_train_reward: -1173.1654\n",
      "Epoch 3443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8367 - total_train_reward: -1167.1997\n",
      "Epoch 3444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3474 - total_train_reward: -1466.8196\n",
      "Epoch 3445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7123 - total_train_reward: -1064.9289\n",
      "Epoch 3446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6663 - total_train_reward: -1165.2748\n",
      "Epoch 3447/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6767 - total_train_reward: -1083.8296\n",
      "Epoch 3448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7268 - total_train_reward: -1833.0947\n",
      "Epoch 3449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5524 - total_train_reward: -1142.3115\n",
      "Epoch 3450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0365 - total_train_reward: -1236.7293\n",
      "Epoch 3451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1092 - total_train_reward: -963.3941\n",
      "Epoch 3452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6479 - total_train_reward: -1156.0103\n",
      "Epoch 3453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6110 - total_train_reward: -1044.3847\n",
      "Epoch 3454/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5900 - total_train_reward: -1360.1688\n",
      "Epoch 3455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7050 - total_train_reward: -1833.0944\n",
      "Epoch 3456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0369 - total_train_reward: -735.6309\n",
      "Epoch 3457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3322 - total_train_reward: -1610.3758\n",
      "Epoch 3458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3425 - total_train_reward: -1330.6403\n",
      "Epoch 3459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7127 - total_train_reward: -1063.2043\n",
      "Epoch 3460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9946 - total_train_reward: -1087.1657\n",
      "Epoch 3461/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8280 - total_train_reward: -1109.8399\n",
      "Epoch 3462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0997 - total_train_reward: -995.8165\n",
      "Epoch 3463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2337 - total_train_reward: -1269.9602\n",
      "Epoch 3464/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8175 - total_train_reward: -1109.8402\n",
      "Epoch 3465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5212 - total_train_reward: -1083.8119\n",
      "Epoch 3466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9120 - total_train_reward: -1469.2441\n",
      "Epoch 3467/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9114 - total_train_reward: -1605.4368\n",
      "Epoch 3468/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.4899 - total_train_reward: -1143.7039\n",
      "Epoch 3469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3349 - total_train_reward: -1099.4209\n",
      "Epoch 3470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3903 - total_train_reward: -1294.7436\n",
      "Epoch 3471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3400 - total_train_reward: -1167.0704\n",
      "Epoch 3472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7192 - total_train_reward: -998.1519\n",
      "Epoch 3473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2138 - total_train_reward: -1777.9348\n",
      "Epoch 3474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4421 - total_train_reward: -1365.6725\n",
      "Epoch 3475/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3815 - total_train_reward: -1067.9373\n",
      "Epoch 3476/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1540 - total_train_reward: -1075.4228\n",
      "Epoch 3477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2591 - total_train_reward: -1341.5129\n",
      "Epoch 3478/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8219 - total_train_reward: -1441.7550\n",
      "Epoch 3479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4225 - total_train_reward: -856.7710\n",
      "Epoch 3480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0618 - total_train_reward: -963.3657\n",
      "Epoch 3481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5355 - total_train_reward: -1095.0734\n",
      "Epoch 3482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5313 - total_train_reward: -1494.9912\n",
      "Epoch 3483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2242 - total_train_reward: -1074.6121\n",
      "Epoch 3484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3765 - total_train_reward: -1538.9518\n",
      "Epoch 3485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5566 - total_train_reward: -1779.2928\n",
      "Epoch 3486/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6991 - total_train_reward: -962.7265\n",
      "Epoch 3487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3912 - total_train_reward: -1436.8978\n",
      "Epoch 3488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7834 - total_train_reward: -853.1710\n",
      "Epoch 3489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1959 - total_train_reward: -953.2016\n",
      "Epoch 3490/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3167 - total_train_reward: -1301.2435\n",
      "Epoch 3491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8103 - total_train_reward: -1647.0081\n",
      "Epoch 3492/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5895 - total_train_reward: -1151.7253\n",
      "Epoch 3493/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7661 - total_train_reward: -1074.7732\n",
      "Epoch 3494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7478 - total_train_reward: -1881.7272\n",
      "Epoch 3495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5271 - total_train_reward: -1330.6413\n",
      "Epoch 3496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6819 - total_train_reward: -1144.3852\n",
      "Epoch 3497/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4789 - total_train_reward: -1372.4327\n",
      "Epoch 3498/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2797 - total_train_reward: -1345.2103\n",
      "Epoch 3499/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6194 - total_train_reward: -1395.2093\n",
      "Epoch 3500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0947 - total_train_reward: -1559.0681\n",
      "Epoch 3501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4695 - total_train_reward: -1107.1226\n",
      "Epoch 3502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0012 - total_train_reward: -1266.3554\n",
      "Epoch 3503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2705 - total_train_reward: -1337.4559\n",
      "Epoch 3504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6426 - total_train_reward: -1460.9277\n",
      "Epoch 3505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5614 - total_train_reward: -1324.6152\n",
      "Epoch 3506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4724 - total_train_reward: -1066.9472\n",
      "Epoch 3507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3197 - total_train_reward: -1616.7417\n",
      "Epoch 3508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7255 - total_train_reward: -1154.3956\n",
      "Epoch 3509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2081 - total_train_reward: -1887.4918\n",
      "Epoch 3510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4603 - total_train_reward: -867.0139\n",
      "Epoch 3511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6309 - total_train_reward: -963.7053\n",
      "Epoch 3512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1909 - total_train_reward: -1083.9651\n",
      "Epoch 3513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3447 - total_train_reward: -958.8072\n",
      "Epoch 3514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9401 - total_train_reward: -1290.8719\n",
      "Epoch 3515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5153 - total_train_reward: -1051.2962\n",
      "Epoch 3516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4990 - total_train_reward: -966.2453\n",
      "Epoch 3517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3061 - total_train_reward: -1071.3549\n",
      "Epoch 3518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5199 - total_train_reward: -1247.7018\n",
      "Epoch 3519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1195 - total_train_reward: -1464.0358\n",
      "Epoch 3520/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0204 - total_train_reward: -1167.2948\n",
      "Epoch 3521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2048 - total_train_reward: -1875.9870\n",
      "Epoch 3522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0169 - total_train_reward: -1056.9884\n",
      "Epoch 3523/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2585 - total_train_reward: -1081.1777\n",
      "Epoch 3524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3336 - total_train_reward: -1055.5952\n",
      "Epoch 3525/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0668 - total_train_reward: -1859.6265\n",
      "Epoch 3526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8348 - total_train_reward: -1171.2103\n",
      "Epoch 3527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2598 - total_train_reward: -1302.8400\n",
      "Epoch 3528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6607 - total_train_reward: -1265.0030\n",
      "Epoch 3529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1974 - total_train_reward: -1068.4657\n",
      "Epoch 3530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7610 - total_train_reward: -1392.9258\n",
      "Epoch 3531/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3673 - total_train_reward: -1057.0367\n",
      "Epoch 3532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9866 - total_train_reward: -964.0826\n",
      "Epoch 3533/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1868 - total_train_reward: -1306.8273\n",
      "Epoch 3534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9058 - total_train_reward: -1300.2869\n",
      "Epoch 3535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9201 - total_train_reward: -906.9482\n",
      "Epoch 3536/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3539 - total_train_reward: -1170.0725\n",
      "Epoch 3537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5253 - total_train_reward: -1588.4922\n",
      "Epoch 3538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1656 - total_train_reward: -1892.6898\n",
      "Epoch 3539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5227 - total_train_reward: -1097.5644\n",
      "Epoch 3540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6090 - total_train_reward: -974.1605\n",
      "Epoch 3541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1230 - total_train_reward: -965.0298\n",
      "Epoch 3542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.8190 - total_train_reward: -1820.4082\n",
      "Epoch 3543/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9802 - total_train_reward: -1163.2293\n",
      "Epoch 3544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2851 - total_train_reward: -1086.2455\n",
      "Epoch 3545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6524 - total_train_reward: -1114.5348\n",
      "Epoch 3546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1665 - total_train_reward: -851.0905\n",
      "Epoch 3547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6389 - total_train_reward: -971.0727\n",
      "Epoch 3548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3267 - total_train_reward: -1042.1057\n",
      "Epoch 3549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3638 - total_train_reward: -1061.0791\n",
      "Epoch 3550/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8124 - total_train_reward: -1061.1378\n",
      "Epoch 3551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5121 - total_train_reward: -1446.8031\n",
      "Epoch 3552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9903 - total_train_reward: -1277.6380\n",
      "Epoch 3553/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8527 - total_train_reward: -1491.6813\n",
      "Epoch 3554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6445 - total_train_reward: -1171.4778\n",
      "Epoch 3555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0844 - total_train_reward: -970.0370\n",
      "Epoch 3556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4300 - total_train_reward: -1054.9089\n",
      "Epoch 3557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3949 - total_train_reward: -1559.1275\n",
      "Epoch 3558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4365 - total_train_reward: -1818.5822\n",
      "Epoch 3559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6228 - total_train_reward: -1069.3776\n",
      "Epoch 3560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9925 - total_train_reward: -968.9722\n",
      "Epoch 3561/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7180 - total_train_reward: -1357.8683\n",
      "Epoch 3562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7822 - total_train_reward: -858.3609\n",
      "Epoch 3563/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4888 - total_train_reward: -966.4430\n",
      "Epoch 3564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0766 - total_train_reward: -1409.7510\n",
      "Epoch 3565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0966 - total_train_reward: -1399.5514\n",
      "Epoch 3566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1372 - total_train_reward: -1859.2496\n",
      "Epoch 3567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9942 - total_train_reward: -1337.8673\n",
      "Epoch 3568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.4742 - total_train_reward: -1824.0474\n",
      "Epoch 3569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8559 - total_train_reward: -1706.3230\n",
      "Epoch 3570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3543 - total_train_reward: -1240.5914\n",
      "Epoch 3571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4266 - total_train_reward: -1342.3613\n",
      "Epoch 3572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1353 - total_train_reward: -1030.6703\n",
      "Epoch 3573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4061 - total_train_reward: -1502.5826\n",
      "Epoch 3574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0307 - total_train_reward: -1095.8928\n",
      "Epoch 3575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4938 - total_train_reward: -1635.4374\n",
      "Epoch 3576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5914 - total_train_reward: -1796.6732\n",
      "Epoch 3577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1509 - total_train_reward: -1078.1146\n",
      "Epoch 3578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4145 - total_train_reward: -1313.9248\n",
      "Epoch 3579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5952 - total_train_reward: -1073.9821\n",
      "Epoch 3580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1885 - total_train_reward: -1735.0822\n",
      "Epoch 3581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1733 - total_train_reward: -1216.5413\n",
      "Epoch 3582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5809 - total_train_reward: -854.2858\n",
      "Epoch 3583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6949 - total_train_reward: -1419.3329\n",
      "Epoch 3584/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4022 - total_train_reward: -1568.5723\n",
      "Epoch 3585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.8022 - total_train_reward: -1147.6258\n",
      "Epoch 3586/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4343 - total_train_reward: -1065.5735\n",
      "Epoch 3587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6665 - total_train_reward: -1021.1600\n",
      "Epoch 3588/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2570 - total_train_reward: -1309.3610\n",
      "Epoch 3589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8730 - total_train_reward: -969.9414\n",
      "Epoch 3590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8251 - total_train_reward: -1600.0419\n",
      "Epoch 3591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5875 - total_train_reward: -1121.7740\n",
      "Epoch 3592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1242 - total_train_reward: -1070.7788\n",
      "Epoch 3593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3474 - total_train_reward: -1140.7326\n",
      "Epoch 3594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2401 - total_train_reward: -1067.8662\n",
      "Epoch 3595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2680 - total_train_reward: -1077.3799\n",
      "Epoch 3596/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6115 - total_train_reward: -1170.3063\n",
      "Epoch 3597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3759 - total_train_reward: -1173.7630\n",
      "Epoch 3598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9366 - total_train_reward: -1392.4948\n",
      "Epoch 3599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6112 - total_train_reward: -1607.7321\n",
      "Epoch 3600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4258 - total_train_reward: -1148.8181\n",
      "Epoch 3601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4229 - total_train_reward: -1296.5476\n",
      "Epoch 3602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2943 - total_train_reward: -1217.5380\n",
      "Epoch 3603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9719 - total_train_reward: -1141.2089\n",
      "Epoch 3604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3727 - total_train_reward: -1769.0267\n",
      "Epoch 3605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3278 - total_train_reward: -1039.3368\n",
      "Epoch 3606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5350 - total_train_reward: -1142.1701\n",
      "Epoch 3607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.7251 - total_train_reward: -909.2154\n",
      "Epoch 3608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8815 - total_train_reward: -1402.9715\n",
      "Epoch 3609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8573 - total_train_reward: -1859.8219\n",
      "Epoch 3610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1529 - total_train_reward: -732.8854\n",
      "Epoch 3611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.2085 - total_train_reward: -1563.3080\n",
      "Epoch 3612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5813 - total_train_reward: -1153.7679\n",
      "Epoch 3613/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0111 - total_train_reward: -1122.2849\n",
      "Epoch 3614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9802 - total_train_reward: -966.1045\n",
      "Epoch 3615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.3304 - total_train_reward: -1177.6746\n",
      "Epoch 3616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2600 - total_train_reward: -1092.6599\n",
      "Epoch 3617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0512 - total_train_reward: -1424.1948\n",
      "Epoch 3618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2528 - total_train_reward: -1287.4789\n",
      "Epoch 3619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6794 - total_train_reward: -1656.7295\n",
      "Epoch 3620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7219 - total_train_reward: -963.4042\n",
      "Epoch 3621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9450 - total_train_reward: -1446.0508\n",
      "Epoch 3622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3778 - total_train_reward: -1844.2374\n",
      "Epoch 3623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7000 - total_train_reward: -1317.1015\n",
      "Epoch 3624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2760 - total_train_reward: -839.5972\n",
      "Epoch 3625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3804 - total_train_reward: -1732.2053\n",
      "Epoch 3626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4867 - total_train_reward: -1708.4177\n",
      "Epoch 3627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4059 - total_train_reward: -1170.5099\n",
      "Epoch 3628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6452 - total_train_reward: -1500.1946\n",
      "Epoch 3629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1478 - total_train_reward: -1171.0157\n",
      "Epoch 3630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6554 - total_train_reward: -1549.0655\n",
      "Epoch 3631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1772 - total_train_reward: -1700.5043\n",
      "Epoch 3632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9118 - total_train_reward: -1089.1331\n",
      "Epoch 3633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3241 - total_train_reward: -1363.4569\n",
      "Epoch 3634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4249 - total_train_reward: -849.7759\n",
      "Epoch 3635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6888 - total_train_reward: -1332.1804\n",
      "Epoch 3636/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3225 - total_train_reward: -1173.1815\n",
      "Epoch 3637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.1433 - total_train_reward: -1778.0892\n",
      "Epoch 3638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4251 - total_train_reward: -1317.7058\n",
      "Epoch 3639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3184 - total_train_reward: -1850.9320\n",
      "Epoch 3640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.8659 - total_train_reward: -1535.4840\n",
      "Epoch 3641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5738 - total_train_reward: -961.7433\n",
      "Epoch 3642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2106 - total_train_reward: -1304.7983\n",
      "Epoch 3643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0759 - total_train_reward: -1603.0033\n",
      "Epoch 3644/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7383 - total_train_reward: -1050.1671\n",
      "Epoch 3645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6133 - total_train_reward: -1474.5930\n",
      "Epoch 3646/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6133 - total_train_reward: -851.7716\n",
      "Epoch 3647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6438 - total_train_reward: -1519.8668\n",
      "Epoch 3648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1214 - total_train_reward: -1835.4648\n",
      "Epoch 3649/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1678 - total_train_reward: -1389.7152\n",
      "Epoch 3650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6509 - total_train_reward: -1168.3460\n",
      "Epoch 3651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8529 - total_train_reward: -1286.1033\n",
      "Epoch 3652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9486 - total_train_reward: -1362.5938\n",
      "Epoch 3653/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2770 - total_train_reward: -1130.8578\n",
      "Epoch 3654/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9342 - total_train_reward: -1686.0553\n",
      "Epoch 3655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.3089 - total_train_reward: -1176.0517\n",
      "Epoch 3656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6118 - total_train_reward: -1139.2338\n",
      "Epoch 3657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4236 - total_train_reward: -1313.5167\n",
      "Epoch 3658/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0746 - total_train_reward: -896.6171\n",
      "Epoch 3659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6190 - total_train_reward: -1167.9516\n",
      "Epoch 3660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.1897 - total_train_reward: -1396.8937\n",
      "Epoch 3661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3378 - total_train_reward: -1277.7389\n",
      "Epoch 3662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5410 - total_train_reward: -1437.9688\n",
      "Epoch 3663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3823 - total_train_reward: -1175.7885\n",
      "Epoch 3664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5769 - total_train_reward: -1076.4437\n",
      "Epoch 3665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4487 - total_train_reward: -1174.3297\n",
      "Epoch 3666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6697 - total_train_reward: -1172.9896\n",
      "Epoch 3667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6606 - total_train_reward: -1527.2347\n",
      "Epoch 3668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8064 - total_train_reward: -1221.1982\n",
      "Epoch 3669/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7002 - total_train_reward: -1067.5562\n",
      "Epoch 3670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0673 - total_train_reward: -915.0858\n",
      "Epoch 3671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6588 - total_train_reward: -1219.1252\n",
      "Epoch 3672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8483 - total_train_reward: -908.6573\n",
      "Epoch 3673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2122 - total_train_reward: -1792.1467\n",
      "Epoch 3674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2806 - total_train_reward: -1658.6020\n",
      "Epoch 3675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9076 - total_train_reward: -1387.6914\n",
      "Epoch 3676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2722 - total_train_reward: -734.2137\n",
      "Epoch 3677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0933 - total_train_reward: -864.1140\n",
      "Epoch 3678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6637 - total_train_reward: -1164.4743\n",
      "Epoch 3679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4262 - total_train_reward: -1794.2976\n",
      "Epoch 3680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9878 - total_train_reward: -1025.4051\n",
      "Epoch 3681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.5889 - total_train_reward: -1080.6144\n",
      "Epoch 3682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0965 - total_train_reward: -1402.0903\n",
      "Epoch 3683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4119 - total_train_reward: -1861.9304\n",
      "Epoch 3684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1092 - total_train_reward: -1880.1645\n",
      "Epoch 3685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5641 - total_train_reward: -1136.8604\n",
      "Epoch 3686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4399 - total_train_reward: -1083.4010\n",
      "Epoch 3687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6081 - total_train_reward: -1169.1836\n",
      "Epoch 3688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3613 - total_train_reward: -964.3754\n",
      "Epoch 3689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0131 - total_train_reward: -1541.2044\n",
      "Epoch 3690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2379 - total_train_reward: -1754.8786\n",
      "Epoch 3691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8375 - total_train_reward: -1108.8596\n",
      "Epoch 3692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1274 - total_train_reward: -1291.8908\n",
      "Epoch 3693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6560 - total_train_reward: -999.7774\n",
      "Epoch 3694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3224 - total_train_reward: -1278.8431\n",
      "Epoch 3695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6430 - total_train_reward: -1043.8950\n",
      "Epoch 3696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1541 - total_train_reward: -1112.9711\n",
      "Epoch 3697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6864 - total_train_reward: -1537.8231\n",
      "Epoch 3698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0407 - total_train_reward: -1161.1166\n",
      "Epoch 3699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8714 - total_train_reward: -953.3444\n",
      "Epoch 3700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2736 - total_train_reward: -1644.6416\n",
      "Epoch 3701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3650 - total_train_reward: -1102.4651\n",
      "Epoch 3702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0390 - total_train_reward: -1085.5902\n",
      "Epoch 3703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1575 - total_train_reward: -1072.7504\n",
      "Epoch 3704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0144 - total_train_reward: -1046.0624\n",
      "Epoch 3705/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9893 - total_train_reward: -1643.3743\n",
      "Epoch 3706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0150 - total_train_reward: -1098.8675\n",
      "Epoch 3707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0710 - total_train_reward: -1731.1026\n",
      "Epoch 3708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.0578 - total_train_reward: -1051.4761\n",
      "Epoch 3709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3386 - total_train_reward: -1449.3028\n",
      "Epoch 3710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4245 - total_train_reward: -1079.4807\n",
      "Epoch 3711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7996 - total_train_reward: -1097.1058\n",
      "Epoch 3712/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0473 - total_train_reward: -1115.5573\n",
      "Epoch 3713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7951 - total_train_reward: -1033.0744\n",
      "Epoch 3714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3750 - total_train_reward: -1514.0254\n",
      "Epoch 3715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5933 - total_train_reward: -1114.7840\n",
      "Epoch 3716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9631 - total_train_reward: -1845.8331\n",
      "Epoch 3717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4819 - total_train_reward: -1064.2292\n",
      "Epoch 3718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0597 - total_train_reward: -854.7712\n",
      "Epoch 3719/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1934 - total_train_reward: -1384.0152\n",
      "Epoch 3720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8982 - total_train_reward: -1682.8483\n",
      "Epoch 3721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4432 - total_train_reward: -1526.0338\n",
      "Epoch 3722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1896 - total_train_reward: -1089.0971\n",
      "Epoch 3723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2739 - total_train_reward: -1071.4681\n",
      "Epoch 3724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8687 - total_train_reward: -1180.9575\n",
      "Epoch 3725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6276 - total_train_reward: -1015.9733\n",
      "Epoch 3726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7536 - total_train_reward: -1091.3726\n",
      "Epoch 3727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5600 - total_train_reward: -1451.7126\n",
      "Epoch 3728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0944 - total_train_reward: -1168.3901\n",
      "Epoch 3729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6785 - total_train_reward: -965.1093\n",
      "Epoch 3730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5853 - total_train_reward: -1293.1252\n",
      "Epoch 3731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2327 - total_train_reward: -967.0864\n",
      "Epoch 3732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9260 - total_train_reward: -1386.8776\n",
      "Epoch 3733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3191 - total_train_reward: -1142.3324\n",
      "Epoch 3734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9230 - total_train_reward: -1730.8216\n",
      "Epoch 3735/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4881 - total_train_reward: -1061.3039\n",
      "Epoch 3736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3803 - total_train_reward: -1140.0357\n",
      "Epoch 3737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7184 - total_train_reward: -1473.8230\n",
      "Epoch 3738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0048 - total_train_reward: -940.5368\n",
      "Epoch 3739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1438 - total_train_reward: -860.1794\n",
      "Epoch 3740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0970 - total_train_reward: -1361.9515\n",
      "Epoch 3741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8080 - total_train_reward: -1309.8980\n",
      "Epoch 3742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8692 - total_train_reward: -1087.1034\n",
      "Epoch 3743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9719 - total_train_reward: -1603.1231\n",
      "Epoch 3744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2375 - total_train_reward: -1352.6503\n",
      "Epoch 3745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0524 - total_train_reward: -1172.3102\n",
      "Epoch 3746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5294 - total_train_reward: -750.7096\n",
      "Epoch 3747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2998 - total_train_reward: -1347.7209\n",
      "Epoch 3748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8712 - total_train_reward: -1750.8186\n",
      "Epoch 3749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4865 - total_train_reward: -1073.8335\n",
      "Epoch 3750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6949 - total_train_reward: -1057.5410\n",
      "Epoch 3751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9686 - total_train_reward: -1063.5363\n",
      "Epoch 3752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3331 - total_train_reward: -1789.4868\n",
      "Epoch 3753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1840 - total_train_reward: -1064.6140\n",
      "Epoch 3754/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0558 - total_train_reward: -975.0825\n",
      "Epoch 3755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7258 - total_train_reward: -1290.3522\n",
      "Epoch 3756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1116 - total_train_reward: -1028.8490\n",
      "Epoch 3757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2968 - total_train_reward: -1156.5543\n",
      "Epoch 3758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4793 - total_train_reward: -1480.3800\n",
      "Epoch 3759/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2496 - total_train_reward: -900.7492\n",
      "Epoch 3760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0148 - total_train_reward: -1274.8918\n",
      "Epoch 3761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1806 - total_train_reward: -1660.7430\n",
      "Epoch 3762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3640 - total_train_reward: -1114.0925\n",
      "Epoch 3763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2163 - total_train_reward: -1264.1718\n",
      "Epoch 3764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7194 - total_train_reward: -1212.5256\n",
      "Epoch 3765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8736 - total_train_reward: -1588.4069\n",
      "Epoch 3766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0344 - total_train_reward: -1180.0891\n",
      "Epoch 3767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4879 - total_train_reward: -1674.2389\n",
      "Epoch 3768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3737 - total_train_reward: -994.0370\n",
      "Epoch 3769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1904 - total_train_reward: -1285.7444\n",
      "Epoch 3770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7470 - total_train_reward: -859.5238\n",
      "Epoch 3771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2196 - total_train_reward: -1004.4723\n",
      "Epoch 3772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4288 - total_train_reward: -1066.7793\n",
      "Epoch 3773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5098 - total_train_reward: -1035.2973\n",
      "Epoch 3774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6023 - total_train_reward: -848.2953\n",
      "Epoch 3775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4279 - total_train_reward: -964.4386\n",
      "Epoch 3776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7468 - total_train_reward: -863.1505\n",
      "Epoch 3777/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0964 - total_train_reward: -1159.1487\n",
      "Epoch 3778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7465 - total_train_reward: -1494.4337\n",
      "Epoch 3779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5409 - total_train_reward: -1441.0561\n",
      "Epoch 3780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7231 - total_train_reward: -1116.9408\n",
      "Epoch 3781/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1892 - total_train_reward: -1066.9547\n",
      "Epoch 3782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4180 - total_train_reward: -734.8224\n",
      "Epoch 3783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7910 - total_train_reward: -1261.5425\n",
      "Epoch 3784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.9366 - total_train_reward: -1075.1013\n",
      "Epoch 3785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2039 - total_train_reward: -1293.1382\n",
      "Epoch 3786/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7466 - total_train_reward: -888.6694\n",
      "Epoch 3787/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1311 - total_train_reward: -1464.2124\n",
      "Epoch 3788/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1617 - total_train_reward: -1059.2963\n",
      "Epoch 3789/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0356 - total_train_reward: -1822.2802\n",
      "Epoch 3790/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5003 - total_train_reward: -1081.2529\n",
      "Epoch 3791/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0369 - total_train_reward: -1135.2616\n",
      "Epoch 3792/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0553 - total_train_reward: -1579.0263\n",
      "Epoch 3793/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6440 - total_train_reward: -1401.2534\n",
      "Epoch 3794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9062 - total_train_reward: -1271.8394\n",
      "Epoch 3795/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4788 - total_train_reward: -1259.1902\n",
      "Epoch 3796/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8938 - total_train_reward: -1603.1842\n",
      "Epoch 3797/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6346 - total_train_reward: -1768.2074\n",
      "Epoch 3798/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4900 - total_train_reward: -1056.8873\n",
      "Epoch 3799/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1209 - total_train_reward: -1807.4291\n",
      "Epoch 3800/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5254 - total_train_reward: -1111.6878\n",
      "Epoch 3801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7570 - total_train_reward: -844.7508\n",
      "Epoch 3802/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1393 - total_train_reward: -1154.0522\n",
      "Epoch 3803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1967 - total_train_reward: -1593.2522\n",
      "Epoch 3804/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9315 - total_train_reward: -1197.1691\n",
      "Epoch 3805/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2771 - total_train_reward: -1165.4682\n",
      "Epoch 3806/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4022 - total_train_reward: -1116.6588\n",
      "Epoch 3807/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1993 - total_train_reward: -949.8049\n",
      "Epoch 3808/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4583 - total_train_reward: -1290.8948\n",
      "Epoch 3809/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5922 - total_train_reward: -1530.4003\n",
      "Epoch 3810/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0598 - total_train_reward: -1469.2374\n",
      "Epoch 3811/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.8985 - total_train_reward: -1067.7049\n",
      "Epoch 3812/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3288 - total_train_reward: -1267.8874\n",
      "Epoch 3813/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4118 - total_train_reward: -802.2974\n",
      "Epoch 3814/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4093 - total_train_reward: -1128.0438\n",
      "Epoch 3815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1014 - total_train_reward: -1274.7729\n",
      "Epoch 3816/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8069 - total_train_reward: -1136.2668\n",
      "Epoch 3817/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1086 - total_train_reward: -853.4771\n",
      "Epoch 3818/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3906 - total_train_reward: -1301.3743\n",
      "Epoch 3819/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6025 - total_train_reward: -1363.8817\n",
      "Epoch 3820/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3320 - total_train_reward: -1101.6499\n",
      "Epoch 3821/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4212 - total_train_reward: -1582.0281\n",
      "Epoch 3822/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0505 - total_train_reward: -1067.8096\n",
      "Epoch 3823/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8927 - total_train_reward: -1515.5667\n",
      "Epoch 3824/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3724 - total_train_reward: -1303.8377\n",
      "Epoch 3825/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1744 - total_train_reward: -1170.8494\n",
      "Epoch 3826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2261 - total_train_reward: -1604.1201\n",
      "Epoch 3827/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1532 - total_train_reward: -1208.8265\n",
      "Epoch 3828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6102 - total_train_reward: -1178.2732\n",
      "Epoch 3829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2450 - total_train_reward: -1850.4339\n",
      "Epoch 3830/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8247 - total_train_reward: -1294.5559\n",
      "Epoch 3831/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8568 - total_train_reward: -1315.0888\n",
      "Epoch 3832/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5494 - total_train_reward: -1749.5709\n",
      "Epoch 3833/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6835 - total_train_reward: -1322.6990\n",
      "Epoch 3834/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1922 - total_train_reward: -1185.9758\n",
      "Epoch 3835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3432 - total_train_reward: -1325.7822\n",
      "Epoch 3836/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3554 - total_train_reward: -1068.0233\n",
      "Epoch 3837/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7413 - total_train_reward: -1628.2514\n",
      "Epoch 3838/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8096 - total_train_reward: -1098.7078\n",
      "Epoch 3839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2106 - total_train_reward: -1783.4290\n",
      "Epoch 3840/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9518 - total_train_reward: -971.6063\n",
      "Epoch 3841/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1311 - total_train_reward: -1142.9552\n",
      "Epoch 3842/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8369 - total_train_reward: -1546.6424\n",
      "Epoch 3843/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9353 - total_train_reward: -1186.3710\n",
      "Epoch 3844/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5889 - total_train_reward: -1570.6055\n",
      "Epoch 3845/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4592 - total_train_reward: -1070.5038\n",
      "Epoch 3846/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9045 - total_train_reward: -828.5765\n",
      "Epoch 3847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9752 - total_train_reward: -869.1835\n",
      "Epoch 3848/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8054 - total_train_reward: -1733.5911\n",
      "Epoch 3849/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0079 - total_train_reward: -1068.1870\n",
      "Epoch 3850/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2883 - total_train_reward: -1073.2933\n",
      "Epoch 3851/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4371 - total_train_reward: -1345.0772\n",
      "Epoch 3852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.6310 - total_train_reward: -1585.4463\n",
      "Epoch 3853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5099e-04 - total_train_reward: -851.5737\n",
      "Epoch 3854/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5325 - total_train_reward: -1863.0839\n",
      "Epoch 3855/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8097 - total_train_reward: -835.7448\n",
      "Epoch 3856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.2388 - total_train_reward: -1633.7709\n",
      "Epoch 3857/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9226 - total_train_reward: -1495.9627\n",
      "Epoch 3858/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9037 - total_train_reward: -1213.7169\n",
      "Epoch 3859/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1677 - total_train_reward: -1595.4658\n",
      "Epoch 3860/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8787 - total_train_reward: -857.5099\n",
      "Epoch 3861/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1049 - total_train_reward: -965.1130\n",
      "Epoch 3862/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3389 - total_train_reward: -1291.2611\n",
      "Epoch 3863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0624 - total_train_reward: -1744.7406\n",
      "Epoch 3864/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0879 - total_train_reward: -903.9125\n",
      "Epoch 3865/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.7098 - total_train_reward: -1101.5741\n",
      "Epoch 3866/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0160 - total_train_reward: -1381.8476\n",
      "Epoch 3867/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7389 - total_train_reward: -1751.4374\n",
      "Epoch 3868/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9877 - total_train_reward: -1337.0112\n",
      "Epoch 3869/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8730 - total_train_reward: -1061.0261\n",
      "Epoch 3870/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7934 - total_train_reward: -1129.8365\n",
      "Epoch 3871/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1631 - total_train_reward: -1046.9514\n",
      "Epoch 3872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0065 - total_train_reward: -1630.7515\n",
      "Epoch 3873/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0458 - total_train_reward: -1072.6806\n",
      "Epoch 3874/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5500 - total_train_reward: -951.4653\n",
      "Epoch 3875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8672 - total_train_reward: -957.7115\n",
      "Epoch 3876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6766 - total_train_reward: -1762.1102\n",
      "Epoch 3877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8380 - total_train_reward: -1124.1017\n",
      "Epoch 3878/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7636 - total_train_reward: -1142.0104\n",
      "Epoch 3879/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6668 - total_train_reward: -1056.0488\n",
      "Epoch 3880/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9366 - total_train_reward: -1618.4432\n",
      "Epoch 3881/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4623 - total_train_reward: -1603.9257\n",
      "Epoch 3882/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1259 - total_train_reward: -1737.5570\n",
      "Epoch 3883/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1116 - total_train_reward: -802.4420\n",
      "Epoch 3884/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0522 - total_train_reward: -1155.9880\n",
      "Epoch 3885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3077 - total_train_reward: -1329.1760\n",
      "Epoch 3886/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4955 - total_train_reward: -1216.4224\n",
      "Epoch 3887/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.1729 - total_train_reward: -1144.8435\n",
      "Epoch 3888/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1110 - total_train_reward: -962.1869\n",
      "Epoch 3889/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4685 - total_train_reward: -1447.4057\n",
      "Epoch 3890/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5851 - total_train_reward: -1068.5978\n",
      "Epoch 3891/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7344 - total_train_reward: -1031.6801\n",
      "Epoch 3892/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0795 - total_train_reward: -1025.9106\n",
      "Epoch 3893/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6292 - total_train_reward: -1496.2822\n",
      "Epoch 3894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1352 - total_train_reward: -1565.0693\n",
      "Epoch 3895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9763 - total_train_reward: -1307.4249\n",
      "Epoch 3896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5930 - total_train_reward: -1152.6053\n",
      "Epoch 3897/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 18.0493 - total_train_reward: -924.4974\n",
      "Epoch 3898/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.0146 - total_train_reward: -1460.5253\n",
      "Epoch 3899/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4302 - total_train_reward: -1068.1382\n",
      "Epoch 3900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2367 - total_train_reward: -1416.4061\n",
      "Epoch 3901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0664 - total_train_reward: -1188.9649\n",
      "Epoch 3902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6508 - total_train_reward: -1182.7917\n",
      "Epoch 3903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3316 - total_train_reward: -1474.7320\n",
      "Epoch 3904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8044 - total_train_reward: -1334.5029\n",
      "Epoch 3905/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1447 - total_train_reward: -1175.4509\n",
      "Epoch 3906/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7947 - total_train_reward: -1285.2372\n",
      "Epoch 3907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7142 - total_train_reward: -730.0217\n",
      "Epoch 3908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.8891 - total_train_reward: -1090.4419\n",
      "Epoch 3909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9710 - total_train_reward: -1488.3849\n",
      "Epoch 3910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1249 - total_train_reward: -1067.1634\n",
      "Epoch 3911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6987 - total_train_reward: -1175.8439\n",
      "Epoch 3912/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4720 - total_train_reward: -1416.2005\n",
      "Epoch 3913/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7043 - total_train_reward: -1123.1722\n",
      "Epoch 3914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9443 - total_train_reward: -1069.8316\n",
      "Epoch 3915/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1004 - total_train_reward: -1432.8764\n",
      "Epoch 3916/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9284 - total_train_reward: -1801.9184\n",
      "Epoch 3917/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4219 - total_train_reward: -1054.1320\n",
      "Epoch 3918/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3837 - total_train_reward: -1298.5614\n",
      "Epoch 3919/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1448 - total_train_reward: -715.8092\n",
      "Epoch 3920/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0508 - total_train_reward: -1434.8595\n",
      "Epoch 3921/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8301 - total_train_reward: -1175.7839\n",
      "Epoch 3922/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6795 - total_train_reward: -1112.2624\n",
      "Epoch 3923/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.4627 - total_train_reward: -1113.0914\n",
      "Epoch 3924/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0381 - total_train_reward: -1145.8855\n",
      "Epoch 3925/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9276 - total_train_reward: -733.1513\n",
      "Epoch 3926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -19.3844 - total_train_reward: -1728.2693\n",
      "Epoch 3927/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1591 - total_train_reward: -1151.8688\n",
      "Epoch 3928/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0322 - total_train_reward: -1450.0001\n",
      "Epoch 3929/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1207 - total_train_reward: -1148.2089\n",
      "Epoch 3930/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8289 - total_train_reward: -1153.2450\n",
      "Epoch 3931/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9525 - total_train_reward: -1773.6755\n",
      "Epoch 3932/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5747 - total_train_reward: -1682.3645\n",
      "Epoch 3933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4834 - total_train_reward: -1240.9426\n",
      "Epoch 3934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5768 - total_train_reward: -965.6032\n",
      "Epoch 3935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7401 - total_train_reward: -1433.7169\n",
      "Epoch 3936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9801 - total_train_reward: -1125.1171\n",
      "Epoch 3937/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0379 - total_train_reward: -860.9543\n",
      "Epoch 3938/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9466 - total_train_reward: -1711.8764\n",
      "Epoch 3939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7644 - total_train_reward: -1553.1130\n",
      "Epoch 3940/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4379 - total_train_reward: -1788.2834\n",
      "Epoch 3941/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9282 - total_train_reward: -1166.4025\n",
      "Epoch 3942/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4898 - total_train_reward: -1287.6060\n",
      "Epoch 3943/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0290 - total_train_reward: -1241.2370\n",
      "Epoch 3944/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1997 - total_train_reward: -1281.3298\n",
      "Epoch 3945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9434 - total_train_reward: -1791.5207\n",
      "Epoch 3946/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8869 - total_train_reward: -1580.8652\n",
      "Epoch 3947/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0964 - total_train_reward: -1069.7210\n",
      "Epoch 3948/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8989 - total_train_reward: -919.8835\n",
      "Epoch 3949/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1911 - total_train_reward: -1540.0376\n",
      "Epoch 3950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3630 - total_train_reward: -1321.6885\n",
      "Epoch 3951/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 37.3383 - total_train_reward: -611.8873\n",
      "Epoch 3952/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 18.7520 - total_train_reward: -1476.2588\n",
      "Epoch 3953/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.1817 - total_train_reward: -1057.3211\n",
      "Epoch 3954/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.0396 - total_train_reward: -1657.4397\n",
      "Epoch 3955/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1931 - total_train_reward: -1068.9948\n",
      "Epoch 3956/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0787 - total_train_reward: -963.5365\n",
      "Epoch 3957/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1869 - total_train_reward: -963.5170\n",
      "Epoch 3958/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0878 - total_train_reward: -1398.3672\n",
      "Epoch 3959/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7182 - total_train_reward: -852.3680\n",
      "Epoch 3960/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1750 - total_train_reward: -1165.5186\n",
      "Epoch 3961/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7239 - total_train_reward: -1457.8040\n",
      "Epoch 3962/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1924 - total_train_reward: -1848.7801\n",
      "Epoch 3963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6258 - total_train_reward: -895.2771\n",
      "Epoch 3964/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1505 - total_train_reward: -1288.8863\n",
      "Epoch 3965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4144 - total_train_reward: -1343.3908\n",
      "Epoch 3966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3741 - total_train_reward: -1600.9435\n",
      "Epoch 3967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3090 - total_train_reward: -1086.2676\n",
      "Epoch 3968/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2268 - total_train_reward: -1810.4816\n",
      "Epoch 3969/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4678 - total_train_reward: -1773.3345\n",
      "Epoch 3970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4524 - total_train_reward: -1165.5023\n",
      "Epoch 3971/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5504 - total_train_reward: -1229.9049\n",
      "Epoch 3972/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7879 - total_train_reward: -1703.3589\n",
      "Epoch 3973/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7508 - total_train_reward: -1420.0170\n",
      "Epoch 3974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6841 - total_train_reward: -1791.0112\n",
      "Epoch 3975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3585 - total_train_reward: -1246.3349\n",
      "Epoch 3976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2819 - total_train_reward: -1436.3370\n",
      "Epoch 3977/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3994 - total_train_reward: -1552.5592\n",
      "Epoch 3978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5892 - total_train_reward: -1602.2706\n",
      "Epoch 3979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7665 - total_train_reward: -1609.3228\n",
      "Epoch 3980/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8626 - total_train_reward: -1172.4617\n",
      "Epoch 3981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0364 - total_train_reward: -1309.8484\n",
      "Epoch 3982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0632 - total_train_reward: -1782.4248\n",
      "Epoch 3983/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0870 - total_train_reward: -1114.7586\n",
      "Epoch 3984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2279 - total_train_reward: -1026.2211\n",
      "Epoch 3985/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1561 - total_train_reward: -1398.0265\n",
      "Epoch 3986/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5305 - total_train_reward: -1143.9472\n",
      "Epoch 3987/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6237 - total_train_reward: -1159.2052\n",
      "Epoch 3988/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1124 - total_train_reward: -1145.7572\n",
      "Epoch 3989/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2223 - total_train_reward: -1297.6788\n",
      "Epoch 3990/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9393 - total_train_reward: -1071.8767\n",
      "Epoch 3991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1321 - total_train_reward: -1678.6526\n",
      "Epoch 3992/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2751 - total_train_reward: -1151.7231\n",
      "Epoch 3993/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7260 - total_train_reward: -1049.8242\n",
      "Epoch 3994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.4311 - total_train_reward: -1177.3507\n",
      "Epoch 3995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2079 - total_train_reward: -1499.1506\n",
      "Epoch 3996/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9483 - total_train_reward: -732.9837\n",
      "Epoch 3997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6296 - total_train_reward: -1489.3083\n",
      "Epoch 3998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1098 - total_train_reward: -1371.0764\n",
      "Epoch 3999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1676 - total_train_reward: -1071.6176\n",
      "Epoch 4000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1662 - total_train_reward: -1299.9745\n",
      "Epoch 4001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7146 - total_train_reward: -1174.1404\n",
      "Epoch 4002/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.3977 - total_train_reward: -1162.3160\n",
      "Epoch 4003/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5980 - total_train_reward: -882.2398\n",
      "Epoch 4004/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0669 - total_train_reward: -1814.6959\n",
      "Epoch 4005/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6500 - total_train_reward: -1069.1622\n",
      "Epoch 4006/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6785 - total_train_reward: -973.6816\n",
      "Epoch 4007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6841 - total_train_reward: -1167.4652\n",
      "Epoch 4008/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0200 - total_train_reward: -1868.5102\n",
      "Epoch 4009/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2287 - total_train_reward: -1569.3068\n",
      "Epoch 4010/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0196 - total_train_reward: -1046.9437\n",
      "Epoch 4011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6167 - total_train_reward: -1289.5775\n",
      "Epoch 4012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2751 - total_train_reward: -1151.7668\n",
      "Epoch 4013/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9442 - total_train_reward: -1173.3620\n",
      "Epoch 4014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8322 - total_train_reward: -966.2667\n",
      "Epoch 4015/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0346 - total_train_reward: -847.6236\n",
      "Epoch 4016/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2323 - total_train_reward: -1586.9074\n",
      "Epoch 4017/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2607 - total_train_reward: -1830.5244\n",
      "Epoch 4018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2688 - total_train_reward: -1867.2317\n",
      "Epoch 4019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2052 - total_train_reward: -1064.3284\n",
      "Epoch 4020/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.0790 - total_train_reward: -1345.8791\n",
      "Epoch 4021/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4451 - total_train_reward: -1041.8406\n",
      "Epoch 4022/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2300 - total_train_reward: -1730.3144\n",
      "Epoch 4023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0030 - total_train_reward: -1853.1982\n",
      "Epoch 4024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0506 - total_train_reward: -1107.5339\n",
      "Epoch 4025/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0273 - total_train_reward: -1136.9167\n",
      "Epoch 4026/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -19.2997 - total_train_reward: -1701.5948\n",
      "Epoch 4027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5577 - total_train_reward: -1543.8803\n",
      "Epoch 4028/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5867 - total_train_reward: -1067.4993\n",
      "Epoch 4029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5557 - total_train_reward: -1422.1578\n",
      "Epoch 4030/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4357 - total_train_reward: -1249.7222\n",
      "Epoch 4031/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2824 - total_train_reward: -964.0240\n",
      "Epoch 4032/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3228 - total_train_reward: -1171.8690\n",
      "Epoch 4033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1133 - total_train_reward: -1637.7550\n",
      "Epoch 4034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2227 - total_train_reward: -1418.9580\n",
      "Epoch 4035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7411 - total_train_reward: -1145.3131\n",
      "Epoch 4036/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2849 - total_train_reward: -1165.5187\n",
      "Epoch 4037/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5274 - total_train_reward: -1086.9295\n",
      "Epoch 4038/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2343 - total_train_reward: -1084.1209\n",
      "Epoch 4039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6919 - total_train_reward: -1336.5009\n",
      "Epoch 4040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6589 - total_train_reward: -1742.8911\n",
      "Epoch 4041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5542 - total_train_reward: -963.4831\n",
      "Epoch 4042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4637 - total_train_reward: -1666.9119\n",
      "Epoch 4043/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9419 - total_train_reward: -1406.7444\n",
      "Epoch 4044/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6874 - total_train_reward: -1065.7285\n",
      "Epoch 4045/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2251 - total_train_reward: -1185.5434\n",
      "Epoch 4046/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9338 - total_train_reward: -843.6575\n",
      "Epoch 4047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3252 - total_train_reward: -1181.4954\n",
      "Epoch 4048/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1556 - total_train_reward: -1348.7078\n",
      "Epoch 4049/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4944 - total_train_reward: -940.7165\n",
      "Epoch 4050/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7858 - total_train_reward: -1689.7581\n",
      "Epoch 4051/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8397 - total_train_reward: -1472.2553\n",
      "Epoch 4052/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5209 - total_train_reward: -1170.9746\n",
      "Epoch 4053/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8600 - total_train_reward: -1710.9374\n",
      "Epoch 4054/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1462 - total_train_reward: -1429.5406\n",
      "Epoch 4055/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9365 - total_train_reward: -1436.6993\n",
      "Epoch 4056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2897 - total_train_reward: -1167.1581\n",
      "Epoch 4057/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0581 - total_train_reward: -1075.9139\n",
      "Epoch 4058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7711 - total_train_reward: -1065.6503\n",
      "Epoch 4059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4978 - total_train_reward: -1169.2905\n",
      "Epoch 4060/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7249 - total_train_reward: -1263.8794\n",
      "Epoch 4061/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2753 - total_train_reward: -1067.9832\n",
      "Epoch 4062/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6934 - total_train_reward: -1452.7280\n",
      "Epoch 4063/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1781 - total_train_reward: -1297.1347\n",
      "Epoch 4064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2336 - total_train_reward: -1884.0468\n",
      "Epoch 4065/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1373 - total_train_reward: -980.3358\n",
      "Epoch 4066/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5799 - total_train_reward: -1753.1542\n",
      "Epoch 4067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4755 - total_train_reward: -909.5495\n",
      "Epoch 4068/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1526 - total_train_reward: -1340.8819\n",
      "Epoch 4069/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7054 - total_train_reward: -1301.5845\n",
      "Epoch 4070/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0419 - total_train_reward: -1163.7836\n",
      "Epoch 4071/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1773 - total_train_reward: -1272.6629\n",
      "Epoch 4072/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2497 - total_train_reward: -1716.5293\n",
      "Epoch 4073/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7530 - total_train_reward: -962.8648\n",
      "Epoch 4074/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.8816 - total_train_reward: -1138.5129\n",
      "Epoch 4075/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5340 - total_train_reward: -1290.0717\n",
      "Epoch 4076/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3033 - total_train_reward: -1501.2294\n",
      "Epoch 4077/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9174 - total_train_reward: -897.3248\n",
      "Epoch 4078/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1694 - total_train_reward: -998.7310\n",
      "Epoch 4079/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7503 - total_train_reward: -1179.9842\n",
      "Epoch 4080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0318 - total_train_reward: -1164.2027\n",
      "Epoch 4081/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8255 - total_train_reward: -1065.3729\n",
      "Epoch 4082/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3811 - total_train_reward: -1031.2930\n",
      "Epoch 4083/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5412 - total_train_reward: -1823.2333\n",
      "Epoch 4084/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.2840 - total_train_reward: -1734.8415\n",
      "Epoch 4085/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9896 - total_train_reward: -1195.0901\n",
      "Epoch 4086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8975 - total_train_reward: -1067.0893\n",
      "Epoch 4087/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2072 - total_train_reward: -735.0278\n",
      "Epoch 4088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7750 - total_train_reward: -1588.4017\n",
      "Epoch 4089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8368 - total_train_reward: -1741.0831\n",
      "Epoch 4090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3089 - total_train_reward: -1240.3990\n",
      "Epoch 4091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9581 - total_train_reward: -1762.0397\n",
      "Epoch 4092/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7857 - total_train_reward: -869.5792\n",
      "Epoch 4093/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6854 - total_train_reward: -1200.6200\n",
      "Epoch 4094/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6928 - total_train_reward: -732.7683\n",
      "Epoch 4095/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3311 - total_train_reward: -1179.2747\n",
      "Epoch 4096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2450 - total_train_reward: -1163.8320\n",
      "Epoch 4097/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.5850 - total_train_reward: -1064.6130\n",
      "Epoch 4098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0534 - total_train_reward: -1059.8155\n",
      "Epoch 4099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7917 - total_train_reward: -1794.1246\n",
      "Epoch 4100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4711 - total_train_reward: -1173.2493\n",
      "Epoch 4101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7686 - total_train_reward: -1182.2852\n",
      "Epoch 4102/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1511 - total_train_reward: -1064.1157\n",
      "Epoch 4103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2049 - total_train_reward: -1151.1442\n",
      "Epoch 4104/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7823 - total_train_reward: -1408.1304\n",
      "Epoch 4105/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1967 - total_train_reward: -1318.8230\n",
      "Epoch 4106/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0629 - total_train_reward: -1175.8603\n",
      "Epoch 4107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5457 - total_train_reward: -1181.9576\n",
      "Epoch 4108/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5261 - total_train_reward: -1569.8083\n",
      "Epoch 4109/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2221 - total_train_reward: -1626.4147\n",
      "Epoch 4110/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7904 - total_train_reward: -1501.2965\n",
      "Epoch 4111/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4809 - total_train_reward: -1073.3055\n",
      "Epoch 4112/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7551 - total_train_reward: -1167.4496\n",
      "Epoch 4113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7605 - total_train_reward: -1288.6179\n",
      "Epoch 4114/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5106 - total_train_reward: -1095.0489\n",
      "Epoch 4115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2880 - total_train_reward: -1090.9477\n",
      "Epoch 4116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5392 - total_train_reward: -770.0040\n",
      "Epoch 4117/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6282 - total_train_reward: -958.3267\n",
      "Epoch 4118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5912 - total_train_reward: -1431.8623\n",
      "Epoch 4119/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0215 - total_train_reward: -1372.1786\n",
      "Epoch 4120/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2715 - total_train_reward: -1136.9017\n",
      "Epoch 4121/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7516 - total_train_reward: -1153.1808\n",
      "Epoch 4122/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7184 - total_train_reward: -1791.8150\n",
      "Epoch 4123/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6178 - total_train_reward: -960.5499\n",
      "Epoch 4124/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0980 - total_train_reward: -1264.2193\n",
      "Epoch 4125/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3051 - total_train_reward: -1864.7043\n",
      "Epoch 4126/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0767 - total_train_reward: -1510.1322\n",
      "Epoch 4127/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2718 - total_train_reward: -1249.3143\n",
      "Epoch 4128/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2163 - total_train_reward: -1325.2799\n",
      "Epoch 4129/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4463 - total_train_reward: -1727.8486\n",
      "Epoch 4130/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8356 - total_train_reward: -941.8784\n",
      "Epoch 4131/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.5789 - total_train_reward: -1162.4402\n",
      "Epoch 4132/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4323 - total_train_reward: -1086.2091\n",
      "Epoch 4133/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9931 - total_train_reward: -1052.9745\n",
      "Epoch 4134/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8143 - total_train_reward: -1066.0936\n",
      "Epoch 4135/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7764 - total_train_reward: -1740.2929\n",
      "Epoch 4136/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5497 - total_train_reward: -859.1262\n",
      "Epoch 4137/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4385 - total_train_reward: -1148.0780\n",
      "Epoch 4138/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1205 - total_train_reward: -1065.5304\n",
      "Epoch 4139/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3546 - total_train_reward: -1194.6989\n",
      "Epoch 4140/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9014 - total_train_reward: -1050.8558\n",
      "Epoch 4141/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4353 - total_train_reward: -1167.9568\n",
      "Epoch 4142/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2486 - total_train_reward: -1167.4164\n",
      "Epoch 4143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3681 - total_train_reward: -1636.7202\n",
      "Epoch 4144/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8767 - total_train_reward: -1117.7161\n",
      "Epoch 4145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3184 - total_train_reward: -1793.5236\n",
      "Epoch 4146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4964 - total_train_reward: -1296.4599\n",
      "Epoch 4147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3508 - total_train_reward: -1177.4119\n",
      "Epoch 4148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1345 - total_train_reward: -1100.2335\n",
      "Epoch 4149/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2028 - total_train_reward: -1168.7766\n",
      "Epoch 4150/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3233 - total_train_reward: -1739.8726\n",
      "Epoch 4151/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8949 - total_train_reward: -1344.8460\n",
      "Epoch 4152/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5625 - total_train_reward: -1780.9851\n",
      "Epoch 4153/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9292 - total_train_reward: -1458.2450\n",
      "Epoch 4154/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9523 - total_train_reward: -1158.2507\n",
      "Epoch 4155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5283 - total_train_reward: -1058.4088\n",
      "Epoch 4156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9113 - total_train_reward: -1117.0983\n",
      "Epoch 4157/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3719 - total_train_reward: -1282.3373\n",
      "Epoch 4158/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2405 - total_train_reward: -1290.4246\n",
      "Epoch 4159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8197 - total_train_reward: -1116.6470\n",
      "Epoch 4160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6705 - total_train_reward: -1380.4166\n",
      "Epoch 4161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3597 - total_train_reward: -1177.4895\n",
      "Epoch 4162/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9810 - total_train_reward: -1313.0180\n",
      "Epoch 4163/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0031 - total_train_reward: -1394.1392\n",
      "Epoch 4164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5312 - total_train_reward: -886.0322\n",
      "Epoch 4165/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8451 - total_train_reward: -1108.8128\n",
      "Epoch 4166/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2195 - total_train_reward: -1565.9133\n",
      "Epoch 4167/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2001 - total_train_reward: -742.3072\n",
      "Epoch 4168/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2423 - total_train_reward: -1628.0383\n",
      "Epoch 4169/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8236 - total_train_reward: -1772.3124\n",
      "Epoch 4170/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5716 - total_train_reward: -962.7273\n",
      "Epoch 4171/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0436 - total_train_reward: -964.0647\n",
      "Epoch 4172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5974 - total_train_reward: -1152.3564\n",
      "Epoch 4173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9960 - total_train_reward: -1155.5163\n",
      "Epoch 4174/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9684 - total_train_reward: -1495.0685\n",
      "Epoch 4175/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3040 - total_train_reward: -1349.6530\n",
      "Epoch 4176/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4481 - total_train_reward: -1065.6119\n",
      "Epoch 4177/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1892 - total_train_reward: -966.2222\n",
      "Epoch 4178/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1637 - total_train_reward: -1630.4884\n",
      "Epoch 4179/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5851 - total_train_reward: -733.0696\n",
      "Epoch 4180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2479 - total_train_reward: -963.2099\n",
      "Epoch 4181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4002 - total_train_reward: -1878.4142\n",
      "Epoch 4182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9503 - total_train_reward: -1482.2312\n",
      "Epoch 4183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5179 - total_train_reward: -1064.8769\n",
      "Epoch 4184/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2981 - total_train_reward: -1423.9492\n",
      "Epoch 4185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5994 - total_train_reward: -1520.3610\n",
      "Epoch 4186/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6320 - total_train_reward: -1371.8723\n",
      "Epoch 4187/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2385 - total_train_reward: -1437.6665\n",
      "Epoch 4188/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0243 - total_train_reward: -1147.8176\n",
      "Epoch 4189/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4558 - total_train_reward: -1424.7422\n",
      "Epoch 4190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1832 - total_train_reward: -1059.1596\n",
      "Epoch 4191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4325 - total_train_reward: -1052.2004\n",
      "Epoch 4192/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2845 - total_train_reward: -1170.5283\n",
      "Epoch 4193/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0697 - total_train_reward: -1258.1470\n",
      "Epoch 4194/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5429 - total_train_reward: -1170.7944\n",
      "Epoch 4195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0228 - total_train_reward: -1241.0367\n",
      "Epoch 4196/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6587 - total_train_reward: -1497.5721\n",
      "Epoch 4197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3506 - total_train_reward: -1291.2343\n",
      "Epoch 4198/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5894 - total_train_reward: -845.7651\n",
      "Epoch 4199/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9774 - total_train_reward: -963.2670\n",
      "Epoch 4200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3076 - total_train_reward: -964.1303\n",
      "Epoch 4201/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4136 - total_train_reward: -969.7234\n",
      "Epoch 4202/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0224 - total_train_reward: -1107.5517\n",
      "Epoch 4203/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2517 - total_train_reward: -1272.9214\n",
      "Epoch 4204/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9193 - total_train_reward: -964.0489\n",
      "Epoch 4205/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7192 - total_train_reward: -962.5907\n",
      "Epoch 4206/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7078 - total_train_reward: -1163.6210\n",
      "Epoch 4207/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2020 - total_train_reward: -1361.6627\n",
      "Epoch 4208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1444 - total_train_reward: -1165.2041\n",
      "Epoch 4209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7034 - total_train_reward: -1164.5090\n",
      "Epoch 4210/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6635 - total_train_reward: -1713.4733\n",
      "Epoch 4211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7082 - total_train_reward: -1081.7253\n",
      "Epoch 4212/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4665 - total_train_reward: -1056.2341\n",
      "Epoch 4213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0731 - total_train_reward: -1466.5079\n",
      "Epoch 4214/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0466 - total_train_reward: -1502.3625\n",
      "Epoch 4215/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3541 - total_train_reward: -1712.3961\n",
      "Epoch 4216/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3192 - total_train_reward: -1365.1289\n",
      "Epoch 4217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0157 - total_train_reward: -1208.3082\n",
      "Epoch 4218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3951 - total_train_reward: -1001.1091\n",
      "Epoch 4219/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9273 - total_train_reward: -1117.9974\n",
      "Epoch 4220/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.5808 - total_train_reward: -1681.0984\n",
      "Epoch 4221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3316 - total_train_reward: -1077.6382\n",
      "Epoch 4222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2181 - total_train_reward: -1297.1953\n",
      "Epoch 4223/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9591 - total_train_reward: -1538.1274\n",
      "Epoch 4224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0990 - total_train_reward: -1597.2705\n",
      "Epoch 4225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1798 - total_train_reward: -1270.1382\n",
      "Epoch 4226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1274 - total_train_reward: -1414.5044\n",
      "Epoch 4227/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6125 - total_train_reward: -1214.2029\n",
      "Epoch 4228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3519 - total_train_reward: -1867.1747\n",
      "Epoch 4229/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2318 - total_train_reward: -1844.1169\n",
      "Epoch 4230/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1078 - total_train_reward: -1869.9219\n",
      "Epoch 4231/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8104 - total_train_reward: -1181.9277\n",
      "Epoch 4232/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9469 - total_train_reward: -1184.3513\n",
      "Epoch 4233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9690 - total_train_reward: -963.6753\n",
      "Epoch 4234/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1478 - total_train_reward: -1379.6166\n",
      "Epoch 4235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9380 - total_train_reward: -937.6455\n",
      "Epoch 4236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0999 - total_train_reward: -1035.6633\n",
      "Epoch 4237/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4175 - total_train_reward: -1262.8741\n",
      "Epoch 4238/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9427 - total_train_reward: -1284.7621\n",
      "Epoch 4239/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0826 - total_train_reward: -1175.1252\n",
      "Epoch 4240/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1529 - total_train_reward: -1161.1091\n",
      "Epoch 4241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0845 - total_train_reward: -1051.1351\n",
      "Epoch 4242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.9628 - total_train_reward: -1125.7739\n",
      "Epoch 4243/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0249 - total_train_reward: -1146.3070\n",
      "Epoch 4244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4153 - total_train_reward: -1586.9617\n",
      "Epoch 4245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9613 - total_train_reward: -1677.1973\n",
      "Epoch 4246/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9248 - total_train_reward: -986.3430\n",
      "Epoch 4247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7858 - total_train_reward: -1171.5803\n",
      "Epoch 4248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9035 - total_train_reward: -1172.9161\n",
      "Epoch 4249/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1604 - total_train_reward: -1507.6727\n",
      "Epoch 4250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7799 - total_train_reward: -1581.7707\n",
      "Epoch 4251/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1187 - total_train_reward: -1449.8355\n",
      "Epoch 4252/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4817 - total_train_reward: -1397.7135\n",
      "Epoch 4253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2829 - total_train_reward: -1872.1157\n",
      "Epoch 4254/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2525 - total_train_reward: -1410.5634\n",
      "Epoch 4255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7093 - total_train_reward: -1366.4240\n",
      "Epoch 4256/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6841 - total_train_reward: -1144.2392\n",
      "Epoch 4257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0153 - total_train_reward: -1557.5464\n",
      "Epoch 4258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1636 - total_train_reward: -1668.6560\n",
      "Epoch 4259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1908 - total_train_reward: -1445.6177\n",
      "Epoch 4260/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3020 - total_train_reward: -1492.9708\n",
      "Epoch 4261/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2050 - total_train_reward: -1276.4444\n",
      "Epoch 4262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2612 - total_train_reward: -872.7947\n",
      "Epoch 4263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8870 - total_train_reward: -1782.4087\n",
      "Epoch 4264/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6978 - total_train_reward: -1307.4878\n",
      "Epoch 4265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9968 - total_train_reward: -1554.8373\n",
      "Epoch 4266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8125 - total_train_reward: -1067.1721\n",
      "Epoch 4267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7455 - total_train_reward: -1426.7966\n",
      "Epoch 4268/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0022 - total_train_reward: -1509.5664\n",
      "Epoch 4269/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9505 - total_train_reward: -1546.0958\n",
      "Epoch 4270/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2022 - total_train_reward: -1705.6428\n",
      "Epoch 4271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8165 - total_train_reward: -1196.3993\n",
      "Epoch 4272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1988 - total_train_reward: -729.0223\n",
      "Epoch 4273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2030 - total_train_reward: -1065.4955\n",
      "Epoch 4274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5726 - total_train_reward: -1277.4285\n",
      "Epoch 4275/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8908 - total_train_reward: -1860.1407\n",
      "Epoch 4276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9653 - total_train_reward: -1688.9324\n",
      "Epoch 4277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0927 - total_train_reward: -1815.0197\n",
      "Epoch 4278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5571 - total_train_reward: -1585.2596\n",
      "Epoch 4279/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0786 - total_train_reward: -1111.7332\n",
      "Epoch 4280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5446 - total_train_reward: -1066.9795\n",
      "Epoch 4281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6903 - total_train_reward: -1799.7284\n",
      "Epoch 4282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8268 - total_train_reward: -961.9940\n",
      "Epoch 4283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5968 - total_train_reward: -1066.6820\n",
      "Epoch 4284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6700 - total_train_reward: -816.8340\n",
      "Epoch 4285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6403 - total_train_reward: -1172.8335\n",
      "Epoch 4286/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5686 - total_train_reward: -1323.5449\n",
      "Epoch 4287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8128 - total_train_reward: -961.9747\n",
      "Epoch 4288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4525 - total_train_reward: -1762.3891\n",
      "Epoch 4289/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8330 - total_train_reward: -1521.3780\n",
      "Epoch 4290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7892 - total_train_reward: -1194.1924\n",
      "Epoch 4291/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0103 - total_train_reward: -1187.1411\n",
      "Epoch 4292/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3509 - total_train_reward: -1386.5959\n",
      "Epoch 4293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9531 - total_train_reward: -1605.8566\n",
      "Epoch 4294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4078 - total_train_reward: -1751.9948\n",
      "Epoch 4295/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7011 - total_train_reward: -1396.5286\n",
      "Epoch 4296/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7445 - total_train_reward: -1798.8165\n",
      "Epoch 4297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6143 - total_train_reward: -1291.3635\n",
      "Epoch 4298/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2519 - total_train_reward: -1232.3448\n",
      "Epoch 4299/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0862 - total_train_reward: -1764.3649\n",
      "Epoch 4300/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0155 - total_train_reward: -1489.1559\n",
      "Epoch 4301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9230 - total_train_reward: -1741.1261\n",
      "Epoch 4302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7449 - total_train_reward: -962.1141\n",
      "Epoch 4303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3776 - total_train_reward: -1182.3562\n",
      "Epoch 4304/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8217 - total_train_reward: -848.9291\n",
      "Epoch 4305/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4945 - total_train_reward: -1446.3565\n",
      "Epoch 4306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8791 - total_train_reward: -1861.3407\n",
      "Epoch 4307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9605 - total_train_reward: -730.9945\n",
      "Epoch 4308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8760 - total_train_reward: -1622.7968\n",
      "Epoch 4309/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7002 - total_train_reward: -1049.5518\n",
      "Epoch 4310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7873 - total_train_reward: -1105.3666\n",
      "Epoch 4311/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3518 - total_train_reward: -1286.7942\n",
      "Epoch 4312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3580 - total_train_reward: -1090.2457\n",
      "Epoch 4313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7807 - total_train_reward: -1434.6795\n",
      "Epoch 4314/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8478 - total_train_reward: -1174.4807\n",
      "Epoch 4315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6962 - total_train_reward: -1040.7121\n",
      "Epoch 4316/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9513 - total_train_reward: -1222.2235\n",
      "Epoch 4317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.6075 - total_train_reward: -1766.3196\n",
      "Epoch 4318/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9590 - total_train_reward: -1172.2700\n",
      "Epoch 4319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5459 - total_train_reward: -1162.0256\n",
      "Epoch 4320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0757 - total_train_reward: -1509.3168\n",
      "Epoch 4321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3819 - total_train_reward: -1195.3124\n",
      "Epoch 4322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3111 - total_train_reward: -1379.1539\n",
      "Epoch 4323/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9370 - total_train_reward: -1194.3450\n",
      "Epoch 4324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5678 - total_train_reward: -1186.5323\n",
      "Epoch 4325/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0727 - total_train_reward: -1719.3039\n",
      "Epoch 4326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3768 - total_train_reward: -1860.2384\n",
      "Epoch 4327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.3518 - total_train_reward: -1792.5433\n",
      "Epoch 4328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7211 - total_train_reward: -1172.0363\n",
      "Epoch 4329/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6570 - total_train_reward: -1308.7099\n",
      "Epoch 4330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3381 - total_train_reward: -1218.0263\n",
      "Epoch 4331/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9389 - total_train_reward: -1399.1517\n",
      "Epoch 4332/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3346 - total_train_reward: -701.3934\n",
      "Epoch 4333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.6325 - total_train_reward: -1090.9030\n",
      "Epoch 4334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1560 - total_train_reward: -1065.4176\n",
      "Epoch 4335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4469 - total_train_reward: -1529.6432\n",
      "Epoch 4336/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3856 - total_train_reward: -1190.6814\n",
      "Epoch 4337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6409 - total_train_reward: -849.1921\n",
      "Epoch 4338/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.3168 - total_train_reward: -1194.6595\n",
      "Epoch 4339/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0822 - total_train_reward: -1195.1558\n",
      "Epoch 4340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7074 - total_train_reward: -963.6307\n",
      "Epoch 4341/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1488 - total_train_reward: -1196.6622\n",
      "Epoch 4342/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8993 - total_train_reward: -1371.7813\n",
      "Epoch 4343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6693 - total_train_reward: -1193.8777\n",
      "Epoch 4344/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9875 - total_train_reward: -1176.1105\n",
      "Epoch 4345/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9654 - total_train_reward: -1178.3015\n",
      "Epoch 4346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9207 - total_train_reward: -961.7951\n",
      "Epoch 4347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1782 - total_train_reward: -847.1965\n",
      "Epoch 4348/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6156 - total_train_reward: -1520.0364\n",
      "Epoch 4349/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9094 - total_train_reward: -1602.5657\n",
      "Epoch 4350/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2873 - total_train_reward: -1740.1864\n",
      "Epoch 4351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0213 - total_train_reward: -1190.4414\n",
      "Epoch 4352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7755 - total_train_reward: -1451.7272\n",
      "Epoch 4353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7634 - total_train_reward: -1472.3661\n",
      "Epoch 4354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4581 - total_train_reward: -1175.3644\n",
      "Epoch 4355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3340 - total_train_reward: -1070.4115\n",
      "Epoch 4356/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6221 - total_train_reward: -1196.4466\n",
      "Epoch 4357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3685 - total_train_reward: -1069.9856\n",
      "Epoch 4358/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1756 - total_train_reward: -1193.2787\n",
      "Epoch 4359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2475 - total_train_reward: -1025.0497\n",
      "Epoch 4360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8990 - total_train_reward: -1716.9867\n",
      "Epoch 4361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4955 - total_train_reward: -1202.6567\n",
      "Epoch 4362/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3586 - total_train_reward: -1193.5449\n",
      "Epoch 4363/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2034 - total_train_reward: -1605.2741\n",
      "Epoch 4364/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8901 - total_train_reward: -1204.5241\n",
      "Epoch 4365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8742 - total_train_reward: -1026.7856\n",
      "Epoch 4366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2315 - total_train_reward: -1061.9054\n",
      "Epoch 4367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3743 - total_train_reward: -1271.4250\n",
      "Epoch 4368/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4342 - total_train_reward: -1206.8643\n",
      "Epoch 4369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2095 - total_train_reward: -1432.9913\n",
      "Epoch 4370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7261 - total_train_reward: -1315.3870\n",
      "Epoch 4371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1591 - total_train_reward: -1576.4545\n",
      "Epoch 4372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9855 - total_train_reward: -1538.6211\n",
      "Epoch 4373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3278 - total_train_reward: -1182.4549\n",
      "Epoch 4374/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5306 - total_train_reward: -963.1552\n",
      "Epoch 4375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2333 - total_train_reward: -1244.4773\n",
      "Epoch 4376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7004 - total_train_reward: -960.6361\n",
      "Epoch 4377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.2399 - total_train_reward: -847.6552\n",
      "Epoch 4378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6581 - total_train_reward: -1523.6862\n",
      "Epoch 4379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0495 - total_train_reward: -1380.7284\n",
      "Epoch 4380/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3681 - total_train_reward: -1205.5455\n",
      "Epoch 4381/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6804 - total_train_reward: -1201.7128\n",
      "Epoch 4382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5933 - total_train_reward: -1172.4265\n",
      "Epoch 4383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.6613 - total_train_reward: -1191.5126\n",
      "Epoch 4384/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9041 - total_train_reward: -1134.7435\n",
      "Epoch 4385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5476 - total_train_reward: -1681.4769\n",
      "Epoch 4386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2850 - total_train_reward: -1213.5744\n",
      "Epoch 4387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5545 - total_train_reward: -1848.7556\n",
      "Epoch 4388/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1182 - total_train_reward: -1212.9411\n",
      "Epoch 4389/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9410 - total_train_reward: -1851.8346\n",
      "Epoch 4390/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6005 - total_train_reward: -960.6167\n",
      "Epoch 4391/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5917 - total_train_reward: -1213.7711\n",
      "Epoch 4392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3215 - total_train_reward: -1199.9821\n",
      "Epoch 4393/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0180 - total_train_reward: -1278.6711\n",
      "Epoch 4394/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4358 - total_train_reward: -769.8429\n",
      "Epoch 4395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4528 - total_train_reward: -1398.9182\n",
      "Epoch 4396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7611 - total_train_reward: -1200.1786\n",
      "Epoch 4397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7134 - total_train_reward: -1206.2695\n",
      "Epoch 4398/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8451 - total_train_reward: -1611.3336\n",
      "Epoch 4399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2438 - total_train_reward: -1787.4240\n",
      "Epoch 4400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1060 - total_train_reward: -1484.4004\n",
      "Epoch 4401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2912 - total_train_reward: -1168.5163\n",
      "Epoch 4402/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6129 - total_train_reward: -1517.9133\n",
      "Epoch 4403/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1562 - total_train_reward: -1700.3410\n",
      "Epoch 4404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0148 - total_train_reward: -1032.0488\n",
      "Epoch 4405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3568 - total_train_reward: -1331.9646\n",
      "Epoch 4406/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1341 - total_train_reward: -1053.9373\n",
      "Epoch 4407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0428 - total_train_reward: -1292.3872\n",
      "Epoch 4408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4964 - total_train_reward: -1112.8990\n",
      "Epoch 4409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6052 - total_train_reward: -1205.0891\n",
      "Epoch 4410/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1069 - total_train_reward: -1483.6566\n",
      "Epoch 4411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2958 - total_train_reward: -959.4257\n",
      "Epoch 4412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6151 - total_train_reward: -1325.3710\n",
      "Epoch 4413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3309 - total_train_reward: -1472.3679\n",
      "Epoch 4414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0004 - total_train_reward: -1662.6694\n",
      "Epoch 4415/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9371 - total_train_reward: -1802.6099\n",
      "Epoch 4416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9363 - total_train_reward: -1372.3593\n",
      "Epoch 4417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5879 - total_train_reward: -1352.2202\n",
      "Epoch 4418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6273 - total_train_reward: -729.3838\n",
      "Epoch 4419/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8836 - total_train_reward: -1739.8506\n",
      "Epoch 4420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8071 - total_train_reward: -748.3899\n",
      "Epoch 4421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7220 - total_train_reward: -960.8025\n",
      "Epoch 4422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.9426 - total_train_reward: -1202.3255\n",
      "Epoch 4423/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6605 - total_train_reward: -958.6030\n",
      "Epoch 4424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.2636 - total_train_reward: -1445.1425\n",
      "Epoch 4425/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8960 - total_train_reward: -1511.2739\n",
      "Epoch 4426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9443 - total_train_reward: -845.6025\n",
      "Epoch 4427/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7112 - total_train_reward: -1192.3262\n",
      "Epoch 4428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8500 - total_train_reward: -1832.1711\n",
      "Epoch 4429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6509 - total_train_reward: -957.2783\n",
      "Epoch 4430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1982 - total_train_reward: -1350.8928\n",
      "Epoch 4431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4631 - total_train_reward: -1177.5679\n",
      "Epoch 4432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5418 - total_train_reward: -1268.2788\n",
      "Epoch 4433/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3758 - total_train_reward: -1202.6244\n",
      "Epoch 4434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9798 - total_train_reward: -1634.9199\n",
      "Epoch 4435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9604 - total_train_reward: -1520.2426\n",
      "Epoch 4436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8029 - total_train_reward: -1203.4311\n",
      "Epoch 4437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4453 - total_train_reward: -1259.9155\n",
      "Epoch 4438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7691 - total_train_reward: -1086.3271\n",
      "Epoch 4439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8053 - total_train_reward: -1440.2031\n",
      "Epoch 4440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8462 - total_train_reward: -1164.9907\n",
      "Epoch 4441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5624 - total_train_reward: -839.8683\n",
      "Epoch 4442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6083 - total_train_reward: -1483.5732\n",
      "Epoch 4443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9672 - total_train_reward: -1539.6094\n",
      "Epoch 4444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6191 - total_train_reward: -958.8733\n",
      "Epoch 4445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8832 - total_train_reward: -1801.1807\n",
      "Epoch 4446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6573 - total_train_reward: -1679.0032\n",
      "Epoch 4447/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7833 - total_train_reward: -1555.7706\n",
      "Epoch 4448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8624 - total_train_reward: -1169.5518\n",
      "Epoch 4449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8723 - total_train_reward: -1433.1319\n",
      "Epoch 4450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4520 - total_train_reward: -1622.2983\n",
      "Epoch 4451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5322 - total_train_reward: -1217.4993\n",
      "Epoch 4452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9183 - total_train_reward: -958.1456\n",
      "Epoch 4453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9110 - total_train_reward: -1065.0974\n",
      "Epoch 4454/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9421 - total_train_reward: -828.4122\n",
      "Epoch 4455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.2844 - total_train_reward: -1199.5201\n",
      "Epoch 4456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9045 - total_train_reward: -960.0871\n",
      "Epoch 4457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3326 - total_train_reward: -1399.3424\n",
      "Epoch 4458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4124 - total_train_reward: -1533.4294\n",
      "Epoch 4459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2772 - total_train_reward: -1653.0883\n",
      "Epoch 4460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7078 - total_train_reward: -1226.6259\n",
      "Epoch 4461/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5592 - total_train_reward: -1282.7458\n",
      "Epoch 4462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9066 - total_train_reward: -1405.3559\n",
      "Epoch 4463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5444 - total_train_reward: -1210.5730\n",
      "Epoch 4464/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0145 - total_train_reward: -1555.3282\n",
      "Epoch 4465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0223 - total_train_reward: -1212.7812\n",
      "Epoch 4466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4129 - total_train_reward: -1002.7807\n",
      "Epoch 4467/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1561 - total_train_reward: -1705.4986\n",
      "Epoch 4468/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4360 - total_train_reward: -1753.1996\n",
      "Epoch 4469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6368 - total_train_reward: -989.2510\n",
      "Epoch 4470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5371 - total_train_reward: -1331.6960\n",
      "Epoch 4471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9694 - total_train_reward: -1199.7633\n",
      "Epoch 4472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5856 - total_train_reward: -1640.4175\n",
      "Epoch 4473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7481 - total_train_reward: -963.3523\n",
      "Epoch 4474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5048 - total_train_reward: -1810.9507\n",
      "Epoch 4475/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3102 - total_train_reward: -1592.7656\n",
      "Epoch 4476/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2070 - total_train_reward: -1335.3974\n",
      "Epoch 4477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8902 - total_train_reward: -1475.3289\n",
      "Epoch 4478/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3030 - total_train_reward: -1273.6676\n",
      "Epoch 4479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7543 - total_train_reward: -1622.0591\n",
      "Epoch 4480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5556 - total_train_reward: -1711.0609\n",
      "Epoch 4481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6841 - total_train_reward: -1479.1285\n",
      "Epoch 4482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4458 - total_train_reward: -1127.6864\n",
      "Epoch 4483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3620 - total_train_reward: -1116.7828\n",
      "Epoch 4484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5119 - total_train_reward: -1714.4532\n",
      "Epoch 4485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1183 - total_train_reward: -1042.6718\n",
      "Epoch 4486/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5683 - total_train_reward: -1065.1412\n",
      "Epoch 4487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3427 - total_train_reward: -1254.7866\n",
      "Epoch 4488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3504 - total_train_reward: -1203.0562\n",
      "Epoch 4489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2239 - total_train_reward: -1232.8150\n",
      "Epoch 4490/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8835 - total_train_reward: -1083.3674\n",
      "Epoch 4491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0324 - total_train_reward: -1142.8914\n",
      "Epoch 4492/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5973 - total_train_reward: -843.1066\n",
      "Epoch 4493/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9982 - total_train_reward: -1234.6300\n",
      "Epoch 4494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0350 - total_train_reward: -957.0276\n",
      "Epoch 4495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5410 - total_train_reward: -1529.0213\n",
      "Epoch 4496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2324 - total_train_reward: -1234.4223\n",
      "Epoch 4497/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7445 - total_train_reward: -1783.0018\n",
      "Epoch 4498/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4200 - total_train_reward: -958.2697\n",
      "Epoch 4499/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4254 - total_train_reward: -848.8015\n",
      "Epoch 4500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7631 - total_train_reward: -1217.5204\n",
      "Epoch 4501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3886 - total_train_reward: -1187.6154\n",
      "Epoch 4502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0202 - total_train_reward: -1053.1858\n",
      "Epoch 4503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8397 - total_train_reward: -1157.9207\n",
      "Epoch 4504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0723 - total_train_reward: -1120.7940\n",
      "Epoch 4505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9682 - total_train_reward: -1800.5725\n",
      "Epoch 4506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9639 - total_train_reward: -1176.3072\n",
      "Epoch 4507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1774 - total_train_reward: -1605.7739\n",
      "Epoch 4508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1459 - total_train_reward: -1805.5781\n",
      "Epoch 4509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6543 - total_train_reward: -1447.5331\n",
      "Epoch 4510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5123 - total_train_reward: -1180.3869\n",
      "Epoch 4511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3245 - total_train_reward: -988.6598\n",
      "Epoch 4512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0430 - total_train_reward: -1532.8625\n",
      "Epoch 4513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9686 - total_train_reward: -1755.6994\n",
      "Epoch 4514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5278 - total_train_reward: -1609.7565\n",
      "Epoch 4515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2607 - total_train_reward: -1170.3048\n",
      "Epoch 4516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2923 - total_train_reward: -1405.0410\n",
      "Epoch 4517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6931 - total_train_reward: -1242.0974\n",
      "Epoch 4518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5105 - total_train_reward: -1257.0485\n",
      "Epoch 4519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0696 - total_train_reward: -1297.0924\n",
      "Epoch 4520/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7931 - total_train_reward: -1823.7117\n",
      "Epoch 4521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1583 - total_train_reward: -963.4266\n",
      "Epoch 4522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1701 - total_train_reward: -1432.0986\n",
      "Epoch 4523/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7780 - total_train_reward: -1066.6467\n",
      "Epoch 4524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9800 - total_train_reward: -1344.1372\n",
      "Epoch 4525/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4889 - total_train_reward: -1088.5867\n",
      "Epoch 4526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7088 - total_train_reward: -1246.4568\n",
      "Epoch 4527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9871 - total_train_reward: -1536.9997\n",
      "Epoch 4528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8409 - total_train_reward: -1213.3477\n",
      "Epoch 4529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9169 - total_train_reward: -1479.1635\n",
      "Epoch 4530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2572 - total_train_reward: -1133.2783\n",
      "Epoch 4531/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1264 - total_train_reward: -1228.7485\n",
      "Epoch 4532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6602 - total_train_reward: -1192.2277\n",
      "Epoch 4533/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7070 - total_train_reward: -1604.7271\n",
      "Epoch 4534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0535 - total_train_reward: -1034.7898\n",
      "Epoch 4535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6437 - total_train_reward: -1197.9593\n",
      "Epoch 4536/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4470 - total_train_reward: -1212.0742\n",
      "Epoch 4537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3453 - total_train_reward: -1823.9242\n",
      "Epoch 4538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3679 - total_train_reward: -1429.4453\n",
      "Epoch 4539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2084 - total_train_reward: -1238.9085\n",
      "Epoch 4540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3521 - total_train_reward: -881.1441\n",
      "Epoch 4541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9207 - total_train_reward: -1299.1621\n",
      "Epoch 4542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6280 - total_train_reward: -1256.0521\n",
      "Epoch 4543/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1465 - total_train_reward: -1225.0532\n",
      "Epoch 4544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3068 - total_train_reward: -957.3849\n",
      "Epoch 4545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2159 - total_train_reward: -1157.7324\n",
      "Epoch 4546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.6845 - total_train_reward: -1755.9708\n",
      "Epoch 4547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0001 - total_train_reward: -1178.9464\n",
      "Epoch 4548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6506 - total_train_reward: -1169.5899\n",
      "Epoch 4549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1360 - total_train_reward: -1309.2601\n",
      "Epoch 4550/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7867 - total_train_reward: -1221.1875\n",
      "Epoch 4551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2868 - total_train_reward: -1236.7856\n",
      "Epoch 4552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9750 - total_train_reward: -1306.2684\n",
      "Epoch 4553/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.5669 - total_train_reward: -1658.2016\n",
      "Epoch 4554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1747 - total_train_reward: -983.7914\n",
      "Epoch 4555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.7673 - total_train_reward: -1605.1701\n",
      "Epoch 4556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4482 - total_train_reward: -1307.3542\n",
      "Epoch 4557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1188 - total_train_reward: -1234.7965\n",
      "Epoch 4558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1606 - total_train_reward: -1207.3415\n",
      "Epoch 4559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2760 - total_train_reward: -941.4636\n",
      "Epoch 4560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4347 - total_train_reward: -1282.3175\n",
      "Epoch 4561/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4178 - total_train_reward: -1192.2885\n",
      "Epoch 4562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7659 - total_train_reward: -1275.4666\n",
      "Epoch 4563/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4500 - total_train_reward: -912.0186\n",
      "Epoch 4564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7501 - total_train_reward: -676.2294\n",
      "Epoch 4565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1692 - total_train_reward: -1708.9988\n",
      "Epoch 4566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1388 - total_train_reward: -951.0420\n",
      "Epoch 4567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3834 - total_train_reward: -1412.5478\n",
      "Epoch 4568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3945 - total_train_reward: -912.1642\n",
      "Epoch 4569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1498 - total_train_reward: -1448.4881\n",
      "Epoch 4570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7497 - total_train_reward: -1282.1679\n",
      "Epoch 4571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2646 - total_train_reward: -1231.1654\n",
      "Epoch 4572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4130 - total_train_reward: -1552.3787\n",
      "Epoch 4573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0954 - total_train_reward: -1238.5809\n",
      "Epoch 4574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3758 - total_train_reward: -1207.2305\n",
      "Epoch 4575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2724 - total_train_reward: -1051.0437\n",
      "Epoch 4576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3587 - total_train_reward: -1172.5270\n",
      "Epoch 4577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2876 - total_train_reward: -1235.6326\n",
      "Epoch 4578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9515 - total_train_reward: -1216.5733\n",
      "Epoch 4579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7874 - total_train_reward: -1647.0799\n",
      "Epoch 4580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3853 - total_train_reward: -1240.1763\n",
      "Epoch 4581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8846 - total_train_reward: -1224.5069\n",
      "Epoch 4582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2393 - total_train_reward: -1728.3294\n",
      "Epoch 4583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1022 - total_train_reward: -1168.5887\n",
      "Epoch 4584/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5314 - total_train_reward: -1718.0636\n",
      "Epoch 4585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5438 - total_train_reward: -1086.7636\n",
      "Epoch 4586/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.6597 - total_train_reward: -1600.8339\n",
      "Epoch 4587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2578 - total_train_reward: -1395.0167\n",
      "Epoch 4588/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3374 - total_train_reward: -1714.5110\n",
      "Epoch 4589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6795 - total_train_reward: -1063.4239\n",
      "Epoch 4590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6235 - total_train_reward: -1184.8549\n",
      "Epoch 4591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9483 - total_train_reward: -1549.7572\n",
      "Epoch 4592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7871 - total_train_reward: -1204.2862\n",
      "Epoch 4593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6338 - total_train_reward: -1634.7572\n",
      "Epoch 4594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0017 - total_train_reward: -1231.5171\n",
      "Epoch 4595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0703 - total_train_reward: -1184.1715\n",
      "Epoch 4596/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3284 - total_train_reward: -1064.1657\n",
      "Epoch 4597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2761 - total_train_reward: -726.7197\n",
      "Epoch 4598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0369 - total_train_reward: -1419.5597\n",
      "Epoch 4599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0300 - total_train_reward: -1824.3317\n",
      "Epoch 4600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2340 - total_train_reward: -1150.6383\n",
      "Epoch 4601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9090 - total_train_reward: -1223.2644\n",
      "Epoch 4602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3379 - total_train_reward: -1429.2990\n",
      "Epoch 4603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8041 - total_train_reward: -1658.0502\n",
      "Epoch 4604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6527 - total_train_reward: -1260.9076\n",
      "Epoch 4605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3417 - total_train_reward: -1176.0216\n",
      "Epoch 4606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5132 - total_train_reward: -1214.2193\n",
      "Epoch 4607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8458 - total_train_reward: -1787.6507\n",
      "Epoch 4608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5827 - total_train_reward: -1535.8482\n",
      "Epoch 4609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2440 - total_train_reward: -1066.9453\n",
      "Epoch 4610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3923 - total_train_reward: -1172.6220\n",
      "Epoch 4611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4130 - total_train_reward: -1344.3664\n",
      "Epoch 4612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3408 - total_train_reward: -1297.8627\n",
      "Epoch 4613/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6077 - total_train_reward: -1246.4824\n",
      "Epoch 4614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0218 - total_train_reward: -1197.7916\n",
      "Epoch 4615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5701 - total_train_reward: -1244.5136\n",
      "Epoch 4616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4874 - total_train_reward: -1212.9277\n",
      "Epoch 4617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6579 - total_train_reward: -846.4501\n",
      "Epoch 4618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2251 - total_train_reward: -1065.4857\n",
      "Epoch 4619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2723 - total_train_reward: -1217.8080\n",
      "Epoch 4620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5946 - total_train_reward: -1789.4658\n",
      "Epoch 4621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9294 - total_train_reward: -1344.6414\n",
      "Epoch 4622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6648 - total_train_reward: -1722.0250\n",
      "Epoch 4623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1256 - total_train_reward: -1479.6076\n",
      "Epoch 4624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2756 - total_train_reward: -1051.4307\n",
      "Epoch 4625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4803 - total_train_reward: -1224.5332\n",
      "Epoch 4626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2030 - total_train_reward: -1347.7091\n",
      "Epoch 4627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9738 - total_train_reward: -807.8489\n",
      "Epoch 4628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6560 - total_train_reward: -1309.5953\n",
      "Epoch 4629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6126 - total_train_reward: -965.7724\n",
      "Epoch 4630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2937 - total_train_reward: -1300.4432\n",
      "Epoch 4631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1360 - total_train_reward: -1181.2855\n",
      "Epoch 4632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4107 - total_train_reward: -1747.4914\n",
      "Epoch 4633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8247 - total_train_reward: -1170.9139\n",
      "Epoch 4634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3521 - total_train_reward: -1167.3652\n",
      "Epoch 4635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7972 - total_train_reward: -1180.0638\n",
      "Epoch 4636/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3061 - total_train_reward: -1063.2780\n",
      "Epoch 4637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3953 - total_train_reward: -1343.1475\n",
      "Epoch 4638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4674 - total_train_reward: -1218.0923\n",
      "Epoch 4639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3635 - total_train_reward: -1224.8846\n",
      "Epoch 4640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2826 - total_train_reward: -962.6754\n",
      "Epoch 4641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9111 - total_train_reward: -1204.8474\n",
      "Epoch 4642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5014 - total_train_reward: -1542.7447\n",
      "Epoch 4643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1080 - total_train_reward: -1218.7960\n",
      "Epoch 4644/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7341 - total_train_reward: -1760.8640\n",
      "Epoch 4645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3248 - total_train_reward: -1371.3537\n",
      "Epoch 4646/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0082 - total_train_reward: -1294.0586\n",
      "Epoch 4647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7636 - total_train_reward: -1222.8421\n",
      "Epoch 4648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.5411 - total_train_reward: -1050.9251\n",
      "Epoch 4649/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7441 - total_train_reward: -1208.0773\n",
      "Epoch 4650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3633 - total_train_reward: -1754.7200\n",
      "Epoch 4651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4571 - total_train_reward: -1399.1618\n",
      "Epoch 4652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7576 - total_train_reward: -1421.5349\n",
      "Epoch 4653/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0286 - total_train_reward: -1206.2988\n",
      "Epoch 4654/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.3884 - total_train_reward: -1597.0931\n",
      "Epoch 4655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7258 - total_train_reward: -1206.4016\n",
      "Epoch 4656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6255 - total_train_reward: -957.2913\n",
      "Epoch 4657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3895 - total_train_reward: -1100.4533\n",
      "Epoch 4658/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5418 - total_train_reward: -1346.9273\n",
      "Epoch 4659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1328 - total_train_reward: -1290.3041\n",
      "Epoch 4660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5103 - total_train_reward: -1563.6066\n",
      "Epoch 4661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0378 - total_train_reward: -1150.1204\n",
      "Epoch 4662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7695 - total_train_reward: -1064.9116\n",
      "Epoch 4663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3446 - total_train_reward: -1044.2959\n",
      "Epoch 4664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4816 - total_train_reward: -1211.5469\n",
      "Epoch 4665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6038 - total_train_reward: -1263.4710\n",
      "Epoch 4666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1900 - total_train_reward: -1571.4813\n",
      "Epoch 4667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4747 - total_train_reward: -1337.5005\n",
      "Epoch 4668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3537 - total_train_reward: -1212.3449\n",
      "Epoch 4669/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0458 - total_train_reward: -1480.4141\n",
      "Epoch 4670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4466 - total_train_reward: -752.5279\n",
      "Epoch 4671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0337 - total_train_reward: -1729.6108\n",
      "Epoch 4672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6036 - total_train_reward: -1085.8435\n",
      "Epoch 4673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4973 - total_train_reward: -1603.4347\n",
      "Epoch 4674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.0902 - total_train_reward: -1208.6383\n",
      "Epoch 4675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1667 - total_train_reward: -1322.4981\n",
      "Epoch 4676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2387 - total_train_reward: -1278.1638\n",
      "Epoch 4677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1585 - total_train_reward: -1458.7256\n",
      "Epoch 4678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1367 - total_train_reward: -1321.4895\n",
      "Epoch 4679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8766 - total_train_reward: -1542.1002\n",
      "Epoch 4680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3431 - total_train_reward: -1272.6182\n",
      "Epoch 4681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9864 - total_train_reward: -1210.3483\n",
      "Epoch 4682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1863 - total_train_reward: -1462.5923\n",
      "Epoch 4683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6700 - total_train_reward: -1257.0974\n",
      "Epoch 4684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3966 - total_train_reward: -1296.2747\n",
      "Epoch 4685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1689 - total_train_reward: -1419.0676\n",
      "Epoch 4686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0973 - total_train_reward: -1204.8105\n",
      "Epoch 4687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0693 - total_train_reward: -1176.6786\n",
      "Epoch 4688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9324 - total_train_reward: -1526.7268\n",
      "Epoch 4689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9807 - total_train_reward: -1815.1995\n",
      "Epoch 4690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9035 - total_train_reward: -895.1286\n",
      "Epoch 4691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8671 - total_train_reward: -1317.7758\n",
      "Epoch 4692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3644 - total_train_reward: -1288.1545\n",
      "Epoch 4693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9308 - total_train_reward: -1209.0998\n",
      "Epoch 4694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6716 - total_train_reward: -1093.6476\n",
      "Epoch 4695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8242 - total_train_reward: -1199.7409\n",
      "Epoch 4696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1101 - total_train_reward: -1784.9212\n",
      "Epoch 4697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6363 - total_train_reward: -934.6388\n",
      "Epoch 4698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2560 - total_train_reward: -1517.3754\n",
      "Epoch 4699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1734 - total_train_reward: -1207.8140\n",
      "Epoch 4700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4226 - total_train_reward: -1207.0527\n",
      "Epoch 4701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9638 - total_train_reward: -1197.6364\n",
      "Epoch 4702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5704 - total_train_reward: -1425.0362\n",
      "Epoch 4703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6812 - total_train_reward: -1809.2769\n",
      "Epoch 4704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3992 - total_train_reward: -1712.9365\n",
      "Epoch 4705/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1205 - total_train_reward: -1194.3741\n",
      "Epoch 4706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5160 - total_train_reward: -1193.4083\n",
      "Epoch 4707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0394 - total_train_reward: -1171.3505\n",
      "Epoch 4708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.8356 - total_train_reward: -842.1458\n",
      "Epoch 4709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8461 - total_train_reward: -1298.7064\n",
      "Epoch 4710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5498 - total_train_reward: -1469.5791\n",
      "Epoch 4711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6269 - total_train_reward: -1064.6594\n",
      "Epoch 4712/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0232 - total_train_reward: -1791.6006\n",
      "Epoch 4713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3600 - total_train_reward: -1213.5970\n",
      "Epoch 4714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5793 - total_train_reward: -1390.5995\n",
      "Epoch 4715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0169 - total_train_reward: -1276.7015\n",
      "Epoch 4716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5155 - total_train_reward: -1763.9857\n",
      "Epoch 4717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5966 - total_train_reward: -1767.3517\n",
      "Epoch 4718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0925 - total_train_reward: -1165.5959\n",
      "Epoch 4719/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4983 - total_train_reward: -952.5748\n",
      "Epoch 4720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1568 - total_train_reward: -1831.6887\n",
      "Epoch 4721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5251 - total_train_reward: -1352.0895\n",
      "Epoch 4722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2145 - total_train_reward: -1491.4494\n",
      "Epoch 4723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7380 - total_train_reward: -1209.5513\n",
      "Epoch 4724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1887 - total_train_reward: -1291.0957\n",
      "Epoch 4725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3605 - total_train_reward: -1211.3528\n",
      "Epoch 4726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4490 - total_train_reward: -1071.3779\n",
      "Epoch 4727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5139 - total_train_reward: -1593.4179\n",
      "Epoch 4728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2033 - total_train_reward: -1193.0729\n",
      "Epoch 4729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1243 - total_train_reward: -1654.1506\n",
      "Epoch 4730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2007 - total_train_reward: -851.0770\n",
      "Epoch 4731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7294 - total_train_reward: -1164.9640\n",
      "Epoch 4732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4488 - total_train_reward: -1515.0076\n",
      "Epoch 4733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8067 - total_train_reward: -972.7587\n",
      "Epoch 4734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0823 - total_train_reward: -1238.3955\n",
      "Epoch 4735/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6533 - total_train_reward: -1196.3267\n",
      "Epoch 4736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0003 - total_train_reward: -1339.0311\n",
      "Epoch 4737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8209 - total_train_reward: -1537.8728\n",
      "Epoch 4738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0399 - total_train_reward: -1526.9221\n",
      "Epoch 4739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2661 - total_train_reward: -1342.8890\n",
      "Epoch 4740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7271 - total_train_reward: -1209.4558\n",
      "Epoch 4741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5251 - total_train_reward: -1211.5307\n",
      "Epoch 4742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7334 - total_train_reward: -1073.7449\n",
      "Epoch 4743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1706 - total_train_reward: -1363.1408\n",
      "Epoch 4744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4367 - total_train_reward: -934.8936\n",
      "Epoch 4745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.0770 - total_train_reward: -1568.6259\n",
      "Epoch 4746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5537 - total_train_reward: -1589.9228\n",
      "Epoch 4747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7156 - total_train_reward: -1548.1499\n",
      "Epoch 4748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2706 - total_train_reward: -1142.1892\n",
      "Epoch 4749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7684 - total_train_reward: -1409.6539\n",
      "Epoch 4750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6648 - total_train_reward: -1188.4825\n",
      "Epoch 4751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4898 - total_train_reward: -1501.0582\n",
      "Epoch 4752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1787 - total_train_reward: -1215.4612\n",
      "Epoch 4753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0990 - total_train_reward: -1290.1433\n",
      "Epoch 4754/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6189 - total_train_reward: -1021.8947\n",
      "Epoch 4755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0317 - total_train_reward: -1550.2724\n",
      "Epoch 4756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3547 - total_train_reward: -1160.3056\n",
      "Epoch 4757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2986 - total_train_reward: -1098.7628\n",
      "Epoch 4758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6979 - total_train_reward: -1043.7511\n",
      "Epoch 4759/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5245 - total_train_reward: -1819.4822\n",
      "Epoch 4760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9449 - total_train_reward: -1049.4300\n",
      "Epoch 4761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1113 - total_train_reward: -1144.6467\n",
      "Epoch 4762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0434 - total_train_reward: -1174.7074\n",
      "Epoch 4763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3356 - total_train_reward: -1423.8203\n",
      "Epoch 4764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2063 - total_train_reward: -1178.5057\n",
      "Epoch 4765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4379 - total_train_reward: -1215.4968\n",
      "Epoch 4766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5987 - total_train_reward: -1326.0035\n",
      "Epoch 4767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7962 - total_train_reward: -1504.2773\n",
      "Epoch 4768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3784 - total_train_reward: -1051.4683\n",
      "Epoch 4769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8315 - total_train_reward: -1836.0517\n",
      "Epoch 4770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9323 - total_train_reward: -1094.3028\n",
      "Epoch 4771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.5131 - total_train_reward: -1626.5365\n",
      "Epoch 4772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1190 - total_train_reward: -1536.8149\n",
      "Epoch 4773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8234 - total_train_reward: -1213.1058\n",
      "Epoch 4774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8262 - total_train_reward: -1215.9736\n",
      "Epoch 4775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4436 - total_train_reward: -1202.6990\n",
      "Epoch 4776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7587 - total_train_reward: -957.8693\n",
      "Epoch 4777/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0870 - total_train_reward: -1142.0493\n",
      "Epoch 4778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0216 - total_train_reward: -1211.6156\n",
      "Epoch 4779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5798 - total_train_reward: -1168.8272\n",
      "Epoch 4780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5777 - total_train_reward: -1832.9260\n",
      "Epoch 4781/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5728 - total_train_reward: -1506.7272\n",
      "Epoch 4782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1701 - total_train_reward: -1301.4274\n",
      "Epoch 4783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4711 - total_train_reward: -1787.2452\n",
      "Epoch 4784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7441 - total_train_reward: -1333.5763\n",
      "Epoch 4785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9023 - total_train_reward: -1206.6065\n",
      "Epoch 4786/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3525 - total_train_reward: -1375.6376\n",
      "Epoch 4787/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9549 - total_train_reward: -1225.4992\n",
      "Epoch 4788/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9114 - total_train_reward: -1587.8850\n",
      "Epoch 4789/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8390 - total_train_reward: -1758.3103\n",
      "Epoch 4790/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9855 - total_train_reward: -1208.9200\n",
      "Epoch 4791/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1719 - total_train_reward: -1218.7025\n",
      "Epoch 4792/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1670 - total_train_reward: -1332.6732\n",
      "Epoch 4793/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3724 - total_train_reward: -1257.6138\n",
      "Epoch 4794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4516 - total_train_reward: -1815.4578\n",
      "Epoch 4795/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9846 - total_train_reward: -1455.0471\n",
      "Epoch 4796/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5752 - total_train_reward: -1201.3130\n",
      "Epoch 4797/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9798 - total_train_reward: -1594.4529\n",
      "Epoch 4798/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5154 - total_train_reward: -1211.1980\n",
      "Epoch 4799/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6883 - total_train_reward: -1596.7780\n",
      "Epoch 4800/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7829 - total_train_reward: -1291.2324\n",
      "Epoch 4801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5121 - total_train_reward: -1410.0255\n",
      "Epoch 4802/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6055 - total_train_reward: -1054.6156\n",
      "Epoch 4803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7494 - total_train_reward: -1208.1128\n",
      "Epoch 4804/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0935 - total_train_reward: -1203.8579\n",
      "Epoch 4805/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4171 - total_train_reward: -1334.4318\n",
      "Epoch 4806/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6793 - total_train_reward: -1215.5823\n",
      "Epoch 4807/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2757 - total_train_reward: -1278.4747\n",
      "Epoch 4808/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3210 - total_train_reward: -1769.7829\n",
      "Epoch 4809/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2198 - total_train_reward: -962.4776\n",
      "Epoch 4810/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7518 - total_train_reward: -1176.9571\n",
      "Epoch 4811/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2369 - total_train_reward: -1213.0958\n",
      "Epoch 4812/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8166 - total_train_reward: -1066.5363\n",
      "Epoch 4813/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7894 - total_train_reward: -1165.7321\n",
      "Epoch 4814/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8150 - total_train_reward: -1196.0201\n",
      "Epoch 4815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0395 - total_train_reward: -1347.9823\n",
      "Epoch 4816/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7905 - total_train_reward: -1242.1960\n",
      "Epoch 4817/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4666 - total_train_reward: -958.5634\n",
      "Epoch 4818/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7693 - total_train_reward: -1653.7275\n",
      "Epoch 4819/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3870 - total_train_reward: -1200.0893\n",
      "Epoch 4820/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0037 - total_train_reward: -1334.9341\n",
      "Epoch 4821/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8126 - total_train_reward: -1520.2246\n",
      "Epoch 4822/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6186 - total_train_reward: -1171.0300\n",
      "Epoch 4823/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8360 - total_train_reward: -1299.9149\n",
      "Epoch 4824/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8134 - total_train_reward: -1529.0439\n",
      "Epoch 4825/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1574 - total_train_reward: -1306.0684\n",
      "Epoch 4826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4440 - total_train_reward: -1316.2657\n",
      "Epoch 4827/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1099 - total_train_reward: -1707.0002\n",
      "Epoch 4828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2412 - total_train_reward: -1785.8445\n",
      "Epoch 4829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8057 - total_train_reward: -1798.4546\n",
      "Epoch 4830/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9465 - total_train_reward: -1521.1705\n",
      "Epoch 4831/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5319 - total_train_reward: -1200.9876\n",
      "Epoch 4832/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8106 - total_train_reward: -1137.2153\n",
      "Epoch 4833/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2678 - total_train_reward: -1488.1265\n",
      "Epoch 4834/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6847 - total_train_reward: -1770.4523\n",
      "Epoch 4835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8447 - total_train_reward: -1747.1515\n",
      "Epoch 4836/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1506 - total_train_reward: -1225.7743\n",
      "Epoch 4837/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0205 - total_train_reward: -1201.6362\n",
      "Epoch 4838/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5621 - total_train_reward: -1239.3133\n",
      "Epoch 4839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0195 - total_train_reward: -1558.6740\n",
      "Epoch 4840/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5375 - total_train_reward: -1197.5838\n",
      "Epoch 4841/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2000 - total_train_reward: -1170.7900\n",
      "Epoch 4842/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2641 - total_train_reward: -1157.8342\n",
      "Epoch 4843/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9066 - total_train_reward: -1204.0867\n",
      "Epoch 4844/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3997 - total_train_reward: -1246.9618\n",
      "Epoch 4845/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4439 - total_train_reward: -1161.5150\n",
      "Epoch 4846/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5577 - total_train_reward: -1214.1471\n",
      "Epoch 4847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8532 - total_train_reward: -968.0092\n",
      "Epoch 4848/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6332 - total_train_reward: -1284.2725\n",
      "Epoch 4849/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9965 - total_train_reward: -1275.3084\n",
      "Epoch 4850/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1342 - total_train_reward: -1222.9780\n",
      "Epoch 4851/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4886 - total_train_reward: -1089.0759\n",
      "Epoch 4852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6973 - total_train_reward: -1198.9239\n",
      "Epoch 4853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2709 - total_train_reward: -1202.8235\n",
      "Epoch 4854/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9353 - total_train_reward: -1102.5024\n",
      "Epoch 4855/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2590 - total_train_reward: -1068.6364\n",
      "Epoch 4856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9970 - total_train_reward: -1195.7057\n",
      "Epoch 4857/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9613 - total_train_reward: -1200.9923\n",
      "Epoch 4858/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7345 - total_train_reward: -1047.4446\n",
      "Epoch 4859/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6206 - total_train_reward: -1097.0506\n",
      "Epoch 4860/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9458 - total_train_reward: -1182.9893\n",
      "Epoch 4861/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5008 - total_train_reward: -1724.2895\n",
      "Epoch 4862/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1340 - total_train_reward: -1493.0015\n",
      "Epoch 4863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1965 - total_train_reward: -1278.8471\n",
      "Epoch 4864/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9917 - total_train_reward: -1426.9840\n",
      "Epoch 4865/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9104 - total_train_reward: -1087.2026\n",
      "Epoch 4866/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0236 - total_train_reward: -1394.1403\n",
      "Epoch 4867/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6931 - total_train_reward: -1670.6863\n",
      "Epoch 4868/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6804 - total_train_reward: -907.4325\n",
      "Epoch 4869/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0401 - total_train_reward: -1066.7732\n",
      "Epoch 4870/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5714 - total_train_reward: -1547.6260\n",
      "Epoch 4871/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8169 - total_train_reward: -1163.2373\n",
      "Epoch 4872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7289 - total_train_reward: -1491.4158\n",
      "Epoch 4873/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8537 - total_train_reward: -1251.0004\n",
      "Epoch 4874/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1437 - total_train_reward: -1203.0361\n",
      "Epoch 4875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2711 - total_train_reward: -1655.9502\n",
      "Epoch 4876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1618 - total_train_reward: -1191.6604\n",
      "Epoch 4877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0997 - total_train_reward: -1235.2919\n",
      "Epoch 4878/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7371 - total_train_reward: -1167.1879\n",
      "Epoch 4879/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3893 - total_train_reward: -1199.2388\n",
      "Epoch 4880/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1735 - total_train_reward: -1022.1622\n",
      "Epoch 4881/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5206 - total_train_reward: -1753.6333\n",
      "Epoch 4882/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7324 - total_train_reward: -1760.1137\n",
      "Epoch 4883/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8085 - total_train_reward: -1170.6103\n",
      "Epoch 4884/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7908 - total_train_reward: -1201.9561\n",
      "Epoch 4885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1074 - total_train_reward: -1192.7219\n",
      "Epoch 4886/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9146 - total_train_reward: -1307.7616\n",
      "Epoch 4887/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0645 - total_train_reward: -1140.3037\n",
      "Epoch 4888/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2434 - total_train_reward: -1368.4492\n",
      "Epoch 4889/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2478 - total_train_reward: -958.6545\n",
      "Epoch 4890/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4178 - total_train_reward: -1153.1333\n",
      "Epoch 4891/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6522 - total_train_reward: -958.7813\n",
      "Epoch 4892/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1582 - total_train_reward: -1198.6230\n",
      "Epoch 4893/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5587 - total_train_reward: -1284.6508\n",
      "Epoch 4894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3553 - total_train_reward: -1410.1402\n",
      "Epoch 4895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5654 - total_train_reward: -1059.6681\n",
      "Epoch 4896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5438 - total_train_reward: -965.0947\n",
      "Epoch 4897/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5151 - total_train_reward: -1212.2057\n",
      "Epoch 4898/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9661 - total_train_reward: -1052.9421\n",
      "Epoch 4899/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1082 - total_train_reward: -1808.2515\n",
      "Epoch 4900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0517 - total_train_reward: -1409.2217\n",
      "Epoch 4901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8751 - total_train_reward: -1202.4422\n",
      "Epoch 4902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5427 - total_train_reward: -1106.5041\n",
      "Epoch 4903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3542 - total_train_reward: -1743.6974\n",
      "Epoch 4904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3369 - total_train_reward: -1064.7552\n",
      "Epoch 4905/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1412 - total_train_reward: -1164.9398\n",
      "Epoch 4906/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4380 - total_train_reward: -985.5050\n",
      "Epoch 4907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5205 - total_train_reward: -1598.8099\n",
      "Epoch 4908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6912 - total_train_reward: -1203.7111\n",
      "Epoch 4909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7287 - total_train_reward: -1715.8091\n",
      "Epoch 4910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8005 - total_train_reward: -1368.9170\n",
      "Epoch 4911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4983 - total_train_reward: -1279.5635\n",
      "Epoch 4912/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8961 - total_train_reward: -1344.9952\n",
      "Epoch 4913/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9897 - total_train_reward: -1848.6067\n",
      "Epoch 4914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2856 - total_train_reward: -1208.0086\n",
      "Epoch 4915/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3359 - total_train_reward: -1037.6859\n",
      "Epoch 4916/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8977 - total_train_reward: -961.4746\n",
      "Epoch 4917/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2875 - total_train_reward: -722.6017\n",
      "Epoch 4918/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9363 - total_train_reward: -727.0266\n",
      "Epoch 4919/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 23.7180 - total_train_reward: -1197.6345\n",
      "Epoch 4920/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9844 - total_train_reward: -1739.9399\n",
      "Epoch 4921/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8887 - total_train_reward: -1717.8046\n",
      "Epoch 4922/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9115 - total_train_reward: -1066.4174\n",
      "Epoch 4923/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3411 - total_train_reward: -1186.1506\n",
      "Epoch 4924/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6229 - total_train_reward: -1171.4457\n",
      "Epoch 4925/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6391 - total_train_reward: -955.1178\n",
      "Epoch 4926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3003 - total_train_reward: -1352.7455\n",
      "Epoch 4927/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9586 - total_train_reward: -748.2064\n",
      "Epoch 4928/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6615 - total_train_reward: -1433.5721\n",
      "Epoch 4929/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1331 - total_train_reward: -1188.5833\n",
      "Epoch 4930/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.7769 - total_train_reward: -1203.6285\n",
      "Epoch 4931/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0099 - total_train_reward: -1756.6612\n",
      "Epoch 4932/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3227 - total_train_reward: -1840.9801\n",
      "Epoch 4933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9086 - total_train_reward: -1163.5004\n",
      "Epoch 4934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3402 - total_train_reward: -870.8799\n",
      "Epoch 4935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8463 - total_train_reward: -984.0844\n",
      "Epoch 4936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6592 - total_train_reward: -1068.6936\n",
      "Epoch 4937/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1822 - total_train_reward: -1064.2018\n",
      "Epoch 4938/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6420 - total_train_reward: -1724.4248\n",
      "Epoch 4939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7902 - total_train_reward: -1722.1137\n",
      "Epoch 4940/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0388 - total_train_reward: -1196.5068\n",
      "Epoch 4941/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9421 - total_train_reward: -1025.2846\n",
      "Epoch 4942/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1161 - total_train_reward: -1066.4382\n",
      "Epoch 4943/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7018 - total_train_reward: -1454.2644\n",
      "Epoch 4944/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7267 - total_train_reward: -1236.4375\n",
      "Epoch 4945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8635 - total_train_reward: -1191.5883\n",
      "Epoch 4946/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6367 - total_train_reward: -1204.5224\n",
      "Epoch 4947/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6337 - total_train_reward: -1060.3648\n",
      "Epoch 4948/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.2150 - total_train_reward: -1650.5316\n",
      "Epoch 4949/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9485 - total_train_reward: -1414.4491\n",
      "Epoch 4950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5343 - total_train_reward: -1200.4711\n",
      "Epoch 4951/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0852 - total_train_reward: -1328.5204\n",
      "Epoch 4952/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6842 - total_train_reward: -1199.8351\n",
      "Epoch 4953/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3774 - total_train_reward: -1567.9907\n",
      "Epoch 4954/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1927 - total_train_reward: -1542.5363\n",
      "Epoch 4955/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3596 - total_train_reward: -1200.9956\n",
      "Epoch 4956/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6709 - total_train_reward: -1503.7503\n",
      "Epoch 4957/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4559 - total_train_reward: -960.5993\n",
      "Epoch 4958/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2548 - total_train_reward: -1745.7418\n",
      "Epoch 4959/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7987 - total_train_reward: -1164.3533\n",
      "Epoch 4960/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1603 - total_train_reward: -1222.5544\n",
      "Epoch 4961/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4496 - total_train_reward: -1090.8103\n",
      "Epoch 4962/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4881 - total_train_reward: -1732.6134\n",
      "Epoch 4963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2417 - total_train_reward: -1393.4963\n",
      "Epoch 4964/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0597 - total_train_reward: -1089.4064\n",
      "Epoch 4965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1035 - total_train_reward: -1160.5646\n",
      "Epoch 4966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5690 - total_train_reward: -840.6005\n",
      "Epoch 4967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2163 - total_train_reward: -727.9673\n",
      "Epoch 4968/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9159 - total_train_reward: -1205.9664\n",
      "Epoch 4969/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9494 - total_train_reward: -1158.0252\n",
      "Epoch 4970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -16.9099 - total_train_reward: -486.1383\n",
      "Epoch 4971/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 25.7105 - total_train_reward: -1473.4407\n",
      "Epoch 4972/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6931 - total_train_reward: -1831.6048\n",
      "Epoch 4973/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.4784 - total_train_reward: -982.8492\n",
      "Epoch 4974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 36.5104 - total_train_reward: -1203.6573\n",
      "Epoch 4975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3833 - total_train_reward: -1100.3639\n",
      "Epoch 4976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2366 - total_train_reward: -1203.0133\n",
      "Epoch 4977/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4426 - total_train_reward: -1677.0103\n",
      "Epoch 4978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1009 - total_train_reward: -1623.2382\n",
      "Epoch 4979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3176 - total_train_reward: -1167.8854\n",
      "Epoch 4980/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1945 - total_train_reward: -1202.0009\n",
      "Epoch 4981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.2279 - total_train_reward: -1821.3401\n",
      "Epoch 4982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.3878 - total_train_reward: -1574.7043\n",
      "Epoch 4983/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0174 - total_train_reward: -1119.0471\n",
      "Epoch 4984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7340 - total_train_reward: -1278.4700\n",
      "Epoch 4985/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3216 - total_train_reward: -1599.5479\n",
      "Epoch 4986/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3560 - total_train_reward: -1588.6275\n",
      "Epoch 4987/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4544 - total_train_reward: -1210.2695\n",
      "Epoch 4988/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9610 - total_train_reward: -1179.0686\n",
      "Epoch 4989/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.9523 - total_train_reward: -1194.5284\n",
      "Epoch 4990/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8574 - total_train_reward: -1185.6360\n",
      "Epoch 4991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2828 - total_train_reward: -1335.2101\n",
      "Epoch 4992/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1367 - total_train_reward: -1307.3495\n",
      "Epoch 4993/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0053 - total_train_reward: -965.6396\n",
      "Epoch 4994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1050 - total_train_reward: -1190.7763\n",
      "Epoch 4995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8287 - total_train_reward: -957.9304\n",
      "Epoch 4996/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4166 - total_train_reward: -1066.1053\n",
      "Epoch 4997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4761 - total_train_reward: -1592.9708\n",
      "Epoch 4998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9929 - total_train_reward: -732.6972\n",
      "Epoch 4999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0597 - total_train_reward: -1198.9152\n",
      "Epoch 5000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9753 - total_train_reward: -1278.5520\n",
      "Epoch 5001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3095 - total_train_reward: -1388.2783\n",
      "Epoch 5002/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3884 - total_train_reward: -1348.1587\n",
      "Epoch 5003/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2652 - total_train_reward: -1544.5150\n",
      "Epoch 5004/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8347 - total_train_reward: -1474.9560\n",
      "Epoch 5005/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0854 - total_train_reward: -1631.4921\n",
      "Epoch 5006/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4086 - total_train_reward: -1323.2222\n",
      "Epoch 5007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3097 - total_train_reward: -1264.9118\n",
      "Epoch 5008/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5054 - total_train_reward: -1201.6797\n",
      "Epoch 5009/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7253 - total_train_reward: -1220.8670\n",
      "Epoch 5010/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 20.3211 - total_train_reward: -909.5358\n",
      "Epoch 5011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8929 - total_train_reward: -1777.1445\n",
      "Epoch 5012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2647 - total_train_reward: -1701.7173\n",
      "Epoch 5013/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7837 - total_train_reward: -1467.1334\n",
      "Epoch 5014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2014 - total_train_reward: -1285.7362\n",
      "Epoch 5015/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9400 - total_train_reward: -1184.2573\n",
      "Epoch 5016/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4831 - total_train_reward: -1076.3759\n",
      "Epoch 5017/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 19.8460 - total_train_reward: -1192.6734\n",
      "Epoch 5018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.9662 - total_train_reward: -1354.5000\n",
      "Epoch 5019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6269 - total_train_reward: -728.9189\n",
      "Epoch 5020/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6430 - total_train_reward: -1201.9217\n",
      "Epoch 5021/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1921 - total_train_reward: -1500.7371\n",
      "Epoch 5022/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3195 - total_train_reward: -1196.6073\n",
      "Epoch 5023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0059 - total_train_reward: -1495.6509\n",
      "Epoch 5024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5481 - total_train_reward: -1330.6147\n",
      "Epoch 5025/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6094 - total_train_reward: -1778.5180\n",
      "Epoch 5026/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0075 - total_train_reward: -960.4098\n",
      "Epoch 5027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6947 - total_train_reward: -1696.8951\n",
      "Epoch 5028/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7541 - total_train_reward: -1435.8422\n",
      "Epoch 5029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0000 - total_train_reward: -1561.0895\n",
      "Epoch 5030/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5985 - total_train_reward: -782.4703\n",
      "Epoch 5031/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 23.9742 - total_train_reward: -730.3065\n",
      "Epoch 5032/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 27.1160 - total_train_reward: -1758.9556\n",
      "Epoch 5033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1152 - total_train_reward: -1485.7561\n",
      "Epoch 5034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6062 - total_train_reward: -1549.1444\n",
      "Epoch 5035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5015 - total_train_reward: -1118.3147\n",
      "Epoch 5036/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 28.1662 - total_train_reward: -1208.3151\n",
      "Epoch 5037/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7929 - total_train_reward: -1208.9694\n",
      "Epoch 5038/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3133 - total_train_reward: -1192.0765\n",
      "Epoch 5039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.4357 - total_train_reward: -1211.6585\n",
      "Epoch 5040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.2104 - total_train_reward: -1235.9929\n",
      "Epoch 5041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1121 - total_train_reward: -1287.5680\n",
      "Epoch 5042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1715 - total_train_reward: -1313.5289\n",
      "Epoch 5043/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1306 - total_train_reward: -1770.5392\n",
      "Epoch 5044/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6065 - total_train_reward: -1309.3780\n",
      "Epoch 5045/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1393 - total_train_reward: -1353.4652\n",
      "Epoch 5046/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1984 - total_train_reward: -1064.8627\n",
      "Epoch 5047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8279 - total_train_reward: -1067.6998\n",
      "Epoch 5048/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3275 - total_train_reward: -1355.8541\n",
      "Epoch 5049/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4727 - total_train_reward: -1626.7159\n",
      "Epoch 5050/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1609 - total_train_reward: -1115.3275\n",
      "Epoch 5051/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8354 - total_train_reward: -1204.1152\n",
      "Epoch 5052/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1528 - total_train_reward: -1168.6754\n",
      "Epoch 5053/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0638 - total_train_reward: -1778.4339\n",
      "Epoch 5054/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2832 - total_train_reward: -961.3078\n",
      "Epoch 5055/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2220 - total_train_reward: -1415.9721\n",
      "Epoch 5056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2040 - total_train_reward: -1599.7417\n",
      "Epoch 5057/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1674 - total_train_reward: -1328.3399\n",
      "Epoch 5058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8308 - total_train_reward: -1647.8366\n",
      "Epoch 5059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2245 - total_train_reward: -1417.3940\n",
      "Epoch 5060/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6870 - total_train_reward: -1214.6393\n",
      "Epoch 5061/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6809 - total_train_reward: -959.6898\n",
      "Epoch 5062/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6027 - total_train_reward: -1328.5127\n",
      "Epoch 5063/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8621 - total_train_reward: -1336.3329\n",
      "Epoch 5064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8677 - total_train_reward: -1525.0202\n",
      "Epoch 5065/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1562 - total_train_reward: -1170.5573\n",
      "Epoch 5066/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8519 - total_train_reward: -1206.4442\n",
      "Epoch 5067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4713 - total_train_reward: -1325.8635\n",
      "Epoch 5068/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6114 - total_train_reward: -1386.1874\n",
      "Epoch 5069/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4665 - total_train_reward: -1073.7072\n",
      "Epoch 5070/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1265 - total_train_reward: -1215.5248\n",
      "Epoch 5071/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0219 - total_train_reward: -1837.8594\n",
      "Epoch 5072/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.2852 - total_train_reward: -1779.4142\n",
      "Epoch 5073/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9420 - total_train_reward: -1617.4314\n",
      "Epoch 5074/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1249 - total_train_reward: -1305.4425\n",
      "Epoch 5075/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9642 - total_train_reward: -1067.8700\n",
      "Epoch 5076/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2397 - total_train_reward: -1206.1459\n",
      "Epoch 5077/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7405 - total_train_reward: -950.0712\n",
      "Epoch 5078/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7584 - total_train_reward: -1215.1090\n",
      "Epoch 5079/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7791 - total_train_reward: -1213.4962\n",
      "Epoch 5080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9363 - total_train_reward: -1313.4500\n",
      "Epoch 5081/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 22.2723 - total_train_reward: -911.5942\n",
      "Epoch 5082/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1369 - total_train_reward: -814.7240\n",
      "Epoch 5083/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0656 - total_train_reward: -1065.8649\n",
      "Epoch 5084/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8792 - total_train_reward: -1213.3195\n",
      "Epoch 5085/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6933 - total_train_reward: -1167.2698\n",
      "Epoch 5086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.6671 - total_train_reward: -1195.8618\n",
      "Epoch 5087/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3730 - total_train_reward: -1816.1527\n",
      "Epoch 5088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9692 - total_train_reward: -1356.6714\n",
      "Epoch 5089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4446 - total_train_reward: -1310.4690\n",
      "Epoch 5090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8599 - total_train_reward: -1210.4095\n",
      "Epoch 5091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0029 - total_train_reward: -1527.4653\n",
      "Epoch 5092/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4526 - total_train_reward: -1755.5800\n",
      "Epoch 5093/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1474 - total_train_reward: -1218.6640\n",
      "Epoch 5094/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4703 - total_train_reward: -1275.5418\n",
      "Epoch 5095/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0155 - total_train_reward: -1216.3008\n",
      "Epoch 5096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9772 - total_train_reward: -1053.4011\n",
      "Epoch 5097/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6559 - total_train_reward: -1169.5065\n",
      "Epoch 5098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.1441 - total_train_reward: -905.7401\n",
      "Epoch 5099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.4485 - total_train_reward: -1200.2490\n",
      "Epoch 5100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6409 - total_train_reward: -1079.1927\n",
      "Epoch 5101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6121 - total_train_reward: -1188.7642\n",
      "Epoch 5102/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1039 - total_train_reward: -1218.0463\n",
      "Epoch 5103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7800 - total_train_reward: -1242.7895\n",
      "Epoch 5104/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2561 - total_train_reward: -1229.9627\n",
      "Epoch 5105/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5140 - total_train_reward: -1821.4181\n",
      "Epoch 5106/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6561 - total_train_reward: -1834.7758\n",
      "Epoch 5107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6815 - total_train_reward: -1106.9095\n",
      "Epoch 5108/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2213 - total_train_reward: -1214.4064\n",
      "Epoch 5109/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9993 - total_train_reward: -1705.7525\n",
      "Epoch 5110/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4038 - total_train_reward: -1306.9985\n",
      "Epoch 5111/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9230 - total_train_reward: -1822.6696\n",
      "Epoch 5112/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4433 - total_train_reward: -965.3727\n",
      "Epoch 5113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0646 - total_train_reward: -1642.8275\n",
      "Epoch 5114/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5701 - total_train_reward: -1214.9155\n",
      "Epoch 5115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5158 - total_train_reward: -1183.1901\n",
      "Epoch 5116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7831 - total_train_reward: -1215.1780\n",
      "Epoch 5117/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8841 - total_train_reward: -1281.3079\n",
      "Epoch 5118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9508 - total_train_reward: -1691.5717\n",
      "Epoch 5119/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9954 - total_train_reward: -1326.2321\n",
      "Epoch 5120/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0477 - total_train_reward: -1419.1609\n",
      "Epoch 5121/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8369 - total_train_reward: -1326.2745\n",
      "Epoch 5122/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8213 - total_train_reward: -1106.2293\n",
      "Epoch 5123/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2634 - total_train_reward: -1524.7604\n",
      "Epoch 5124/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0609 - total_train_reward: -1429.5108\n",
      "Epoch 5125/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5931 - total_train_reward: -1214.9330\n",
      "Epoch 5126/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4173 - total_train_reward: -1600.2033\n",
      "Epoch 5127/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3554 - total_train_reward: -1706.0702\n",
      "Epoch 5128/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7024 - total_train_reward: -1078.3850\n",
      "Epoch 5129/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7975 - total_train_reward: -1087.1450\n",
      "Epoch 5130/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8955 - total_train_reward: -1384.8551\n",
      "Epoch 5131/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1855 - total_train_reward: -1454.7220\n",
      "Epoch 5132/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1747 - total_train_reward: -1169.5872\n",
      "Epoch 5133/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5910 - total_train_reward: -1792.5579\n",
      "Epoch 5134/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9863 - total_train_reward: -1697.3397\n",
      "Epoch 5135/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8148 - total_train_reward: -1201.4100\n",
      "Epoch 5136/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1394 - total_train_reward: -1424.1046\n",
      "Epoch 5137/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1522 - total_train_reward: -1645.6076\n",
      "Epoch 5138/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9356 - total_train_reward: -1211.3049\n",
      "Epoch 5139/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6553 - total_train_reward: -1212.3922\n",
      "Epoch 5140/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1864 - total_train_reward: -1400.2440\n",
      "Epoch 5141/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6343 - total_train_reward: -1168.6371\n",
      "Epoch 5142/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0704 - total_train_reward: -1153.9980\n",
      "Epoch 5143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2295 - total_train_reward: -1348.2456\n",
      "Epoch 5144/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9004 - total_train_reward: -1645.0413\n",
      "Epoch 5145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0877 - total_train_reward: -729.9935\n",
      "Epoch 5146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0906 - total_train_reward: -1552.4199\n",
      "Epoch 5147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8660 - total_train_reward: -1548.4221\n",
      "Epoch 5148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.5109 - total_train_reward: -1220.2647\n",
      "Epoch 5149/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 18.9915 - total_train_reward: -901.8318\n",
      "Epoch 5150/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 18.5387 - total_train_reward: -1582.6108\n",
      "Epoch 5151/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4985 - total_train_reward: -1573.9799\n",
      "Epoch 5152/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5788 - total_train_reward: -1512.3777\n",
      "Epoch 5153/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6108 - total_train_reward: -1835.6120\n",
      "Epoch 5154/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.7866 - total_train_reward: -1216.6261\n",
      "Epoch 5155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7826 - total_train_reward: -1290.7140\n",
      "Epoch 5156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2549 - total_train_reward: -1148.1290\n",
      "Epoch 5157/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9752 - total_train_reward: -1208.6143\n",
      "Epoch 5158/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3466 - total_train_reward: -1291.6329\n",
      "Epoch 5159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7366 - total_train_reward: -1503.0717\n",
      "Epoch 5160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7981 - total_train_reward: -1174.9854\n",
      "Epoch 5161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0017 - total_train_reward: -1546.2674\n",
      "Epoch 5162/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2051 - total_train_reward: -1174.8865\n",
      "Epoch 5163/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6554 - total_train_reward: -1810.5557\n",
      "Epoch 5164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8840 - total_train_reward: -1210.5541\n",
      "Epoch 5165/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4269 - total_train_reward: -1425.8559\n",
      "Epoch 5166/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5169 - total_train_reward: -1800.2033\n",
      "Epoch 5167/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4722 - total_train_reward: -1128.9614\n",
      "Epoch 5168/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3404 - total_train_reward: -956.8685\n",
      "Epoch 5169/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6085 - total_train_reward: -1752.3546\n",
      "Epoch 5170/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1985 - total_train_reward: -1401.5262\n",
      "Epoch 5171/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3404 - total_train_reward: -1522.6679\n",
      "Epoch 5172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3387 - total_train_reward: -1442.4974\n",
      "Epoch 5173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9346 - total_train_reward: -1317.2113\n",
      "Epoch 5174/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9170 - total_train_reward: -1217.9419\n",
      "Epoch 5175/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3507 - total_train_reward: -1357.1418\n",
      "Epoch 5176/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4573 - total_train_reward: -1752.8925\n",
      "Epoch 5177/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4011 - total_train_reward: -958.0061\n",
      "Epoch 5178/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9787 - total_train_reward: -1011.5657\n",
      "Epoch 5179/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4814 - total_train_reward: -1170.6553\n",
      "Epoch 5180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3264 - total_train_reward: -1362.5735\n",
      "Epoch 5181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5483 - total_train_reward: -1164.7633\n",
      "Epoch 5182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9447 - total_train_reward: -1819.9894\n",
      "Epoch 5183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8463 - total_train_reward: -1195.0637\n",
      "Epoch 5184/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9997 - total_train_reward: -1220.3794\n",
      "Epoch 5185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7498 - total_train_reward: -1500.4098\n",
      "Epoch 5186/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6874 - total_train_reward: -1747.8715\n",
      "Epoch 5187/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8342 - total_train_reward: -1213.8361\n",
      "Epoch 5188/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9566 - total_train_reward: -1217.4604\n",
      "Epoch 5189/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2480 - total_train_reward: -861.3773\n",
      "Epoch 5190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5054 - total_train_reward: -918.6079\n",
      "Epoch 5191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3330 - total_train_reward: -1602.2930\n",
      "Epoch 5192/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9024 - total_train_reward: -1833.6682\n",
      "Epoch 5193/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7481 - total_train_reward: -606.2580\n",
      "Epoch 5194/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2927 - total_train_reward: -1322.9937\n",
      "Epoch 5195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1323 - total_train_reward: -1831.2127\n",
      "Epoch 5196/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9018 - total_train_reward: -1271.8318\n",
      "Epoch 5197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2503 - total_train_reward: -1504.3460\n",
      "Epoch 5198/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4074 - total_train_reward: -1179.7615\n",
      "Epoch 5199/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2751 - total_train_reward: -958.5145\n",
      "Epoch 5200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.7684 - total_train_reward: -1403.5127\n",
      "Epoch 5201/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4248 - total_train_reward: -1005.3898\n",
      "Epoch 5202/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9294 - total_train_reward: -1309.0412\n",
      "Epoch 5203/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4739 - total_train_reward: -1815.7549\n",
      "Epoch 5204/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7703 - total_train_reward: -1316.8751\n",
      "Epoch 5205/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6530 - total_train_reward: -1266.7084\n",
      "Epoch 5206/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8963 - total_train_reward: -1204.4692\n",
      "Epoch 5207/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5275 - total_train_reward: -1082.8583\n",
      "Epoch 5208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3774 - total_train_reward: -1736.3984\n",
      "Epoch 5209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1231 - total_train_reward: -1209.5771\n",
      "Epoch 5210/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5100 - total_train_reward: -1815.0645\n",
      "Epoch 5211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2710 - total_train_reward: -1553.6081\n",
      "Epoch 5212/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2697 - total_train_reward: -1397.5844\n",
      "Epoch 5213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5984 - total_train_reward: -1252.1088\n",
      "Epoch 5214/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6025 - total_train_reward: -1283.3152\n",
      "Epoch 5215/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2304 - total_train_reward: -1073.3817\n",
      "Epoch 5216/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3669 - total_train_reward: -1249.0769\n",
      "Epoch 5217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2178 - total_train_reward: -1203.3214\n",
      "Epoch 5218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1627 - total_train_reward: -1209.6342\n",
      "Epoch 5219/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9303 - total_train_reward: -1216.9701\n",
      "Epoch 5220/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9513 - total_train_reward: -1067.5865\n",
      "Epoch 5221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7556 - total_train_reward: -1213.5283\n",
      "Epoch 5222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0425 - total_train_reward: -1447.5223\n",
      "Epoch 5223/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.6788 - total_train_reward: -1687.1027\n",
      "Epoch 5224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3201 - total_train_reward: -1188.4963\n",
      "Epoch 5225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4156 - total_train_reward: -1322.6836\n",
      "Epoch 5226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3991 - total_train_reward: -1210.5797\n",
      "Epoch 5227/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1614 - total_train_reward: -1264.8359\n",
      "Epoch 5228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7249 - total_train_reward: -1217.1390\n",
      "Epoch 5229/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5666 - total_train_reward: -1296.2303\n",
      "Epoch 5230/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6322 - total_train_reward: -1515.0708\n",
      "Epoch 5231/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3564 - total_train_reward: -844.6410\n",
      "Epoch 5232/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8644 - total_train_reward: -1710.2963\n",
      "Epoch 5233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3227 - total_train_reward: -1215.4288\n",
      "Epoch 5234/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7610 - total_train_reward: -1219.5846\n",
      "Epoch 5235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3690 - total_train_reward: -958.6440\n",
      "Epoch 5236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6691 - total_train_reward: -1220.7711\n",
      "Epoch 5237/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5744 - total_train_reward: -918.8068\n",
      "Epoch 5238/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.4749 - total_train_reward: -1123.2525\n",
      "Epoch 5239/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2099 - total_train_reward: -1328.6869\n",
      "Epoch 5240/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.0728 - total_train_reward: -1514.2540\n",
      "Epoch 5241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1887 - total_train_reward: -1221.9604\n",
      "Epoch 5242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9724 - total_train_reward: -1466.6070\n",
      "Epoch 5243/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0990 - total_train_reward: -1207.5878\n",
      "Epoch 5244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7173 - total_train_reward: -1208.6352\n",
      "Epoch 5245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9097 - total_train_reward: -1760.1303\n",
      "Epoch 5246/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8661 - total_train_reward: -959.2606\n",
      "Epoch 5247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5761 - total_train_reward: -1415.9283\n",
      "Epoch 5248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9162 - total_train_reward: -1203.9106\n",
      "Epoch 5249/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4215 - total_train_reward: -1306.5409\n",
      "Epoch 5250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2525 - total_train_reward: -852.6215\n",
      "Epoch 5251/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8730 - total_train_reward: -1186.8776\n",
      "Epoch 5252/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3208 - total_train_reward: -1287.2051\n",
      "Epoch 5253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4820 - total_train_reward: -1214.2309\n",
      "Epoch 5254/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6556 - total_train_reward: -1062.9493\n",
      "Epoch 5255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6722 - total_train_reward: -726.8400\n",
      "Epoch 5256/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4258 - total_train_reward: -1448.2101\n",
      "Epoch 5257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4922 - total_train_reward: -1285.3168\n",
      "Epoch 5258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0661 - total_train_reward: -1561.8432\n",
      "Epoch 5259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9020 - total_train_reward: -1212.0436\n",
      "Epoch 5260/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0011 - total_train_reward: -845.5754\n",
      "Epoch 5261/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 19.3960 - total_train_reward: -1231.9422\n",
      "Epoch 5262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4311 - total_train_reward: -1742.9307\n",
      "Epoch 5263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2519 - total_train_reward: -1051.3283\n",
      "Epoch 5264/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6012 - total_train_reward: -1091.5096\n",
      "Epoch 5265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7300 - total_train_reward: -1511.1380\n",
      "Epoch 5266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0017 - total_train_reward: -1685.9531\n",
      "Epoch 5267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9632 - total_train_reward: -1082.3251\n",
      "Epoch 5268/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0150 - total_train_reward: -1804.6067\n",
      "Epoch 5269/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1128 - total_train_reward: -1634.7390\n",
      "Epoch 5270/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9913 - total_train_reward: -1289.2428\n",
      "Epoch 5271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4864 - total_train_reward: -1064.0660\n",
      "Epoch 5272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1847 - total_train_reward: -1710.7994\n",
      "Epoch 5273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5700 - total_train_reward: -1199.9349\n",
      "Epoch 5274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7485 - total_train_reward: -960.8742\n",
      "Epoch 5275/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0933 - total_train_reward: -1836.3898\n",
      "Epoch 5276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6531 - total_train_reward: -1216.0927\n",
      "Epoch 5277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0329 - total_train_reward: -1441.4704\n",
      "Epoch 5278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5353 - total_train_reward: -1483.6471\n",
      "Epoch 5279/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1102 - total_train_reward: -1063.6531\n",
      "Epoch 5280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8812 - total_train_reward: -1206.0885\n",
      "Epoch 5281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4778 - total_train_reward: -819.6413\n",
      "Epoch 5282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7602 - total_train_reward: -1442.2785\n",
      "Epoch 5283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9812 - total_train_reward: -542.4923\n",
      "Epoch 5284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.2792 - total_train_reward: -1259.1226\n",
      "Epoch 5285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6411 - total_train_reward: -1059.4064\n",
      "Epoch 5286/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0899 - total_train_reward: -958.8732\n",
      "Epoch 5287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9123 - total_train_reward: -1389.6897\n",
      "Epoch 5288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 19.8176 - total_train_reward: -1212.9902\n",
      "Epoch 5289/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1205 - total_train_reward: -1283.5655\n",
      "Epoch 5290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3436 - total_train_reward: -831.1694\n",
      "Epoch 5291/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6543 - total_train_reward: -1043.9922\n",
      "Epoch 5292/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6868 - total_train_reward: -1216.2897\n",
      "Epoch 5293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3122 - total_train_reward: -747.6752\n",
      "Epoch 5294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4535 - total_train_reward: -1391.3790\n",
      "Epoch 5295/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2125 - total_train_reward: -1593.5632\n",
      "Epoch 5296/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2481 - total_train_reward: -1700.4570\n",
      "Epoch 5297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6355 - total_train_reward: -1782.2559\n",
      "Epoch 5298/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2611 - total_train_reward: -1634.0243\n",
      "Epoch 5299/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1486 - total_train_reward: -963.3175\n",
      "Epoch 5300/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2992 - total_train_reward: -1283.6463\n",
      "Epoch 5301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7382 - total_train_reward: -1346.5706\n",
      "Epoch 5302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1002 - total_train_reward: -1462.6287\n",
      "Epoch 5303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6028 - total_train_reward: -1257.6774\n",
      "Epoch 5304/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6718 - total_train_reward: -955.5298\n",
      "Epoch 5305/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2389 - total_train_reward: -1756.2175\n",
      "Epoch 5306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2403 - total_train_reward: -1222.1511\n",
      "Epoch 5307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9244 - total_train_reward: -1027.7604\n",
      "Epoch 5308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6899 - total_train_reward: -1471.2202\n",
      "Epoch 5309/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2409 - total_train_reward: -1752.4526\n",
      "Epoch 5310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5702 - total_train_reward: -1169.6153\n",
      "Epoch 5311/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8885 - total_train_reward: -1501.6335\n",
      "Epoch 5312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9795 - total_train_reward: -1277.6986\n",
      "Epoch 5313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8190 - total_train_reward: -1293.8673\n",
      "Epoch 5314/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3976 - total_train_reward: -743.9529\n",
      "Epoch 5315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8466 - total_train_reward: -1177.1921\n",
      "Epoch 5316/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4937 - total_train_reward: -1222.6574\n",
      "Epoch 5317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3557 - total_train_reward: -1227.3537\n",
      "Epoch 5318/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0262 - total_train_reward: -1192.3467\n",
      "Epoch 5319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6868 - total_train_reward: -1552.5233\n",
      "Epoch 5320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1313 - total_train_reward: -1687.3790\n",
      "Epoch 5321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3389 - total_train_reward: -1258.2961\n",
      "Epoch 5322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8535 - total_train_reward: -958.3707\n",
      "Epoch 5323/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4117 - total_train_reward: -1826.6420\n",
      "Epoch 5324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8047 - total_train_reward: -973.9127\n",
      "Epoch 5325/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7730 - total_train_reward: -1796.6746\n",
      "Epoch 5326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5245 - total_train_reward: -1781.6208\n",
      "Epoch 5327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4846 - total_train_reward: -956.9529\n",
      "Epoch 5328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4588 - total_train_reward: -1211.1525\n",
      "Epoch 5329/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0823 - total_train_reward: -960.5452\n",
      "Epoch 5330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7153 - total_train_reward: -1283.4946\n",
      "Epoch 5331/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1596 - total_train_reward: -1213.4500\n",
      "Epoch 5332/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6556 - total_train_reward: -1165.8970\n",
      "Epoch 5333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0404 - total_train_reward: -1167.6821\n",
      "Epoch 5334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4895 - total_train_reward: -1445.5769\n",
      "Epoch 5335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1461 - total_train_reward: -1440.7451\n",
      "Epoch 5336/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.6107 - total_train_reward: -1209.9950\n",
      "Epoch 5337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8329 - total_train_reward: -1211.4984\n",
      "Epoch 5338/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.3670 - total_train_reward: -1066.2585\n",
      "Epoch 5339/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6619 - total_train_reward: -1837.2736\n",
      "Epoch 5340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8702 - total_train_reward: -1453.9398\n",
      "Epoch 5341/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.2048 - total_train_reward: -1197.1482\n",
      "Epoch 5342/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2339 - total_train_reward: -1201.1127\n",
      "Epoch 5343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6521 - total_train_reward: -1786.5044\n",
      "Epoch 5344/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7167 - total_train_reward: -1201.1249\n",
      "Epoch 5345/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1670 - total_train_reward: -1212.7498\n",
      "Epoch 5346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8120 - total_train_reward: -1833.9865\n",
      "Epoch 5347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6123 - total_train_reward: -1211.4971\n",
      "Epoch 5348/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6834 - total_train_reward: -1171.7849\n",
      "Epoch 5349/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3095 - total_train_reward: -1199.6144\n",
      "Epoch 5350/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3587 - total_train_reward: -1211.9994\n",
      "Epoch 5351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0076 - total_train_reward: -1080.4780\n",
      "Epoch 5352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2541 - total_train_reward: -960.2157\n",
      "Epoch 5353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9985 - total_train_reward: -1357.2372\n",
      "Epoch 5354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9935 - total_train_reward: -1169.9734\n",
      "Epoch 5355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7757 - total_train_reward: -1580.1876\n",
      "Epoch 5356/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6639 - total_train_reward: -1681.3191\n",
      "Epoch 5357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6122 - total_train_reward: -1001.8219\n",
      "Epoch 5358/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8933 - total_train_reward: -873.0533\n",
      "Epoch 5359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.8547 - total_train_reward: -1071.5970\n",
      "Epoch 5360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2218 - total_train_reward: -586.6098\n",
      "Epoch 5361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1750 - total_train_reward: -1481.0177\n",
      "Epoch 5362/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6335 - total_train_reward: -1567.8589\n",
      "Epoch 5363/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.2751 - total_train_reward: -1196.2337\n",
      "Epoch 5364/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4853 - total_train_reward: -1402.7917\n",
      "Epoch 5365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0761 - total_train_reward: -1804.4228\n",
      "Epoch 5366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5024 - total_train_reward: -1790.1807\n",
      "Epoch 5367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.4504 - total_train_reward: -790.0866\n",
      "Epoch 5368/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8233 - total_train_reward: -1154.9080\n",
      "Epoch 5369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1659 - total_train_reward: -1095.5994\n",
      "Epoch 5370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5962 - total_train_reward: -1200.9020\n",
      "Epoch 5371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6007 - total_train_reward: -1057.3480\n",
      "Epoch 5372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4598 - total_train_reward: -1167.0497\n",
      "Epoch 5373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3745 - total_train_reward: -1525.1807\n",
      "Epoch 5374/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6209 - total_train_reward: -1024.6912\n",
      "Epoch 5375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1599 - total_train_reward: -1713.2778\n",
      "Epoch 5376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1527 - total_train_reward: -606.4785\n",
      "Epoch 5377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7379 - total_train_reward: -1728.8648\n",
      "Epoch 5378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7363 - total_train_reward: -1262.9391\n",
      "Epoch 5379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1779 - total_train_reward: -1195.4050\n",
      "Epoch 5380/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6205 - total_train_reward: -1291.1213\n",
      "Epoch 5381/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.9987 - total_train_reward: -1210.0616\n",
      "Epoch 5382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9712 - total_train_reward: -1405.0603\n",
      "Epoch 5383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9301 - total_train_reward: -1711.1905\n",
      "Epoch 5384/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6451 - total_train_reward: -1503.9571\n",
      "Epoch 5385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9560 - total_train_reward: -1270.6039\n",
      "Epoch 5386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1108 - total_train_reward: -1070.4552\n",
      "Epoch 5387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.4237 - total_train_reward: -1204.6715\n",
      "Epoch 5388/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1408 - total_train_reward: -1014.2863\n",
      "Epoch 5389/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6911 - total_train_reward: -1474.4649\n",
      "Epoch 5390/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8965 - total_train_reward: -1747.6132\n",
      "Epoch 5391/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6039 - total_train_reward: -1163.2544\n",
      "Epoch 5392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3929 - total_train_reward: -1795.4630\n",
      "Epoch 5393/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8087 - total_train_reward: -1330.4741\n",
      "Epoch 5394/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9779 - total_train_reward: -1552.2631\n",
      "Epoch 5395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2281 - total_train_reward: -936.2794\n",
      "Epoch 5396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7971 - total_train_reward: -1308.4045\n",
      "Epoch 5397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3537 - total_train_reward: -1200.7478\n",
      "Epoch 5398/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2516 - total_train_reward: -1074.2629\n",
      "Epoch 5399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3242 - total_train_reward: -1193.4265\n",
      "Epoch 5400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.2279 - total_train_reward: -907.1952\n",
      "Epoch 5401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7852 - total_train_reward: -1372.3508\n",
      "Epoch 5402/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4774 - total_train_reward: -1711.9498\n",
      "Epoch 5403/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4163 - total_train_reward: -967.2591\n",
      "Epoch 5404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2529 - total_train_reward: -958.9765\n",
      "Epoch 5405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5288 - total_train_reward: -1296.7469\n",
      "Epoch 5406/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8291 - total_train_reward: -1191.5470\n",
      "Epoch 5407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2152 - total_train_reward: -1171.5591\n",
      "Epoch 5408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5776 - total_train_reward: -1196.7899\n",
      "Epoch 5409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8639 - total_train_reward: -1643.5722\n",
      "Epoch 5410/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8555 - total_train_reward: -960.5520\n",
      "Epoch 5411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9447 - total_train_reward: -1062.7070\n",
      "Epoch 5412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8896 - total_train_reward: -969.5595\n",
      "Epoch 5413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2530 - total_train_reward: -1168.6028\n",
      "Epoch 5414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1048 - total_train_reward: -1840.0993\n",
      "Epoch 5415/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2164 - total_train_reward: -962.4766\n",
      "Epoch 5416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9697 - total_train_reward: -1722.6843\n",
      "Epoch 5417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3906 - total_train_reward: -1612.8828\n",
      "Epoch 5418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0186 - total_train_reward: -1831.3943\n",
      "Epoch 5419/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3100 - total_train_reward: -1142.4660\n",
      "Epoch 5420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3115 - total_train_reward: -960.9008\n",
      "Epoch 5421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -17.3317 - total_train_reward: -1537.9681\n",
      "Epoch 5422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.9162 - total_train_reward: -1202.6445\n",
      "Epoch 5423/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9903 - total_train_reward: -1072.2030\n",
      "Epoch 5424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1486 - total_train_reward: -1179.0074\n",
      "Epoch 5425/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5690 - total_train_reward: -1186.9383\n",
      "Epoch 5426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5899 - total_train_reward: -1069.4714\n",
      "Epoch 5427/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1500 - total_train_reward: -955.0992\n",
      "Epoch 5428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4415 - total_train_reward: -1344.2085\n",
      "Epoch 5429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9151 - total_train_reward: -1524.4438\n",
      "Epoch 5430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0738 - total_train_reward: -1206.0925\n",
      "Epoch 5431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4972 - total_train_reward: -1196.6357\n",
      "Epoch 5432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2044 - total_train_reward: -1708.5976\n",
      "Epoch 5433/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4939 - total_train_reward: -1078.0503\n",
      "Epoch 5434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3362 - total_train_reward: -1049.5681\n",
      "Epoch 5435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1285 - total_train_reward: -1664.9613\n",
      "Epoch 5436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.7180 - total_train_reward: -1186.4902\n",
      "Epoch 5437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5212 - total_train_reward: -980.2613\n",
      "Epoch 5438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1170 - total_train_reward: -1625.9450\n",
      "Epoch 5439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6880 - total_train_reward: -1202.1790\n",
      "Epoch 5440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1760 - total_train_reward: -1204.7114\n",
      "Epoch 5441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0538 - total_train_reward: -1167.0001\n",
      "Epoch 5442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1479 - total_train_reward: -1186.6432\n",
      "Epoch 5443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0518 - total_train_reward: -1305.6144\n",
      "Epoch 5444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0461 - total_train_reward: -1279.7503\n",
      "Epoch 5445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6872 - total_train_reward: -1724.4975\n",
      "Epoch 5446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5652 - total_train_reward: -1834.3780\n",
      "Epoch 5447/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2060 - total_train_reward: -1707.7673\n",
      "Epoch 5448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2611 - total_train_reward: -961.1677\n",
      "Epoch 5449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0865 - total_train_reward: -1614.7761\n",
      "Epoch 5450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1370 - total_train_reward: -1255.8620\n",
      "Epoch 5451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4533 - total_train_reward: -1695.3883\n",
      "Epoch 5452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2352 - total_train_reward: -1450.5562\n",
      "Epoch 5453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5034 - total_train_reward: -1059.3797\n",
      "Epoch 5454/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4423 - total_train_reward: -1180.8764\n",
      "Epoch 5455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0921 - total_train_reward: -1646.8695\n",
      "Epoch 5456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4795 - total_train_reward: -1340.3005\n",
      "Epoch 5457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8914 - total_train_reward: -1746.3316\n",
      "Epoch 5458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2979 - total_train_reward: -1207.6910\n",
      "Epoch 5459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2976 - total_train_reward: -1272.7066\n",
      "Epoch 5460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4939 - total_train_reward: -1199.3900\n",
      "Epoch 5461/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9093 - total_train_reward: -1492.4534\n",
      "Epoch 5462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.7452 - total_train_reward: -1201.4150\n",
      "Epoch 5463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8905 - total_train_reward: -1822.7217\n",
      "Epoch 5464/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5618 - total_train_reward: -1357.4953\n",
      "Epoch 5465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0216 - total_train_reward: -1258.3006\n",
      "Epoch 5466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1549 - total_train_reward: -1592.7639\n",
      "Epoch 5467/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7046 - total_train_reward: -1201.7605\n",
      "Epoch 5468/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8793 - total_train_reward: -1207.9620\n",
      "Epoch 5469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0320 - total_train_reward: -1733.2142\n",
      "Epoch 5470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2924 - total_train_reward: -1213.8465\n",
      "Epoch 5471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2522 - total_train_reward: -1449.0998\n",
      "Epoch 5472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0745 - total_train_reward: -1080.1714\n",
      "Epoch 5473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1701 - total_train_reward: -958.8928\n",
      "Epoch 5474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0735 - total_train_reward: -1585.5302\n",
      "Epoch 5475/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.9639 - total_train_reward: -1200.4058\n",
      "Epoch 5476/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2323 - total_train_reward: -1435.0349\n",
      "Epoch 5477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6072 - total_train_reward: -1767.0231\n",
      "Epoch 5478/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9510 - total_train_reward: -1063.9231\n",
      "Epoch 5479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2132 - total_train_reward: -930.9383\n",
      "Epoch 5480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6439 - total_train_reward: -955.8467\n",
      "Epoch 5481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0882 - total_train_reward: -1080.7460\n",
      "Epoch 5482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2042 - total_train_reward: -1203.5947\n",
      "Epoch 5483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7116 - total_train_reward: -1388.7601\n",
      "Epoch 5484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3969 - total_train_reward: -1809.4910\n",
      "Epoch 5485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5106 - total_train_reward: -1683.3866\n",
      "Epoch 5486/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8658 - total_train_reward: -1197.3564\n",
      "Epoch 5487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6780 - total_train_reward: -727.4749\n",
      "Epoch 5488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8390 - total_train_reward: -1202.2241\n",
      "Epoch 5489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4579 - total_train_reward: -1670.7491\n",
      "Epoch 5490/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7808 - total_train_reward: -1271.3365\n",
      "Epoch 5491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8638 - total_train_reward: -1024.8739\n",
      "Epoch 5492/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1375 - total_train_reward: -1201.6487\n",
      "Epoch 5493/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8696 - total_train_reward: -1740.9637\n",
      "Epoch 5494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5938 - total_train_reward: -1409.9003\n",
      "Epoch 5495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1088 - total_train_reward: -1203.2538\n",
      "Epoch 5496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1248 - total_train_reward: -1726.1323\n",
      "Epoch 5497/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5685 - total_train_reward: -1221.4463\n",
      "Epoch 5498/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8621 - total_train_reward: -852.5253\n",
      "Epoch 5499/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9591 - total_train_reward: -1176.5399\n",
      "Epoch 5500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3386 - total_train_reward: -1226.2018\n",
      "Epoch 5501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5405 - total_train_reward: -1211.8677\n",
      "Epoch 5502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9345 - total_train_reward: -1148.4976\n",
      "Epoch 5503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8853 - total_train_reward: -1307.7203\n",
      "Epoch 5504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4547 - total_train_reward: -1076.7887\n",
      "Epoch 5505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9810 - total_train_reward: -1081.3588\n",
      "Epoch 5506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7050 - total_train_reward: -1572.1941\n",
      "Epoch 5507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8901 - total_train_reward: -1318.2811\n",
      "Epoch 5508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4042 - total_train_reward: -1154.7076\n",
      "Epoch 5509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0517 - total_train_reward: -1070.9135\n",
      "Epoch 5510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6766 - total_train_reward: -725.8893\n",
      "Epoch 5511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8257 - total_train_reward: -1067.4819\n",
      "Epoch 5512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0871 - total_train_reward: -1062.6002\n",
      "Epoch 5513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0271 - total_train_reward: -1349.6981\n",
      "Epoch 5514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7657 - total_train_reward: -1352.9379\n",
      "Epoch 5515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2188 - total_train_reward: -1177.4062\n",
      "Epoch 5516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0223 - total_train_reward: -783.9028\n",
      "Epoch 5517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4294 - total_train_reward: -1005.6066\n",
      "Epoch 5518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7429 - total_train_reward: -1285.1359\n",
      "Epoch 5519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0751 - total_train_reward: -1807.6277\n",
      "Epoch 5520/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6841 - total_train_reward: -1341.1921\n",
      "Epoch 5521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0676 - total_train_reward: -1743.7628\n",
      "Epoch 5522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3892 - total_train_reward: -1444.6442\n",
      "Epoch 5523/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8736 - total_train_reward: -1039.2871\n",
      "Epoch 5524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0996 - total_train_reward: -1176.0482\n",
      "Epoch 5525/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1751 - total_train_reward: -1475.4473\n",
      "Epoch 5526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6234 - total_train_reward: -1674.1968\n",
      "Epoch 5527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9681 - total_train_reward: -1203.6920\n",
      "Epoch 5528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4988 - total_train_reward: -1424.6384\n",
      "Epoch 5529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7992 - total_train_reward: -1670.3124\n",
      "Epoch 5530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2175 - total_train_reward: -1173.5448\n",
      "Epoch 5531/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8080 - total_train_reward: -1604.1256\n",
      "Epoch 5532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8435 - total_train_reward: -1061.5001\n",
      "Epoch 5533/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3191 - total_train_reward: -1229.0170\n",
      "Epoch 5534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8453 - total_train_reward: -1209.3344\n",
      "Epoch 5535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5988 - total_train_reward: -1221.0965\n",
      "Epoch 5536/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9886 - total_train_reward: -1786.7765\n",
      "Epoch 5537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3581 - total_train_reward: -1218.4668\n",
      "Epoch 5538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4309 - total_train_reward: -1214.8667\n",
      "Epoch 5539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2210 - total_train_reward: -1565.7996\n",
      "Epoch 5540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4109 - total_train_reward: -1818.0117\n",
      "Epoch 5541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9616 - total_train_reward: -1167.6672\n",
      "Epoch 5542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2882 - total_train_reward: -827.5345\n",
      "Epoch 5543/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7536 - total_train_reward: -1462.5248\n",
      "Epoch 5544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4420 - total_train_reward: -1503.8298\n",
      "Epoch 5545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3673 - total_train_reward: -1200.1215\n",
      "Epoch 5546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6074 - total_train_reward: -1163.9508\n",
      "Epoch 5547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9425 - total_train_reward: -1449.7758\n",
      "Epoch 5548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6578 - total_train_reward: -1782.4717\n",
      "Epoch 5549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2068 - total_train_reward: -1279.0146\n",
      "Epoch 5550/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0878 - total_train_reward: -1532.0425\n",
      "Epoch 5551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3314 - total_train_reward: -1235.6386\n",
      "Epoch 5552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0422 - total_train_reward: -1267.0133\n",
      "Epoch 5553/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0235 - total_train_reward: -1202.3524\n",
      "Epoch 5554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4505 - total_train_reward: -1223.4992\n",
      "Epoch 5555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7678 - total_train_reward: -1359.2976\n",
      "Epoch 5556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7542 - total_train_reward: -1214.8434\n",
      "Epoch 5557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8868 - total_train_reward: -1713.2639\n",
      "Epoch 5558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2174 - total_train_reward: -1091.8773\n",
      "Epoch 5559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8456 - total_train_reward: -1222.3127\n",
      "Epoch 5560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3640 - total_train_reward: -1188.0104\n",
      "Epoch 5561/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6921 - total_train_reward: -1303.4762\n",
      "Epoch 5562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5742 - total_train_reward: -1196.8737\n",
      "Epoch 5563/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4868 - total_train_reward: -1223.8309\n",
      "Epoch 5564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2800 - total_train_reward: -1146.5397\n",
      "Epoch 5565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2834 - total_train_reward: -1212.1329\n",
      "Epoch 5566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8405 - total_train_reward: -1718.4870\n",
      "Epoch 5567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5798 - total_train_reward: -1151.7942\n",
      "Epoch 5568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8986 - total_train_reward: -986.4936\n",
      "Epoch 5569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2481 - total_train_reward: -1218.5557\n",
      "Epoch 5570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2106 - total_train_reward: -1655.4411\n",
      "Epoch 5571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8410 - total_train_reward: -1808.1936\n",
      "Epoch 5572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8064 - total_train_reward: -952.0506\n",
      "Epoch 5573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0917 - total_train_reward: -1210.2438\n",
      "Epoch 5574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1393 - total_train_reward: -1023.0440\n",
      "Epoch 5575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9855 - total_train_reward: -1498.2143\n",
      "Epoch 5576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8822 - total_train_reward: -1501.7485\n",
      "Epoch 5577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3705 - total_train_reward: -1241.4297\n",
      "Epoch 5578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0760 - total_train_reward: -1203.5329\n",
      "Epoch 5579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6931 - total_train_reward: -1065.5102\n",
      "Epoch 5580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8651 - total_train_reward: -1304.2857\n",
      "Epoch 5581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6597 - total_train_reward: -1059.3997\n",
      "Epoch 5582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5863 - total_train_reward: -1545.1380\n",
      "Epoch 5583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4404 - total_train_reward: -960.3968\n",
      "Epoch 5584/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3185 - total_train_reward: -1324.7810\n",
      "Epoch 5585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3912 - total_train_reward: -1072.8801\n",
      "Epoch 5586/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7790 - total_train_reward: -1314.5139\n",
      "Epoch 5587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1664 - total_train_reward: -1216.1680\n",
      "Epoch 5588/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2548 - total_train_reward: -1221.9715\n",
      "Epoch 5589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7443 - total_train_reward: -1209.3367\n",
      "Epoch 5590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3765 - total_train_reward: -1658.7778\n",
      "Epoch 5591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5443 - total_train_reward: -1147.7061\n",
      "Epoch 5592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4550 - total_train_reward: -1406.1699\n",
      "Epoch 5593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6443 - total_train_reward: -1212.4567\n",
      "Epoch 5594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1909 - total_train_reward: -1168.4183\n",
      "Epoch 5595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8515 - total_train_reward: -1794.5827\n",
      "Epoch 5596/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5367 - total_train_reward: -1317.7326\n",
      "Epoch 5597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8093 - total_train_reward: -1747.5604\n",
      "Epoch 5598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1823 - total_train_reward: -1682.6932\n",
      "Epoch 5599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4748 - total_train_reward: -1200.8071\n",
      "Epoch 5600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3623 - total_train_reward: -1231.7562\n",
      "Epoch 5601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6885 - total_train_reward: -1211.8148\n",
      "Epoch 5602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0187 - total_train_reward: -1202.2122\n",
      "Epoch 5603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3525 - total_train_reward: -1525.0648\n",
      "Epoch 5604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9389 - total_train_reward: -1832.4687\n",
      "Epoch 5605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5193 - total_train_reward: -777.0545\n",
      "Epoch 5606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9795 - total_train_reward: -1072.0937\n",
      "Epoch 5607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7046 - total_train_reward: -1082.1625\n",
      "Epoch 5608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9517 - total_train_reward: -1143.7629\n",
      "Epoch 5609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0112 - total_train_reward: -1583.4109\n",
      "Epoch 5610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5266 - total_train_reward: -1760.2416\n",
      "Epoch 5611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6467 - total_train_reward: -1065.9747\n",
      "Epoch 5612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1624 - total_train_reward: -1068.4291\n",
      "Epoch 5613/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2709 - total_train_reward: -1218.4020\n",
      "Epoch 5614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.8660 - total_train_reward: -1653.5483\n",
      "Epoch 5615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6202 - total_train_reward: -1064.7891\n",
      "Epoch 5616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6318 - total_train_reward: -1272.7313\n",
      "Epoch 5617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0055 - total_train_reward: -1150.8564\n",
      "Epoch 5618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0232 - total_train_reward: -1345.0065\n",
      "Epoch 5619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9300 - total_train_reward: -1069.0974\n",
      "Epoch 5620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9290 - total_train_reward: -798.0510\n",
      "Epoch 5621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8703 - total_train_reward: -1672.0333\n",
      "Epoch 5622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4780 - total_train_reward: -1481.9772\n",
      "Epoch 5623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1123 - total_train_reward: -1432.8625\n",
      "Epoch 5624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3590 - total_train_reward: -948.5626\n",
      "Epoch 5625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7614 - total_train_reward: -745.2395\n",
      "Epoch 5626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4157 - total_train_reward: -1281.7975\n",
      "Epoch 5627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3073 - total_train_reward: -1415.4316\n",
      "Epoch 5628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0477 - total_train_reward: -1155.3888\n",
      "Epoch 5629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7503 - total_train_reward: -1828.6254\n",
      "Epoch 5630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8690 - total_train_reward: -1568.5281\n",
      "Epoch 5631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3375 - total_train_reward: -1400.6247\n",
      "Epoch 5632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1341 - total_train_reward: -957.4500\n",
      "Epoch 5633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7780 - total_train_reward: -1350.5104\n",
      "Epoch 5634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6445 - total_train_reward: -1504.3578\n",
      "Epoch 5635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4976 - total_train_reward: -1210.3789\n",
      "Epoch 5636/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6042 - total_train_reward: -925.5997\n",
      "Epoch 5637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5263 - total_train_reward: -1530.5595\n",
      "Epoch 5638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 17.0047 - total_train_reward: -890.8927\n",
      "Epoch 5639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 56.8726 - total_train_reward: -1196.3452\n",
      "Epoch 5640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4544 - total_train_reward: -1770.6690\n",
      "Epoch 5641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8709 - total_train_reward: -1289.9425\n",
      "Epoch 5642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8961 - total_train_reward: -1528.7636\n",
      "Epoch 5643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3884 - total_train_reward: -1243.1418\n",
      "Epoch 5644/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2846 - total_train_reward: -1626.4503\n",
      "Epoch 5645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0014 - total_train_reward: -1152.0954\n",
      "Epoch 5646/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9687 - total_train_reward: -1462.0779\n",
      "Epoch 5647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8226 - total_train_reward: -1184.1929\n",
      "Epoch 5648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8995 - total_train_reward: -1629.7622\n",
      "Epoch 5649/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0608 - total_train_reward: -1054.0100\n",
      "Epoch 5650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4627 - total_train_reward: -1439.5579\n",
      "Epoch 5651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9830 - total_train_reward: -1282.9567\n",
      "Epoch 5652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8877 - total_train_reward: -841.9549\n",
      "Epoch 5653/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.0324 - total_train_reward: -727.3545\n",
      "Epoch 5654/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.1163 - total_train_reward: -1176.2104\n",
      "Epoch 5655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.9786 - total_train_reward: -1778.7909\n",
      "Epoch 5656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7272 - total_train_reward: -1565.1083\n",
      "Epoch 5657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0676 - total_train_reward: -1371.6887\n",
      "Epoch 5658/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2047 - total_train_reward: -1811.9951\n",
      "Epoch 5659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2926 - total_train_reward: -1351.0499\n",
      "Epoch 5660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2812 - total_train_reward: -1664.1890\n",
      "Epoch 5661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6923 - total_train_reward: -1067.6659\n",
      "Epoch 5662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8531 - total_train_reward: -1435.3215\n",
      "Epoch 5663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7728 - total_train_reward: -1236.9623\n",
      "Epoch 5664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1212 - total_train_reward: -1289.1265\n",
      "Epoch 5665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8577 - total_train_reward: -1366.3617\n",
      "Epoch 5666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3459 - total_train_reward: -1786.0405\n",
      "Epoch 5667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5159 - total_train_reward: -1170.5099\n",
      "Epoch 5668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1382 - total_train_reward: -1485.4422\n",
      "Epoch 5669/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8167 - total_train_reward: -1168.0324\n",
      "Epoch 5670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6331 - total_train_reward: -1788.8688\n",
      "Epoch 5671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.8601 - total_train_reward: -1558.8386\n",
      "Epoch 5672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6211 - total_train_reward: -749.8138\n",
      "Epoch 5673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7198 - total_train_reward: -1415.7110\n",
      "Epoch 5674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1469 - total_train_reward: -1520.5802\n",
      "Epoch 5675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6828 - total_train_reward: -1312.1298\n",
      "Epoch 5676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1672 - total_train_reward: -1764.3055\n",
      "Epoch 5677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7659 - total_train_reward: -1205.9391\n",
      "Epoch 5678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6158 - total_train_reward: -1106.8080\n",
      "Epoch 5679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3945 - total_train_reward: -1562.5014\n",
      "Epoch 5680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3694 - total_train_reward: -1201.0311\n",
      "Epoch 5681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7337 - total_train_reward: -1040.4206\n",
      "Epoch 5682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2032 - total_train_reward: -1200.3170\n",
      "Epoch 5683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3606 - total_train_reward: -1664.8008\n",
      "Epoch 5684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7123 - total_train_reward: -1445.8038\n",
      "Epoch 5685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9768 - total_train_reward: -1038.4202\n",
      "Epoch 5686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2579 - total_train_reward: -1205.6692\n",
      "Epoch 5687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9192 - total_train_reward: -1290.2041\n",
      "Epoch 5688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6749 - total_train_reward: -960.9400\n",
      "Epoch 5689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7357 - total_train_reward: -1163.1496\n",
      "Epoch 5690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4573 - total_train_reward: -1251.7642\n",
      "Epoch 5691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6711 - total_train_reward: -1182.2615\n",
      "Epoch 5692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5886 - total_train_reward: -1643.4175\n",
      "Epoch 5693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9421 - total_train_reward: -1069.0481\n",
      "Epoch 5694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8036 - total_train_reward: -1266.7090\n",
      "Epoch 5695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3668 - total_train_reward: -1054.6211\n",
      "Epoch 5696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3802 - total_train_reward: -1089.1123\n",
      "Epoch 5697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4420 - total_train_reward: -1275.0151\n",
      "Epoch 5698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8312 - total_train_reward: -1283.2071\n",
      "Epoch 5699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0603 - total_train_reward: -1669.8371\n",
      "Epoch 5700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8461 - total_train_reward: -1157.3975\n",
      "Epoch 5701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3764 - total_train_reward: -1086.7162\n",
      "Epoch 5702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4922 - total_train_reward: -1195.5188\n",
      "Epoch 5703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5587 - total_train_reward: -1187.1165\n",
      "Epoch 5704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8608 - total_train_reward: -1085.8091\n",
      "Epoch 5705/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4586 - total_train_reward: -1449.4166\n",
      "Epoch 5706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0496 - total_train_reward: -1055.0564\n",
      "Epoch 5707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8315 - total_train_reward: -1035.6353\n",
      "Epoch 5708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8892 - total_train_reward: -1749.1652\n",
      "Epoch 5709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0928 - total_train_reward: -1838.3648\n",
      "Epoch 5710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1396 - total_train_reward: -1177.2049\n",
      "Epoch 5711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1801 - total_train_reward: -1422.7016\n",
      "Epoch 5712/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1477 - total_train_reward: -1355.5728\n",
      "Epoch 5713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.4773 - total_train_reward: -1185.7108\n",
      "Epoch 5714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3606 - total_train_reward: -1133.6848\n",
      "Epoch 5715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9783 - total_train_reward: -1724.0960\n",
      "Epoch 5716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1502 - total_train_reward: -1491.1970\n",
      "Epoch 5717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1034 - total_train_reward: -1419.5701\n",
      "Epoch 5718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4856 - total_train_reward: -1196.4585\n",
      "Epoch 5719/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8261 - total_train_reward: -1444.3541\n",
      "Epoch 5720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2551 - total_train_reward: -1193.5284\n",
      "Epoch 5721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6007 - total_train_reward: -1354.9934\n",
      "Epoch 5722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3265 - total_train_reward: -1450.6062\n",
      "Epoch 5723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0096 - total_train_reward: -1168.4805\n",
      "Epoch 5724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2129 - total_train_reward: -1318.7016\n",
      "Epoch 5725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7969 - total_train_reward: -850.7552\n",
      "Epoch 5726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.6514 - total_train_reward: -1489.3598\n",
      "Epoch 5727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5031 - total_train_reward: -1798.3362\n",
      "Epoch 5728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0823 - total_train_reward: -1372.5542\n",
      "Epoch 5729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.1686 - total_train_reward: -1686.0804\n",
      "Epoch 5730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2574 - total_train_reward: -1063.8905\n",
      "Epoch 5731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4356 - total_train_reward: -925.5232\n",
      "Epoch 5732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3801 - total_train_reward: -997.6415\n",
      "Epoch 5733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9990 - total_train_reward: -1245.7181\n",
      "Epoch 5734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7906 - total_train_reward: -1189.7674\n",
      "Epoch 5735/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.5085 - total_train_reward: -621.2398\n",
      "Epoch 5736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.0647 - total_train_reward: -1075.8723\n",
      "Epoch 5737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5898 - total_train_reward: -1731.4563\n",
      "Epoch 5738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0960 - total_train_reward: -1855.5685\n",
      "Epoch 5739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5472 - total_train_reward: -1834.4673\n",
      "Epoch 5740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3633 - total_train_reward: -1320.0810\n",
      "Epoch 5741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.7000 - total_train_reward: -1199.6002\n",
      "Epoch 5742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -17.8813 - total_train_reward: -1763.3860\n",
      "Epoch 5743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9435 - total_train_reward: -1158.4828\n",
      "Epoch 5744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2889 - total_train_reward: -1859.0280\n",
      "Epoch 5745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2395 - total_train_reward: -1364.7958\n",
      "Epoch 5746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9325 - total_train_reward: -1194.2524\n",
      "Epoch 5747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2531 - total_train_reward: -1839.4498\n",
      "Epoch 5748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2263 - total_train_reward: -1523.9256\n",
      "Epoch 5749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3670 - total_train_reward: -732.7119\n",
      "Epoch 5750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7409 - total_train_reward: -1298.0649\n",
      "Epoch 5751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1990 - total_train_reward: -1196.3083\n",
      "Epoch 5752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8012 - total_train_reward: -1199.0392\n",
      "Epoch 5753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1245 - total_train_reward: -1163.0534\n",
      "Epoch 5754/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4962 - total_train_reward: -1210.9613\n",
      "Epoch 5755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2375 - total_train_reward: -1076.7429\n",
      "Epoch 5756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8256 - total_train_reward: -1733.9912\n",
      "Epoch 5757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5642 - total_train_reward: -1206.1446\n",
      "Epoch 5758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3066 - total_train_reward: -1162.9493\n",
      "Epoch 5759/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.7328 - total_train_reward: -1198.8797\n",
      "Epoch 5760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4229 - total_train_reward: -1686.1109\n",
      "Epoch 5761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1612 - total_train_reward: -752.8845\n",
      "Epoch 5762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7850 - total_train_reward: -1005.5622\n",
      "Epoch 5763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.5196 - total_train_reward: -1441.1758\n",
      "Epoch 5764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3166 - total_train_reward: -1182.9983\n",
      "Epoch 5765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0258 - total_train_reward: -1788.6774\n",
      "Epoch 5766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5883 - total_train_reward: -1274.4246\n",
      "Epoch 5767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5102 - total_train_reward: -1292.6336\n",
      "Epoch 5768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4269 - total_train_reward: -1173.0900\n",
      "Epoch 5769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3983 - total_train_reward: -1202.4653\n",
      "Epoch 5770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8049 - total_train_reward: -967.0440\n",
      "Epoch 5771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9005 - total_train_reward: -1594.5201\n",
      "Epoch 5772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3828 - total_train_reward: -1200.0796\n",
      "Epoch 5773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8933 - total_train_reward: -1604.8615\n",
      "Epoch 5774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4236 - total_train_reward: -1833.7489\n",
      "Epoch 5775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7199 - total_train_reward: -1200.0068\n",
      "Epoch 5776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2392 - total_train_reward: -629.1425\n",
      "Epoch 5777/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 21.4834 - total_train_reward: -1045.8974\n",
      "Epoch 5778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0565 - total_train_reward: -1060.7538\n",
      "Epoch 5779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9098 - total_train_reward: -1812.9835\n",
      "Epoch 5780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4914 - total_train_reward: -1503.3301\n",
      "Epoch 5781/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9517 - total_train_reward: -1159.0457\n",
      "Epoch 5782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8787 - total_train_reward: -1390.3918\n",
      "Epoch 5783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2957 - total_train_reward: -1475.9251\n",
      "Epoch 5784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2742 - total_train_reward: -1654.0521\n",
      "Epoch 5785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6218 - total_train_reward: -728.6343\n",
      "Epoch 5786/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5865 - total_train_reward: -1199.9226\n",
      "Epoch 5787/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7277 - total_train_reward: -736.7056\n",
      "Epoch 5788/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4755 - total_train_reward: -1117.8820\n",
      "Epoch 5789/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3079 - total_train_reward: -1201.6643\n",
      "Epoch 5790/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3973 - total_train_reward: -1389.3839\n",
      "Epoch 5791/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1465 - total_train_reward: -1068.2615\n",
      "Epoch 5792/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9432 - total_train_reward: -1074.7149\n",
      "Epoch 5793/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8100 - total_train_reward: -1545.3787\n",
      "Epoch 5794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1715 - total_train_reward: -1266.7417\n",
      "Epoch 5795/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5525 - total_train_reward: -1124.4653\n",
      "Epoch 5796/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3429 - total_train_reward: -866.4531\n",
      "Epoch 5797/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1987 - total_train_reward: -1089.5220\n",
      "Epoch 5798/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7199 - total_train_reward: -1031.3515\n",
      "Epoch 5799/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5384 - total_train_reward: -1382.8480\n",
      "Epoch 5800/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1594 - total_train_reward: -728.1430\n",
      "Epoch 5801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2612 - total_train_reward: -730.2099\n",
      "Epoch 5802/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8361 - total_train_reward: -1846.7239\n",
      "Epoch 5803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9438 - total_train_reward: -1201.7822\n",
      "Epoch 5804/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5853 - total_train_reward: -1234.3338\n",
      "Epoch 5805/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8090 - total_train_reward: -1199.5445\n",
      "Epoch 5806/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6904 - total_train_reward: -938.0660\n",
      "Epoch 5807/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7546 - total_train_reward: -1203.9518\n",
      "Epoch 5808/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2704 - total_train_reward: -1071.1315\n",
      "Epoch 5809/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2109 - total_train_reward: -1067.1310\n",
      "Epoch 5810/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7860 - total_train_reward: -1203.7171\n",
      "Epoch 5811/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7720 - total_train_reward: -1267.4262\n",
      "Epoch 5812/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3103 - total_train_reward: -1170.2255\n",
      "Epoch 5813/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8817 - total_train_reward: -1198.3045\n",
      "Epoch 5814/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6758 - total_train_reward: -1192.7500\n",
      "Epoch 5815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.0129 - total_train_reward: -1816.6369\n",
      "Epoch 5816/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3534 - total_train_reward: -1399.7129\n",
      "Epoch 5817/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0521 - total_train_reward: -1318.0797\n",
      "Epoch 5818/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2518 - total_train_reward: -1792.9932\n",
      "Epoch 5819/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8394 - total_train_reward: -1093.4558\n",
      "Epoch 5820/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5129 - total_train_reward: -1016.9718\n",
      "Epoch 5821/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1164 - total_train_reward: -1708.1599\n",
      "Epoch 5822/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5033 - total_train_reward: -1204.9590\n",
      "Epoch 5823/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.2624 - total_train_reward: -1590.9532\n",
      "Epoch 5824/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7983 - total_train_reward: -1204.5985\n",
      "Epoch 5825/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9572 - total_train_reward: -1541.4219\n",
      "Epoch 5826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0819 - total_train_reward: -1204.5333\n",
      "Epoch 5827/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4560 - total_train_reward: -1008.9139\n",
      "Epoch 5828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4153 - total_train_reward: -1300.0793\n",
      "Epoch 5829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5069 - total_train_reward: -1202.9762\n",
      "Epoch 5830/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7973 - total_train_reward: -1518.6513\n",
      "Epoch 5831/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8971 - total_train_reward: -1493.0656\n",
      "Epoch 5832/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9497 - total_train_reward: -1253.2878\n",
      "Epoch 5833/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0270 - total_train_reward: -1311.2311\n",
      "Epoch 5834/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4617 - total_train_reward: -1282.1701\n",
      "Epoch 5835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8683 - total_train_reward: -1387.7991\n",
      "Epoch 5836/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8106 - total_train_reward: -1497.3195\n",
      "Epoch 5837/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9823 - total_train_reward: -1174.2260\n",
      "Epoch 5838/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1738 - total_train_reward: -1328.4533\n",
      "Epoch 5839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1050 - total_train_reward: -1066.7768\n",
      "Epoch 5840/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5318 - total_train_reward: -1838.4234\n",
      "Epoch 5841/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3086 - total_train_reward: -1423.5372\n",
      "Epoch 5842/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1618 - total_train_reward: -1593.1615\n",
      "Epoch 5843/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4669 - total_train_reward: -1169.6571\n",
      "Epoch 5844/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4135 - total_train_reward: -1166.2826\n",
      "Epoch 5845/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7315 - total_train_reward: -727.3009\n",
      "Epoch 5846/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0183 - total_train_reward: -1715.5118\n",
      "Epoch 5847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5297 - total_train_reward: -847.5984\n",
      "Epoch 5848/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7563 - total_train_reward: -1325.4488\n",
      "Epoch 5849/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8479 - total_train_reward: -1352.4830\n",
      "Epoch 5850/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1148 - total_train_reward: -1166.2273\n",
      "Epoch 5851/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.2714 - total_train_reward: -1209.2541\n",
      "Epoch 5852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1762 - total_train_reward: -1835.5704\n",
      "Epoch 5853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6231 - total_train_reward: -1243.1472\n",
      "Epoch 5854/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3197 - total_train_reward: -1768.9237\n",
      "Epoch 5855/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3416 - total_train_reward: -1838.8221\n",
      "Epoch 5856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1582 - total_train_reward: -1303.8310\n",
      "Epoch 5857/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4477 - total_train_reward: -1228.0007\n",
      "Epoch 5858/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9815 - total_train_reward: -943.6853\n",
      "Epoch 5859/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5010 - total_train_reward: -959.6112\n",
      "Epoch 5860/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0946 - total_train_reward: -1107.3101\n",
      "Epoch 5861/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7905 - total_train_reward: -1474.7625\n",
      "Epoch 5862/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2369 - total_train_reward: -1450.1385\n",
      "Epoch 5863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7269 - total_train_reward: -1249.6586\n",
      "Epoch 5864/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5535 - total_train_reward: -1055.8140\n",
      "Epoch 5865/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9787 - total_train_reward: -1262.3362\n",
      "Epoch 5866/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8275 - total_train_reward: -1253.3934\n",
      "Epoch 5867/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2371 - total_train_reward: -955.2502\n",
      "Epoch 5868/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8616 - total_train_reward: -1578.5638\n",
      "Epoch 5869/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6706 - total_train_reward: -1135.1927\n",
      "Epoch 5870/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1583 - total_train_reward: -1255.5209\n",
      "Epoch 5871/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7487 - total_train_reward: -1563.2195\n",
      "Epoch 5872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1185 - total_train_reward: -1316.4275\n",
      "Epoch 5873/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2421 - total_train_reward: -1785.0885\n",
      "Epoch 5874/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0820 - total_train_reward: -1354.2089\n",
      "Epoch 5875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4236 - total_train_reward: -1646.3789\n",
      "Epoch 5876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0469 - total_train_reward: -887.2081\n",
      "Epoch 5877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.9231 - total_train_reward: -1207.6871\n",
      "Epoch 5878/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8807 - total_train_reward: -1083.1714\n",
      "Epoch 5879/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1270 - total_train_reward: -1188.1274\n",
      "Epoch 5880/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4177 - total_train_reward: -1765.6388\n",
      "Epoch 5881/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1646 - total_train_reward: -1729.2348\n",
      "Epoch 5882/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9113 - total_train_reward: -1460.0353\n",
      "Epoch 5883/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6370 - total_train_reward: -1203.4532\n",
      "Epoch 5884/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0036 - total_train_reward: -959.4888\n",
      "Epoch 5885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9154 - total_train_reward: -1064.6321\n",
      "Epoch 5886/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3324 - total_train_reward: -1366.1541\n",
      "Epoch 5887/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7535 - total_train_reward: -1621.6348\n",
      "Epoch 5888/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2841 - total_train_reward: -1208.0780\n",
      "Epoch 5889/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8504 - total_train_reward: -995.4326\n",
      "Epoch 5890/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3104 - total_train_reward: -1147.5379\n",
      "Epoch 5891/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9780 - total_train_reward: -850.2233\n",
      "Epoch 5892/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4953 - total_train_reward: -1834.3383\n",
      "Epoch 5893/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2113 - total_train_reward: -1209.0328\n",
      "Epoch 5894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4976 - total_train_reward: -1217.5505\n",
      "Epoch 5895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2275 - total_train_reward: -726.8713\n",
      "Epoch 5896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0283 - total_train_reward: -1143.3971\n",
      "Epoch 5897/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6056 - total_train_reward: -923.9384\n",
      "Epoch 5898/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.7335 - total_train_reward: -1209.9302\n",
      "Epoch 5899/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9790 - total_train_reward: -1203.8011\n",
      "Epoch 5900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0128 - total_train_reward: -1167.0212\n",
      "Epoch 5901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4703 - total_train_reward: -727.2283\n",
      "Epoch 5902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.5189 - total_train_reward: -1215.4708\n",
      "Epoch 5903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6420 - total_train_reward: -1213.5208\n",
      "Epoch 5904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9933 - total_train_reward: -1534.9502\n",
      "Epoch 5905/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4525 - total_train_reward: -1200.0414\n",
      "Epoch 5906/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5916 - total_train_reward: -1064.6479\n",
      "Epoch 5907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2607 - total_train_reward: -1094.1339\n",
      "Epoch 5908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6661 - total_train_reward: -1222.0689\n",
      "Epoch 5909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6442 - total_train_reward: -1663.9987\n",
      "Epoch 5910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6485 - total_train_reward: -1399.1672\n",
      "Epoch 5911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6937 - total_train_reward: -1260.8725\n",
      "Epoch 5912/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7649 - total_train_reward: -1026.0932\n",
      "Epoch 5913/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7256 - total_train_reward: -1331.2263\n",
      "Epoch 5914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3338 - total_train_reward: -1723.7172\n",
      "Epoch 5915/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6977 - total_train_reward: -844.5647\n",
      "Epoch 5916/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4715 - total_train_reward: -1609.4456\n",
      "Epoch 5917/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5975 - total_train_reward: -1815.3302\n",
      "Epoch 5918/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5210 - total_train_reward: -942.4120\n",
      "Epoch 5919/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1314 - total_train_reward: -941.7407\n",
      "Epoch 5920/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1531 - total_train_reward: -1207.0816\n",
      "Epoch 5921/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7736 - total_train_reward: -1374.1577\n",
      "Epoch 5922/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1484 - total_train_reward: -855.6540\n",
      "Epoch 5923/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9595 - total_train_reward: -1208.9039\n",
      "Epoch 5924/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1012 - total_train_reward: -1204.3280\n",
      "Epoch 5925/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3382 - total_train_reward: -958.7325\n",
      "Epoch 5926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1624 - total_train_reward: -919.2251\n",
      "Epoch 5927/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4174 - total_train_reward: -1461.2963\n",
      "Epoch 5928/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2643 - total_train_reward: -1196.2357\n",
      "Epoch 5929/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2439 - total_train_reward: -1636.1118\n",
      "Epoch 5930/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9104 - total_train_reward: -1207.6852\n",
      "Epoch 5931/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3003 - total_train_reward: -1790.5395\n",
      "Epoch 5932/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2727 - total_train_reward: -1132.8042\n",
      "Epoch 5933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9348 - total_train_reward: -1760.0137\n",
      "Epoch 5934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8645 - total_train_reward: -842.6926\n",
      "Epoch 5935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4132 - total_train_reward: -1282.1541\n",
      "Epoch 5936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9824 - total_train_reward: -1054.3990\n",
      "Epoch 5937/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.6829 - total_train_reward: -1714.3715\n",
      "Epoch 5938/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0053 - total_train_reward: -1207.1389\n",
      "Epoch 5939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6512 - total_train_reward: -970.3193\n",
      "Epoch 5940/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4076 - total_train_reward: -1753.3764\n",
      "Epoch 5941/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4787 - total_train_reward: -960.3561\n",
      "Epoch 5942/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4897 - total_train_reward: -1788.5638\n",
      "Epoch 5943/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3355 - total_train_reward: -1170.3917\n",
      "Epoch 5944/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9700 - total_train_reward: -1166.5794\n",
      "Epoch 5945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8398 - total_train_reward: -1231.7814\n",
      "Epoch 5946/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5902 - total_train_reward: -1471.9419\n",
      "Epoch 5947/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2171 - total_train_reward: -1218.6274\n",
      "Epoch 5948/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8256 - total_train_reward: -1068.8803\n",
      "Epoch 5949/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2027 - total_train_reward: -1207.6431\n",
      "Epoch 5950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4079 - total_train_reward: -1063.7673\n",
      "Epoch 5951/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.6593 - total_train_reward: -1818.9918\n",
      "Epoch 5952/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1121 - total_train_reward: -1214.5705\n",
      "Epoch 5953/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7369 - total_train_reward: -927.4035\n",
      "Epoch 5954/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7260 - total_train_reward: -1838.3576\n",
      "Epoch 5955/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7543 - total_train_reward: -1317.3420\n",
      "Epoch 5956/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2172 - total_train_reward: -1593.8702\n",
      "Epoch 5957/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0572 - total_train_reward: -1219.2381\n",
      "Epoch 5958/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8221 - total_train_reward: -848.4835\n",
      "Epoch 5959/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2328 - total_train_reward: -1557.8244\n",
      "Epoch 5960/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1505 - total_train_reward: -1211.7882\n",
      "Epoch 5961/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9700 - total_train_reward: -1583.6644\n",
      "Epoch 5962/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0080 - total_train_reward: -1495.4300\n",
      "Epoch 5963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8915 - total_train_reward: -1196.6409\n",
      "Epoch 5964/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7592 - total_train_reward: -1758.8017\n",
      "Epoch 5965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8713 - total_train_reward: -1719.0586\n",
      "Epoch 5966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1695 - total_train_reward: -1152.3299\n",
      "Epoch 5967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8497 - total_train_reward: -1175.6910\n",
      "Epoch 5968/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7485 - total_train_reward: -1214.6113\n",
      "Epoch 5969/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6506 - total_train_reward: -1181.4001\n",
      "Epoch 5970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2084 - total_train_reward: -1125.9276\n",
      "Epoch 5971/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7683 - total_train_reward: -1067.1006\n",
      "Epoch 5972/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7245 - total_train_reward: -1573.5636\n",
      "Epoch 5973/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0637 - total_train_reward: -1566.7403\n",
      "Epoch 5974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5972 - total_train_reward: -1197.6503\n",
      "Epoch 5975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7542 - total_train_reward: -1207.2640\n",
      "Epoch 5976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6237 - total_train_reward: -982.7506\n",
      "Epoch 5977/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9155 - total_train_reward: -1393.3321\n",
      "Epoch 5978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2627 - total_train_reward: -1222.3552\n",
      "Epoch 5979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8943 - total_train_reward: -1170.5853\n",
      "Epoch 5980/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.6911 - total_train_reward: -1193.8781\n",
      "Epoch 5981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4047 - total_train_reward: -1109.9354\n",
      "Epoch 5982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7745 - total_train_reward: -1778.7904\n",
      "Epoch 5983/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3153 - total_train_reward: -966.8993\n",
      "Epoch 5984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3018 - total_train_reward: -1479.4968\n",
      "Epoch 5985/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8674 - total_train_reward: -1209.6594\n",
      "Epoch 5986/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1823 - total_train_reward: -1211.8476\n",
      "Epoch 5987/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8251 - total_train_reward: -1776.7234\n",
      "Epoch 5988/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3507 - total_train_reward: -1481.0404\n",
      "Epoch 5989/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8437 - total_train_reward: -1405.3716\n",
      "Epoch 5990/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6165 - total_train_reward: -1209.2152\n",
      "Epoch 5991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1000 - total_train_reward: -1518.7074\n",
      "Epoch 5992/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8361 - total_train_reward: -1256.4677\n",
      "Epoch 5993/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4349 - total_train_reward: -1202.3138\n",
      "Epoch 5994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0965 - total_train_reward: -1210.0976\n",
      "Epoch 5995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.5986 - total_train_reward: -1593.5679\n",
      "Epoch 5996/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6196 - total_train_reward: -1247.9377\n",
      "Epoch 5997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4788 - total_train_reward: -1204.3951\n",
      "Epoch 5998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3318 - total_train_reward: -1218.9167\n",
      "Epoch 5999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2366 - total_train_reward: -1310.2559\n",
      "Epoch 6000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8462 - total_train_reward: -1788.2456\n",
      "Epoch 6001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4009 - total_train_reward: -1202.4379\n",
      "Epoch 6002/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9370 - total_train_reward: -1205.0415\n",
      "Epoch 6003/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3619 - total_train_reward: -1731.5229\n",
      "Epoch 6004/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3075 - total_train_reward: -1213.5747\n",
      "Epoch 6005/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4220 - total_train_reward: -957.6386\n",
      "Epoch 6006/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7502 - total_train_reward: -1481.6241\n",
      "Epoch 6007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1803 - total_train_reward: -974.3636\n",
      "Epoch 6008/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3264 - total_train_reward: -1049.3497\n",
      "Epoch 6009/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0042 - total_train_reward: -895.1880\n",
      "Epoch 6010/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2527 - total_train_reward: -1083.1491\n",
      "Epoch 6011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4907 - total_train_reward: -1822.1083\n",
      "Epoch 6012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5430 - total_train_reward: -1070.0818\n",
      "Epoch 6013/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4504 - total_train_reward: -1173.4833\n",
      "Epoch 6014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9109 - total_train_reward: -1524.5260\n",
      "Epoch 6015/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1546 - total_train_reward: -1207.1635\n",
      "Epoch 6016/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.4075 - total_train_reward: -1760.5324\n",
      "Epoch 6017/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4488 - total_train_reward: -1210.9273\n",
      "Epoch 6018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6569 - total_train_reward: -1572.8410\n",
      "Epoch 6019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2807 - total_train_reward: -1204.9067\n",
      "Epoch 6020/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7364 - total_train_reward: -1684.8966\n",
      "Epoch 6021/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9148 - total_train_reward: -1320.8474\n",
      "Epoch 6022/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7464 - total_train_reward: -1212.8710\n",
      "Epoch 6023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7966 - total_train_reward: -1196.5206\n",
      "Epoch 6024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6210 - total_train_reward: -1166.3964\n",
      "Epoch 6025/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1525 - total_train_reward: -1549.5333\n",
      "Epoch 6026/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6201 - total_train_reward: -1073.9888\n",
      "Epoch 6027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2578 - total_train_reward: -1044.9753\n",
      "Epoch 6028/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2105 - total_train_reward: -1185.2846\n",
      "Epoch 6029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2878 - total_train_reward: -1202.9091\n",
      "Epoch 6030/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6586 - total_train_reward: -1116.5059\n",
      "Epoch 6031/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1943 - total_train_reward: -973.3948\n",
      "Epoch 6032/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9546 - total_train_reward: -1377.9443\n",
      "Epoch 6033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0402 - total_train_reward: -967.6000\n",
      "Epoch 6034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5968 - total_train_reward: -1472.7445\n",
      "Epoch 6035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9669 - total_train_reward: -1621.2118\n",
      "Epoch 6036/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0056 - total_train_reward: -1378.4087\n",
      "Epoch 6037/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4778 - total_train_reward: -1560.7328\n",
      "Epoch 6038/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7294 - total_train_reward: -1046.1191\n",
      "Epoch 6039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4550 - total_train_reward: -1297.4161\n",
      "Epoch 6040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3613 - total_train_reward: -1080.9669\n",
      "Epoch 6041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8087 - total_train_reward: -1130.6586\n",
      "Epoch 6042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8146 - total_train_reward: -1063.7048\n",
      "Epoch 6043/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9481 - total_train_reward: -1207.2385\n",
      "Epoch 6044/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8494 - total_train_reward: -1260.4376\n",
      "Epoch 6045/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0081 - total_train_reward: -1216.9732\n",
      "Epoch 6046/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.1685 - total_train_reward: -1762.0373\n",
      "Epoch 6047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3210 - total_train_reward: -1212.5684\n",
      "Epoch 6048/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3282 - total_train_reward: -1258.7041\n",
      "Epoch 6049/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6640 - total_train_reward: -1694.3492\n",
      "Epoch 6050/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1710 - total_train_reward: -1427.1216\n",
      "Epoch 6051/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9080 - total_train_reward: -806.0852\n",
      "Epoch 6052/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3800 - total_train_reward: -1767.7214\n",
      "Epoch 6053/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3203 - total_train_reward: -1410.4529\n",
      "Epoch 6054/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5445 - total_train_reward: -1697.0502\n",
      "Epoch 6055/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3433 - total_train_reward: -1650.3911\n",
      "Epoch 6056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5426 - total_train_reward: -1331.3889\n",
      "Epoch 6057/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0108 - total_train_reward: -1697.4089\n",
      "Epoch 6058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9905 - total_train_reward: -1754.9746\n",
      "Epoch 6059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0082 - total_train_reward: -1346.0546\n",
      "Epoch 6060/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7716 - total_train_reward: -958.2058\n",
      "Epoch 6061/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0571 - total_train_reward: -1452.6660\n",
      "Epoch 6062/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3574 - total_train_reward: -1503.3417\n",
      "Epoch 6063/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6320 - total_train_reward: -1702.2718\n",
      "Epoch 6064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5387 - total_train_reward: -1504.7091\n",
      "Epoch 6065/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0312 - total_train_reward: -1740.3888\n",
      "Epoch 6066/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9213 - total_train_reward: -1787.0935\n",
      "Epoch 6067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9168 - total_train_reward: -1166.6443\n",
      "Epoch 6068/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6887 - total_train_reward: -1166.7834\n",
      "Epoch 6069/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.5585 - total_train_reward: -1166.0546\n",
      "Epoch 6070/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0272 - total_train_reward: -1704.6765\n",
      "Epoch 6071/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.1230 - total_train_reward: -1213.5672\n",
      "Epoch 6072/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2946 - total_train_reward: -1064.7236\n",
      "Epoch 6073/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6530 - total_train_reward: -1619.6571\n",
      "Epoch 6074/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0175 - total_train_reward: -1235.9653\n",
      "Epoch 6075/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9590 - total_train_reward: -1202.7754\n",
      "Epoch 6076/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1349 - total_train_reward: -1169.0545\n",
      "Epoch 6077/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3921 - total_train_reward: -1419.3801\n",
      "Epoch 6078/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9818 - total_train_reward: -1736.8688\n",
      "Epoch 6079/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4242 - total_train_reward: -1469.5670\n",
      "Epoch 6080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2962 - total_train_reward: -1164.8605\n",
      "Epoch 6081/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3130 - total_train_reward: -1507.8170\n",
      "Epoch 6082/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0291 - total_train_reward: -1427.3459\n",
      "Epoch 6083/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9846 - total_train_reward: -1533.9186\n",
      "Epoch 6084/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3883 - total_train_reward: -1040.8777\n",
      "Epoch 6085/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.6826 - total_train_reward: -1203.6772\n",
      "Epoch 6086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7155 - total_train_reward: -1165.9129\n",
      "Epoch 6087/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6753 - total_train_reward: -1204.6960\n",
      "Epoch 6088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4868 - total_train_reward: -1050.0904\n",
      "Epoch 6089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5902 - total_train_reward: -958.8199\n",
      "Epoch 6090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8935 - total_train_reward: -1467.5022\n",
      "Epoch 6091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9831 - total_train_reward: -1408.5617\n",
      "Epoch 6092/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7837 - total_train_reward: -1193.5466\n",
      "Epoch 6093/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2567 - total_train_reward: -1062.3163\n",
      "Epoch 6094/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4110 - total_train_reward: -1257.3205\n",
      "Epoch 6095/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7505 - total_train_reward: -1218.2163\n",
      "Epoch 6096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9281 - total_train_reward: -1282.8650\n",
      "Epoch 6097/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5294 - total_train_reward: -1167.0744\n",
      "Epoch 6098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9475 - total_train_reward: -1163.3122\n",
      "Epoch 6099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6602 - total_train_reward: -1261.1438\n",
      "Epoch 6100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9474 - total_train_reward: -1077.2023\n",
      "Epoch 6101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8375 - total_train_reward: -1543.5653\n",
      "Epoch 6102/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0063 - total_train_reward: -1047.3584\n",
      "Epoch 6103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2459 - total_train_reward: -1169.7264\n",
      "Epoch 6104/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7863 - total_train_reward: -1179.6141\n",
      "Epoch 6105/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8876 - total_train_reward: -1788.4962\n",
      "Epoch 6106/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5386 - total_train_reward: -1207.8210\n",
      "Epoch 6107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3621 - total_train_reward: -1502.6176\n",
      "Epoch 6108/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6522 - total_train_reward: -1317.0482\n",
      "Epoch 6109/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4491 - total_train_reward: -1086.7373\n",
      "Epoch 6110/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1740 - total_train_reward: -1188.2218\n",
      "Epoch 6111/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5444 - total_train_reward: -1265.5456\n",
      "Epoch 6112/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7021 - total_train_reward: -1208.6221\n",
      "Epoch 6113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1421 - total_train_reward: -1360.2455\n",
      "Epoch 6114/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8138 - total_train_reward: -1599.8010\n",
      "Epoch 6115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7875 - total_train_reward: -952.6622\n",
      "Epoch 6116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7271 - total_train_reward: -1207.1508\n",
      "Epoch 6117/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6896 - total_train_reward: -1553.5648\n",
      "Epoch 6118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8618 - total_train_reward: -1041.5877\n",
      "Epoch 6119/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1061 - total_train_reward: -940.7886\n",
      "Epoch 6120/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2966 - total_train_reward: -1208.3649\n",
      "Epoch 6121/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5891 - total_train_reward: -1501.0796\n",
      "Epoch 6122/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0747 - total_train_reward: -1191.2816\n",
      "Epoch 6123/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7632 - total_train_reward: -1453.5038\n",
      "Epoch 6124/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4006 - total_train_reward: -1333.3535\n",
      "Epoch 6125/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7946 - total_train_reward: -1203.7980\n",
      "Epoch 6126/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5111 - total_train_reward: -1360.8837\n",
      "Epoch 6127/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9272 - total_train_reward: -1427.5634\n",
      "Epoch 6128/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0408 - total_train_reward: -1235.0775\n",
      "Epoch 6129/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4715 - total_train_reward: -1205.9265\n",
      "Epoch 6130/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1005 - total_train_reward: -1397.1687\n",
      "Epoch 6131/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5907 - total_train_reward: -1165.6697\n",
      "Epoch 6132/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4296 - total_train_reward: -1277.1738\n",
      "Epoch 6133/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2588 - total_train_reward: -1581.8459\n",
      "Epoch 6134/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3473 - total_train_reward: -1294.6777\n",
      "Epoch 6135/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3895 - total_train_reward: -1063.1220\n",
      "Epoch 6136/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8151 - total_train_reward: -978.9100\n",
      "Epoch 6137/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3180 - total_train_reward: -960.5055\n",
      "Epoch 6138/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4761 - total_train_reward: -1633.0346\n",
      "Epoch 6139/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8962 - total_train_reward: -1201.9247\n",
      "Epoch 6140/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6172 - total_train_reward: -1203.3557\n",
      "Epoch 6141/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8166 - total_train_reward: -1298.1684\n",
      "Epoch 6142/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6686 - total_train_reward: -1525.8515\n",
      "Epoch 6143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9678 - total_train_reward: -853.9673\n",
      "Epoch 6144/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1748 - total_train_reward: -1745.6513\n",
      "Epoch 6145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4902 - total_train_reward: -1728.7722\n",
      "Epoch 6146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9071 - total_train_reward: -1308.4884\n",
      "Epoch 6147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5512 - total_train_reward: -1462.0214\n",
      "Epoch 6148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8923 - total_train_reward: -1074.6252\n",
      "Epoch 6149/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.6635 - total_train_reward: -1171.5793\n",
      "Epoch 6150/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6147 - total_train_reward: -1055.0190\n",
      "Epoch 6151/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2800 - total_train_reward: -882.6469\n",
      "Epoch 6152/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1796 - total_train_reward: -1589.5571\n",
      "Epoch 6153/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5663 - total_train_reward: -1154.5594\n",
      "Epoch 6154/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1395 - total_train_reward: -1252.3240\n",
      "Epoch 6155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4246 - total_train_reward: -1721.1463\n",
      "Epoch 6156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5422 - total_train_reward: -999.3006\n",
      "Epoch 6157/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7034 - total_train_reward: -1527.9607\n",
      "Epoch 6158/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0758 - total_train_reward: -1057.8202\n",
      "Epoch 6159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4729 - total_train_reward: -1357.3044\n",
      "Epoch 6160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3554 - total_train_reward: -1415.1721\n",
      "Epoch 6161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3857 - total_train_reward: -1261.8566\n",
      "Epoch 6162/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6793 - total_train_reward: -618.4151\n",
      "Epoch 6163/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4982 - total_train_reward: -1159.6317\n",
      "Epoch 6164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3469 - total_train_reward: -1126.7661\n",
      "Epoch 6165/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9103 - total_train_reward: -1051.8122\n",
      "Epoch 6166/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4898 - total_train_reward: -1240.2019\n",
      "Epoch 6167/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4716 - total_train_reward: -1851.5905\n",
      "Epoch 6168/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2378 - total_train_reward: -1200.9613\n",
      "Epoch 6169/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0328 - total_train_reward: -1239.8252\n",
      "Epoch 6170/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9034 - total_train_reward: -858.4755\n",
      "Epoch 6171/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2008 - total_train_reward: -1683.8522\n",
      "Epoch 6172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0656 - total_train_reward: -1522.5763\n",
      "Epoch 6173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.9767 - total_train_reward: -1200.9232\n",
      "Epoch 6174/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6679 - total_train_reward: -1282.5207\n",
      "Epoch 6175/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9729 - total_train_reward: -1058.8725\n",
      "Epoch 6176/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8019 - total_train_reward: -1645.3837\n",
      "Epoch 6177/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5078 - total_train_reward: -1663.2354\n",
      "Epoch 6178/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9037 - total_train_reward: -1197.4416\n",
      "Epoch 6179/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4413 - total_train_reward: -1678.4555\n",
      "Epoch 6180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4902 - total_train_reward: -1282.3539\n",
      "Epoch 6181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8203 - total_train_reward: -1614.6307\n",
      "Epoch 6182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2559 - total_train_reward: -1282.7073\n",
      "Epoch 6183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1116 - total_train_reward: -1666.3185\n",
      "Epoch 6184/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4409 - total_train_reward: -1783.5188\n",
      "Epoch 6185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5842 - total_train_reward: -1192.1156\n",
      "Epoch 6186/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4039 - total_train_reward: -938.5126\n",
      "Epoch 6187/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3352 - total_train_reward: -1398.7735\n",
      "Epoch 6188/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0619 - total_train_reward: -1446.8239\n",
      "Epoch 6189/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2566 - total_train_reward: -1058.7537\n",
      "Epoch 6190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6587 - total_train_reward: -1216.1713\n",
      "Epoch 6191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1925 - total_train_reward: -1593.2041\n",
      "Epoch 6192/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0961 - total_train_reward: -1444.2358\n",
      "Epoch 6193/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1859 - total_train_reward: -1280.9471\n",
      "Epoch 6194/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4123 - total_train_reward: -1194.7049\n",
      "Epoch 6195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9795 - total_train_reward: -1198.7764\n",
      "Epoch 6196/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.6727 - total_train_reward: -1701.3154\n",
      "Epoch 6197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5226 - total_train_reward: -1741.1880\n",
      "Epoch 6198/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5175 - total_train_reward: -1409.4996\n",
      "Epoch 6199/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0427 - total_train_reward: -976.3898\n",
      "Epoch 6200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1317 - total_train_reward: -1803.2699\n",
      "Epoch 6201/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0529 - total_train_reward: -1041.5471\n",
      "Epoch 6202/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3055 - total_train_reward: -1195.0291\n",
      "Epoch 6203/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5761 - total_train_reward: -1765.1763\n",
      "Epoch 6204/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6617 - total_train_reward: -961.6614\n",
      "Epoch 6205/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3227 - total_train_reward: -1288.0831\n",
      "Epoch 6206/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9396 - total_train_reward: -953.4961\n",
      "Epoch 6207/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9406 - total_train_reward: -1370.4979\n",
      "Epoch 6208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0618 - total_train_reward: -1565.1803\n",
      "Epoch 6209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9028 - total_train_reward: -1733.6590\n",
      "Epoch 6210/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3604 - total_train_reward: -1056.6058\n",
      "Epoch 6211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7777 - total_train_reward: -1565.9676\n",
      "Epoch 6212/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3659 - total_train_reward: -1487.6326\n",
      "Epoch 6213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8986 - total_train_reward: -1663.6931\n",
      "Epoch 6214/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9467 - total_train_reward: -1249.7923\n",
      "Epoch 6215/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6987 - total_train_reward: -1177.6713\n",
      "Epoch 6216/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9485 - total_train_reward: -1165.7903\n",
      "Epoch 6217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9788 - total_train_reward: -1199.4805\n",
      "Epoch 6218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6011 - total_train_reward: -1858.5209\n",
      "Epoch 6219/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5836 - total_train_reward: -1187.2013\n",
      "Epoch 6220/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0328 - total_train_reward: -1192.7740\n",
      "Epoch 6221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2999 - total_train_reward: -1186.8993\n",
      "Epoch 6222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5555 - total_train_reward: -1395.1023\n",
      "Epoch 6223/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2932 - total_train_reward: -1588.6989\n",
      "Epoch 6224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0887 - total_train_reward: -1585.1938\n",
      "Epoch 6225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6421 - total_train_reward: -1052.8144\n",
      "Epoch 6226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1480 - total_train_reward: -1181.4657\n",
      "Epoch 6227/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4678 - total_train_reward: -1343.4706\n",
      "Epoch 6228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5072 - total_train_reward: -1172.7414\n",
      "Epoch 6229/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4678 - total_train_reward: -1190.7789\n",
      "Epoch 6230/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3170 - total_train_reward: -731.4173\n",
      "Epoch 6231/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6730 - total_train_reward: -1650.1913\n",
      "Epoch 6232/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5032 - total_train_reward: -1243.1444\n",
      "Epoch 6233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.9095 - total_train_reward: -1156.3025\n",
      "Epoch 6234/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6100 - total_train_reward: -1122.4184\n",
      "Epoch 6235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0550 - total_train_reward: -1597.9696\n",
      "Epoch 6236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5705 - total_train_reward: -1190.4962\n",
      "Epoch 6237/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7007 - total_train_reward: -961.4107\n",
      "Epoch 6238/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5923 - total_train_reward: -1411.9330\n",
      "Epoch 6239/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6820 - total_train_reward: -1732.1476\n",
      "Epoch 6240/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7005 - total_train_reward: -1077.4934\n",
      "Epoch 6241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1745 - total_train_reward: -1860.0705\n",
      "Epoch 6242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5337 - total_train_reward: -1669.5576\n",
      "Epoch 6243/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1626 - total_train_reward: -1171.8750\n",
      "Epoch 6244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2581 - total_train_reward: -1192.9590\n",
      "Epoch 6245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8768 - total_train_reward: -1195.2283\n",
      "Epoch 6246/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0250 - total_train_reward: -1676.0605\n",
      "Epoch 6247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.9844 - total_train_reward: -1193.8955\n",
      "Epoch 6248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2175 - total_train_reward: -1182.3895\n",
      "Epoch 6249/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7196 - total_train_reward: -1186.9215\n",
      "Epoch 6250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1372 - total_train_reward: -1371.3965\n",
      "Epoch 6251/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8208 - total_train_reward: -1165.3263\n",
      "Epoch 6252/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1472 - total_train_reward: -1063.4271\n",
      "Epoch 6253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7515 - total_train_reward: -1294.5837\n",
      "Epoch 6254/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4535 - total_train_reward: -1777.6262\n",
      "Epoch 6255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1157 - total_train_reward: -1148.4505\n",
      "Epoch 6256/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5569 - total_train_reward: -1150.1757\n",
      "Epoch 6257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5391 - total_train_reward: -1181.8505\n",
      "Epoch 6258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5618 - total_train_reward: -723.4917\n",
      "Epoch 6259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6569 - total_train_reward: -1267.8273\n",
      "Epoch 6260/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6431 - total_train_reward: -1194.1492\n",
      "Epoch 6261/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3397 - total_train_reward: -1286.2923\n",
      "Epoch 6262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.0319 - total_train_reward: -1542.1570\n",
      "Epoch 6263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3003 - total_train_reward: -1332.1954\n",
      "Epoch 6264/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9477 - total_train_reward: -1136.4045\n",
      "Epoch 6265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9210 - total_train_reward: -1159.5972\n",
      "Epoch 6266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7120 - total_train_reward: -1458.8652\n",
      "Epoch 6267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6632 - total_train_reward: -962.6743\n",
      "Epoch 6268/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9047 - total_train_reward: -1757.7130\n",
      "Epoch 6269/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1145 - total_train_reward: -1182.0095\n",
      "Epoch 6270/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6963 - total_train_reward: -1418.5216\n",
      "Epoch 6271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8418 - total_train_reward: -1170.4157\n",
      "Epoch 6272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4957 - total_train_reward: -1253.3290\n",
      "Epoch 6273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2358 - total_train_reward: -1064.0886\n",
      "Epoch 6274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5219 - total_train_reward: -1728.0301\n",
      "Epoch 6275/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2836 - total_train_reward: -1168.9514\n",
      "Epoch 6276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9123 - total_train_reward: -1589.0410\n",
      "Epoch 6277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3382 - total_train_reward: -854.2323\n",
      "Epoch 6278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6238 - total_train_reward: -1353.3243\n",
      "Epoch 6279/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6150 - total_train_reward: -1077.2826\n",
      "Epoch 6280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8780 - total_train_reward: -863.6555\n",
      "Epoch 6281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 49.9817 - total_train_reward: -1194.3045\n",
      "Epoch 6282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9205 - total_train_reward: -1193.4552\n",
      "Epoch 6283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8759 - total_train_reward: -1348.8410\n",
      "Epoch 6284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7969 - total_train_reward: -1193.5809\n",
      "Epoch 6285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0690 - total_train_reward: -1196.3249\n",
      "Epoch 6286/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4318 - total_train_reward: -1170.6077\n",
      "Epoch 6287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4403 - total_train_reward: -1142.2761\n",
      "Epoch 6288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8253 - total_train_reward: -1162.5711\n",
      "Epoch 6289/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3233 - total_train_reward: -1083.5527\n",
      "Epoch 6290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 27.7939 - total_train_reward: -742.1419\n",
      "Epoch 6291/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 23.0614 - total_train_reward: -1003.5823\n",
      "Epoch 6292/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3125 - total_train_reward: -1699.9887\n",
      "Epoch 6293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0978 - total_train_reward: -1680.5824\n",
      "Epoch 6294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0503 - total_train_reward: -1311.0329\n",
      "Epoch 6295/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.0576 - total_train_reward: -1192.5483\n",
      "Epoch 6296/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4817 - total_train_reward: -1184.1739\n",
      "Epoch 6297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7664 - total_train_reward: -1279.0268\n",
      "Epoch 6298/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3493 - total_train_reward: -1180.7470\n",
      "Epoch 6299/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7722 - total_train_reward: -1166.2266\n",
      "Epoch 6300/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5257 - total_train_reward: -854.6578\n",
      "Epoch 6301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2545 - total_train_reward: -1325.5221\n",
      "Epoch 6302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3473 - total_train_reward: -1066.8916\n",
      "Epoch 6303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0870 - total_train_reward: -1066.9447\n",
      "Epoch 6304/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8690 - total_train_reward: -1473.5376\n",
      "Epoch 6305/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6557 - total_train_reward: -1192.3343\n",
      "Epoch 6306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0460 - total_train_reward: -1779.3436\n",
      "Epoch 6307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3052 - total_train_reward: -1189.0254\n",
      "Epoch 6308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1430 - total_train_reward: -1159.5119\n",
      "Epoch 6309/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6238 - total_train_reward: -1787.2566\n",
      "Epoch 6310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0507 - total_train_reward: -1776.3347\n",
      "Epoch 6311/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3255 - total_train_reward: -731.7065\n",
      "Epoch 6312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7335 - total_train_reward: -1429.0313\n",
      "Epoch 6313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5024 - total_train_reward: -847.2657\n",
      "Epoch 6314/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2384 - total_train_reward: -1068.3846\n",
      "Epoch 6315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3326 - total_train_reward: -1179.5447\n",
      "Epoch 6316/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4158 - total_train_reward: -1428.0280\n",
      "Epoch 6317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6368 - total_train_reward: -1169.9503\n",
      "Epoch 6318/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7258 - total_train_reward: -1300.1427\n",
      "Epoch 6319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3733 - total_train_reward: -1263.3764\n",
      "Epoch 6320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6286 - total_train_reward: -1250.5862\n",
      "Epoch 6321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6297 - total_train_reward: -731.4455\n",
      "Epoch 6322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1142 - total_train_reward: -1789.4223\n",
      "Epoch 6323/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1366 - total_train_reward: -790.3121\n",
      "Epoch 6324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3625 - total_train_reward: -1174.1484\n",
      "Epoch 6325/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4904 - total_train_reward: -954.2515\n",
      "Epoch 6326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.8988 - total_train_reward: -1178.3697\n",
      "Epoch 6327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9100 - total_train_reward: -1737.4696\n",
      "Epoch 6328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8198 - total_train_reward: -1442.6839\n",
      "Epoch 6329/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0023 - total_train_reward: -1306.1692\n",
      "Epoch 6330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8508 - total_train_reward: -1478.7843\n",
      "Epoch 6331/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6415 - total_train_reward: -1407.3781\n",
      "Epoch 6332/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9020 - total_train_reward: -1803.6460\n",
      "Epoch 6333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5860 - total_train_reward: -1192.7848\n",
      "Epoch 6334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6822 - total_train_reward: -1402.0354\n",
      "Epoch 6335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2299 - total_train_reward: -1184.1557\n",
      "Epoch 6336/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9369 - total_train_reward: -1557.2208\n",
      "Epoch 6337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8538 - total_train_reward: -960.2641\n",
      "Epoch 6338/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4047 - total_train_reward: -1184.9452\n",
      "Epoch 6339/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1404 - total_train_reward: -1040.3719\n",
      "Epoch 6340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9669 - total_train_reward: -1174.7885\n",
      "Epoch 6341/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4312 - total_train_reward: -1191.7463\n",
      "Epoch 6342/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8958 - total_train_reward: -1621.7113\n",
      "Epoch 6343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6782 - total_train_reward: -1308.3038\n",
      "Epoch 6344/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3932 - total_train_reward: -1157.6235\n",
      "Epoch 6345/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7074 - total_train_reward: -1323.2814\n",
      "Epoch 6346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7669 - total_train_reward: -877.0246\n",
      "Epoch 6347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8077 - total_train_reward: -1480.9807\n",
      "Epoch 6348/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6577 - total_train_reward: -1167.1615\n",
      "Epoch 6349/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2398 - total_train_reward: -1639.8977\n",
      "Epoch 6350/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0062 - total_train_reward: -1471.4860\n",
      "Epoch 6351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3544 - total_train_reward: -1310.8133\n",
      "Epoch 6352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0186 - total_train_reward: -1065.8965\n",
      "Epoch 6353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7983 - total_train_reward: -1070.5802\n",
      "Epoch 6354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5540 - total_train_reward: -1196.5790\n",
      "Epoch 6355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7563 - total_train_reward: -1371.5406\n",
      "Epoch 6356/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5278 - total_train_reward: -1196.6868\n",
      "Epoch 6357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7868 - total_train_reward: -1208.7834\n",
      "Epoch 6358/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0544 - total_train_reward: -1647.2293\n",
      "Epoch 6359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1547 - total_train_reward: -1487.1311\n",
      "Epoch 6360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9955 - total_train_reward: -1404.6748\n",
      "Epoch 6361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6944 - total_train_reward: -1845.6937\n",
      "Epoch 6362/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5862 - total_train_reward: -1181.6984\n",
      "Epoch 6363/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5555 - total_train_reward: -1195.1577\n",
      "Epoch 6364/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4956 - total_train_reward: -1322.9722\n",
      "Epoch 6365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8654 - total_train_reward: -1778.6148\n",
      "Epoch 6366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2119 - total_train_reward: -1207.7467\n",
      "Epoch 6367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8684 - total_train_reward: -1309.3167\n",
      "Epoch 6368/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0760 - total_train_reward: -1690.9126\n",
      "Epoch 6369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2760 - total_train_reward: -1065.3111\n",
      "Epoch 6370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3051 - total_train_reward: -1163.5245\n",
      "Epoch 6371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1764 - total_train_reward: -1171.4601\n",
      "Epoch 6372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0993 - total_train_reward: -1198.3731\n",
      "Epoch 6373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5379 - total_train_reward: -912.7603\n",
      "Epoch 6374/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8786 - total_train_reward: -1728.9567\n",
      "Epoch 6375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9264 - total_train_reward: -1193.0263\n",
      "Epoch 6376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7778 - total_train_reward: -1597.8251\n",
      "Epoch 6377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1600 - total_train_reward: -1117.9210\n",
      "Epoch 6378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6564 - total_train_reward: -1065.5025\n",
      "Epoch 6379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9298 - total_train_reward: -1241.6538\n",
      "Epoch 6380/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1420 - total_train_reward: -856.5345\n",
      "Epoch 6381/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1600 - total_train_reward: -1168.4370\n",
      "Epoch 6382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5555 - total_train_reward: -1623.3603\n",
      "Epoch 6383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1573 - total_train_reward: -1672.2792\n",
      "Epoch 6384/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4206 - total_train_reward: -1179.6138\n",
      "Epoch 6385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7034 - total_train_reward: -1082.4457\n",
      "Epoch 6386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8556 - total_train_reward: -1197.6893\n",
      "Epoch 6387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4401 - total_train_reward: -1484.5699\n",
      "Epoch 6388/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0326 - total_train_reward: -1781.5107\n",
      "Epoch 6389/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4898 - total_train_reward: -1330.5248\n",
      "Epoch 6390/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5840 - total_train_reward: -1388.9913\n",
      "Epoch 6391/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9276 - total_train_reward: -1463.8618\n",
      "Epoch 6392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1264 - total_train_reward: -1189.1325\n",
      "Epoch 6393/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0661 - total_train_reward: -1071.7371\n",
      "Epoch 6394/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3118 - total_train_reward: -1196.3749\n",
      "Epoch 6395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3390 - total_train_reward: -1416.5026\n",
      "Epoch 6396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7930 - total_train_reward: -1185.3413\n",
      "Epoch 6397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4569 - total_train_reward: -997.9303\n",
      "Epoch 6398/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3736 - total_train_reward: -1544.9141\n",
      "Epoch 6399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1407 - total_train_reward: -1852.3200\n",
      "Epoch 6400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4673 - total_train_reward: -1195.9458\n",
      "Epoch 6401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7648 - total_train_reward: -1186.2947\n",
      "Epoch 6402/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.8817 - total_train_reward: -1645.1991\n",
      "Epoch 6403/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3294 - total_train_reward: -1168.1961\n",
      "Epoch 6404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1643 - total_train_reward: -731.4372\n",
      "Epoch 6405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7556 - total_train_reward: -1173.4772\n",
      "Epoch 6406/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0773 - total_train_reward: -1852.1217\n",
      "Epoch 6407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1767 - total_train_reward: -716.0061\n",
      "Epoch 6408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3724 - total_train_reward: -1337.8303\n",
      "Epoch 6409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9444 - total_train_reward: -1359.1918\n",
      "Epoch 6410/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4429 - total_train_reward: -1065.5459\n",
      "Epoch 6411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4404 - total_train_reward: -1235.1449\n",
      "Epoch 6412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9799 - total_train_reward: -1597.7750\n",
      "Epoch 6413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1949 - total_train_reward: -957.4182\n",
      "Epoch 6414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8622 - total_train_reward: -959.9401\n",
      "Epoch 6415/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9724 - total_train_reward: -1850.2620\n",
      "Epoch 6416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0571 - total_train_reward: -1185.9703\n",
      "Epoch 6417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0526 - total_train_reward: -1188.3246\n",
      "Epoch 6418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4271 - total_train_reward: -1120.7544\n",
      "Epoch 6419/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0683 - total_train_reward: -1672.7292\n",
      "Epoch 6420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7910 - total_train_reward: -1320.6585\n",
      "Epoch 6421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8430 - total_train_reward: -1071.9240\n",
      "Epoch 6422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9270 - total_train_reward: -1606.0663\n",
      "Epoch 6423/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6398 - total_train_reward: -1060.5177\n",
      "Epoch 6424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2086 - total_train_reward: -1250.7815\n",
      "Epoch 6425/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6918 - total_train_reward: -1056.4675\n",
      "Epoch 6426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6676 - total_train_reward: -1305.0475\n",
      "Epoch 6427/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1237 - total_train_reward: -1711.9362\n",
      "Epoch 6428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1358 - total_train_reward: -1535.4456\n",
      "Epoch 6429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8884 - total_train_reward: -1158.4325\n",
      "Epoch 6430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.4179 - total_train_reward: -1188.3782\n",
      "Epoch 6431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.6022 - total_train_reward: -1585.9701\n",
      "Epoch 6432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7524 - total_train_reward: -1843.4093\n",
      "Epoch 6433/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1724 - total_train_reward: -1196.5104\n",
      "Epoch 6434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2515 - total_train_reward: -1849.6729\n",
      "Epoch 6435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3973 - total_train_reward: -1196.2828\n",
      "Epoch 6436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9572 - total_train_reward: -1498.5471\n",
      "Epoch 6437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4172 - total_train_reward: -1178.8013\n",
      "Epoch 6438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8668 - total_train_reward: -1101.1328\n",
      "Epoch 6439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1404 - total_train_reward: -1443.0402\n",
      "Epoch 6440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0076 - total_train_reward: -1236.3541\n",
      "Epoch 6441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5674 - total_train_reward: -729.8835\n",
      "Epoch 6442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7081 - total_train_reward: -1759.4011\n",
      "Epoch 6443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7303 - total_train_reward: -1354.9283\n",
      "Epoch 6444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3785 - total_train_reward: -1165.8701\n",
      "Epoch 6445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5579 - total_train_reward: -1785.5075\n",
      "Epoch 6446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3517 - total_train_reward: -1293.9252\n",
      "Epoch 6447/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8516 - total_train_reward: -1775.2007\n",
      "Epoch 6448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4416 - total_train_reward: -1192.0208\n",
      "Epoch 6449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0608 - total_train_reward: -1288.3356\n",
      "Epoch 6450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8964 - total_train_reward: -1471.3879\n",
      "Epoch 6451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3619 - total_train_reward: -961.2876\n",
      "Epoch 6452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9136 - total_train_reward: -1378.2013\n",
      "Epoch 6453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1814 - total_train_reward: -1592.9388\n",
      "Epoch 6454/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6736 - total_train_reward: -1576.0181\n",
      "Epoch 6455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7951 - total_train_reward: -1175.8158\n",
      "Epoch 6456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0019 - total_train_reward: -1542.4544\n",
      "Epoch 6457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0754 - total_train_reward: -1181.8418\n",
      "Epoch 6458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0975 - total_train_reward: -1110.4644\n",
      "Epoch 6459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9007 - total_train_reward: -1198.9600\n",
      "Epoch 6460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1813 - total_train_reward: -1622.9455\n",
      "Epoch 6461/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8786 - total_train_reward: -1387.1783\n",
      "Epoch 6462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6621 - total_train_reward: -1224.1860\n",
      "Epoch 6463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2148 - total_train_reward: -1195.1716\n",
      "Epoch 6464/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2546 - total_train_reward: -1201.2867\n",
      "Epoch 6465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9639 - total_train_reward: -960.8564\n",
      "Epoch 6466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6927 - total_train_reward: -856.1578\n",
      "Epoch 6467/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4159 - total_train_reward: -1390.4843\n",
      "Epoch 6468/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6821 - total_train_reward: -1303.7618\n",
      "Epoch 6469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6163 - total_train_reward: -1201.2641\n",
      "Epoch 6470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3028 - total_train_reward: -1348.9988\n",
      "Epoch 6471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2758 - total_train_reward: -1163.0293\n",
      "Epoch 6472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3988 - total_train_reward: -1192.2787\n",
      "Epoch 6473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7013 - total_train_reward: -1670.6251\n",
      "Epoch 6474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3905 - total_train_reward: -1635.6224\n",
      "Epoch 6475/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7640 - total_train_reward: -1646.3785\n",
      "Epoch 6476/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6513 - total_train_reward: -848.9326\n",
      "Epoch 6477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5854 - total_train_reward: -1519.3360\n",
      "Epoch 6478/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8362 - total_train_reward: -1183.7318\n",
      "Epoch 6479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8705 - total_train_reward: -1065.1566\n",
      "Epoch 6480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1582 - total_train_reward: -1473.7531\n",
      "Epoch 6481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3966 - total_train_reward: -1202.1506\n",
      "Epoch 6482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4061 - total_train_reward: -1318.7565\n",
      "Epoch 6483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0897 - total_train_reward: -1504.4992\n",
      "Epoch 6484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0766 - total_train_reward: -1673.8155\n",
      "Epoch 6485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1387 - total_train_reward: -1184.2262\n",
      "Epoch 6486/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3753 - total_train_reward: -1169.5744\n",
      "Epoch 6487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5879 - total_train_reward: -1444.7151\n",
      "Epoch 6488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6919 - total_train_reward: -1166.8940\n",
      "Epoch 6489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4283 - total_train_reward: -1324.3501\n",
      "Epoch 6490/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3987 - total_train_reward: -1614.0563\n",
      "Epoch 6491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3661 - total_train_reward: -1183.9624\n",
      "Epoch 6492/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8198 - total_train_reward: -1804.4044\n",
      "Epoch 6493/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7317 - total_train_reward: -1571.5608\n",
      "Epoch 6494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0425 - total_train_reward: -1177.0614\n",
      "Epoch 6495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5448 - total_train_reward: -762.0860\n",
      "Epoch 6496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2428 - total_train_reward: -970.7366\n",
      "Epoch 6497/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2598 - total_train_reward: -1696.2819\n",
      "Epoch 6498/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.9757 - total_train_reward: -660.6895\n",
      "Epoch 6499/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1922 - total_train_reward: -1070.3457\n",
      "Epoch 6500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8205 - total_train_reward: -1170.6741\n",
      "Epoch 6501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9799 - total_train_reward: -1378.7036\n",
      "Epoch 6502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1863 - total_train_reward: -1303.5276\n",
      "Epoch 6503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0172 - total_train_reward: -1856.0473\n",
      "Epoch 6504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6079 - total_train_reward: -1182.4944\n",
      "Epoch 6505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1906 - total_train_reward: -1548.6430\n",
      "Epoch 6506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4531 - total_train_reward: -1312.5865\n",
      "Epoch 6507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.9038 - total_train_reward: -1043.6314\n",
      "Epoch 6508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7093 - total_train_reward: -1476.0864\n",
      "Epoch 6509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2818 - total_train_reward: -851.7189\n",
      "Epoch 6510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3422 - total_train_reward: -1248.8671\n",
      "Epoch 6511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0441 - total_train_reward: -1162.0675\n",
      "Epoch 6512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3556 - total_train_reward: -1029.2944\n",
      "Epoch 6513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8390 - total_train_reward: -1486.0285\n",
      "Epoch 6514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0335 - total_train_reward: -1763.4101\n",
      "Epoch 6515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4857 - total_train_reward: -1312.5312\n",
      "Epoch 6516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9898 - total_train_reward: -1248.0708\n",
      "Epoch 6517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1101 - total_train_reward: -1675.9289\n",
      "Epoch 6518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9600 - total_train_reward: -1176.0817\n",
      "Epoch 6519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9289 - total_train_reward: -1119.5235\n",
      "Epoch 6520/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3895 - total_train_reward: -1193.6329\n",
      "Epoch 6521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8733 - total_train_reward: -1518.7983\n",
      "Epoch 6522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7132 - total_train_reward: -961.5439\n",
      "Epoch 6523/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0882 - total_train_reward: -1174.6323\n",
      "Epoch 6524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8807 - total_train_reward: -1286.3214\n",
      "Epoch 6525/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1226 - total_train_reward: -1359.1124\n",
      "Epoch 6526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7974 - total_train_reward: -1607.5595\n",
      "Epoch 6527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1604 - total_train_reward: -1365.9022\n",
      "Epoch 6528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7636 - total_train_reward: -1341.8435\n",
      "Epoch 6529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2086 - total_train_reward: -1043.7985\n",
      "Epoch 6530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2866 - total_train_reward: -1180.0046\n",
      "Epoch 6531/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1806 - total_train_reward: -1621.8989\n",
      "Epoch 6532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1519 - total_train_reward: -1242.1442\n",
      "Epoch 6533/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2137 - total_train_reward: -1353.4370\n",
      "Epoch 6534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9121 - total_train_reward: -1361.7211\n",
      "Epoch 6535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5640 - total_train_reward: -1175.7193\n",
      "Epoch 6536/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9945 - total_train_reward: -1454.4818\n",
      "Epoch 6537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3426 - total_train_reward: -1208.9841\n",
      "Epoch 6538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0481 - total_train_reward: -1560.1278\n",
      "Epoch 6539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1918 - total_train_reward: -1703.0865\n",
      "Epoch 6540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4060 - total_train_reward: -1061.7658\n",
      "Epoch 6541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5255 - total_train_reward: -1214.3871\n",
      "Epoch 6542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3392 - total_train_reward: -1764.2302\n",
      "Epoch 6543/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6246 - total_train_reward: -1197.0190\n",
      "Epoch 6544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7073 - total_train_reward: -1193.5679\n",
      "Epoch 6545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0200 - total_train_reward: -1067.7522\n",
      "Epoch 6546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3783 - total_train_reward: -1325.5574\n",
      "Epoch 6547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2811 - total_train_reward: -1276.5184\n",
      "Epoch 6548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4197 - total_train_reward: -1833.1573\n",
      "Epoch 6549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1192 - total_train_reward: -1343.0947\n",
      "Epoch 6550/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6621 - total_train_reward: -1192.4732\n",
      "Epoch 6551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6205 - total_train_reward: -1286.3641\n",
      "Epoch 6552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5706 - total_train_reward: -1206.0522\n",
      "Epoch 6553/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4572 - total_train_reward: -1213.9772\n",
      "Epoch 6554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4582 - total_train_reward: -1313.7380\n",
      "Epoch 6555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3677 - total_train_reward: -1198.8084\n",
      "Epoch 6556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2782 - total_train_reward: -1166.1754\n",
      "Epoch 6557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2181 - total_train_reward: -1065.4320\n",
      "Epoch 6558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1090 - total_train_reward: -1237.7704\n",
      "Epoch 6559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2956 - total_train_reward: -1105.1248\n",
      "Epoch 6560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2622 - total_train_reward: -1215.8880\n",
      "Epoch 6561/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1345 - total_train_reward: -1749.3474\n",
      "Epoch 6562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3739 - total_train_reward: -847.7345\n",
      "Epoch 6563/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6292 - total_train_reward: -1065.6369\n",
      "Epoch 6564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1450 - total_train_reward: -1764.4075\n",
      "Epoch 6565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8669 - total_train_reward: -1551.6444\n",
      "Epoch 6566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8501 - total_train_reward: -1115.0417\n",
      "Epoch 6567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9390 - total_train_reward: -1494.7697\n",
      "Epoch 6568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3290 - total_train_reward: -957.8388\n",
      "Epoch 6569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3905 - total_train_reward: -1679.5687\n",
      "Epoch 6570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7964 - total_train_reward: -1705.4665\n",
      "Epoch 6571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6081 - total_train_reward: -1659.5430\n",
      "Epoch 6572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1731 - total_train_reward: -1222.5613\n",
      "Epoch 6573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7925 - total_train_reward: -1364.4524\n",
      "Epoch 6574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1912 - total_train_reward: -1255.7300\n",
      "Epoch 6575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4244 - total_train_reward: -1746.6360\n",
      "Epoch 6576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9243 - total_train_reward: -1242.9046\n",
      "Epoch 6577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2007 - total_train_reward: -832.4519\n",
      "Epoch 6578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3761 - total_train_reward: -1195.4283\n",
      "Epoch 6579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.4076 - total_train_reward: -1198.6348\n",
      "Epoch 6580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1520 - total_train_reward: -1330.9689\n",
      "Epoch 6581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4967 - total_train_reward: -1031.2537\n",
      "Epoch 6582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3189 - total_train_reward: -1130.6495\n",
      "Epoch 6583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4891 - total_train_reward: -1175.9682\n",
      "Epoch 6584/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2900 - total_train_reward: -1041.8180\n",
      "Epoch 6585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0068 - total_train_reward: -1452.5934\n",
      "Epoch 6586/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5056 - total_train_reward: -1303.9605\n",
      "Epoch 6587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1240 - total_train_reward: -1202.1524\n",
      "Epoch 6588/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4129 - total_train_reward: -1283.8457\n",
      "Epoch 6589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6222 - total_train_reward: -1167.6936\n",
      "Epoch 6590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9051 - total_train_reward: -1038.2421\n",
      "Epoch 6591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7524 - total_train_reward: -1797.1061\n",
      "Epoch 6592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9546 - total_train_reward: -1390.9942\n",
      "Epoch 6593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5684 - total_train_reward: -1793.2208\n",
      "Epoch 6594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1273 - total_train_reward: -959.5815\n",
      "Epoch 6595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7276 - total_train_reward: -1842.6902\n",
      "Epoch 6596/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8184 - total_train_reward: -1364.7113\n",
      "Epoch 6597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7690 - total_train_reward: -1444.8667\n",
      "Epoch 6598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2291 - total_train_reward: -1392.6273\n",
      "Epoch 6599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6792 - total_train_reward: -1264.9324\n",
      "Epoch 6600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9213 - total_train_reward: -1343.7043\n",
      "Epoch 6601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6453 - total_train_reward: -1201.9874\n",
      "Epoch 6602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2779 - total_train_reward: -1333.4524\n",
      "Epoch 6603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9525 - total_train_reward: -1204.2147\n",
      "Epoch 6604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8409 - total_train_reward: -960.4473\n",
      "Epoch 6605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2992 - total_train_reward: -1170.4683\n",
      "Epoch 6606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5124 - total_train_reward: -1197.8844\n",
      "Epoch 6607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0149 - total_train_reward: -1156.8438\n",
      "Epoch 6608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2162 - total_train_reward: -1474.5293\n",
      "Epoch 6609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8561 - total_train_reward: -1203.1523\n",
      "Epoch 6610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4673 - total_train_reward: -1262.4778\n",
      "Epoch 6611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7328 - total_train_reward: -1426.8447\n",
      "Epoch 6612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9245 - total_train_reward: -1540.1750\n",
      "Epoch 6613/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4085 - total_train_reward: -960.2860\n",
      "Epoch 6614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6612 - total_train_reward: -1526.0290\n",
      "Epoch 6615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3498 - total_train_reward: -1560.9327\n",
      "Epoch 6616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2592 - total_train_reward: -1195.7640\n",
      "Epoch 6617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9512 - total_train_reward: -1195.9873\n",
      "Epoch 6618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2915 - total_train_reward: -1150.6448\n",
      "Epoch 6619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7473 - total_train_reward: -1199.6969\n",
      "Epoch 6620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5032 - total_train_reward: -1058.0823\n",
      "Epoch 6621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8571 - total_train_reward: -1150.7505\n",
      "Epoch 6622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3589 - total_train_reward: -1502.1019\n",
      "Epoch 6623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8413 - total_train_reward: -1164.4492\n",
      "Epoch 6624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3883 - total_train_reward: -1474.6629\n",
      "Epoch 6625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4422 - total_train_reward: -1509.4466\n",
      "Epoch 6626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7035 - total_train_reward: -1101.5736\n",
      "Epoch 6627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4677 - total_train_reward: -1204.7101\n",
      "Epoch 6628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1983 - total_train_reward: -1122.7756\n",
      "Epoch 6629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1372 - total_train_reward: -1190.3774\n",
      "Epoch 6630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3636 - total_train_reward: -1391.9904\n",
      "Epoch 6631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2611 - total_train_reward: -1800.1794\n",
      "Epoch 6632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8208 - total_train_reward: -1236.0626\n",
      "Epoch 6633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9158 - total_train_reward: -1098.1693\n",
      "Epoch 6634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3124 - total_train_reward: -1196.1170\n",
      "Epoch 6635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5340 - total_train_reward: -1064.2578\n",
      "Epoch 6636/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2770 - total_train_reward: -1082.2145\n",
      "Epoch 6637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0944 - total_train_reward: -1648.0702\n",
      "Epoch 6638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3003 - total_train_reward: -1054.8510\n",
      "Epoch 6639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4902 - total_train_reward: -1288.3000\n",
      "Epoch 6640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4646 - total_train_reward: -1333.5402\n",
      "Epoch 6641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3547 - total_train_reward: -1762.9987\n",
      "Epoch 6642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8375 - total_train_reward: -1429.3893\n",
      "Epoch 6643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1072 - total_train_reward: -1400.1842\n",
      "Epoch 6644/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5840 - total_train_reward: -1540.9652\n",
      "Epoch 6645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1241 - total_train_reward: -1553.3614\n",
      "Epoch 6646/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8583 - total_train_reward: -1311.1326\n",
      "Epoch 6647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6260 - total_train_reward: -1159.4474\n",
      "Epoch 6648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8456 - total_train_reward: -1149.0110\n",
      "Epoch 6649/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1646 - total_train_reward: -1170.5782\n",
      "Epoch 6650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0254 - total_train_reward: -1202.5090\n",
      "Epoch 6651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7241 - total_train_reward: -1378.5725\n",
      "Epoch 6652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1534 - total_train_reward: -1230.2386\n",
      "Epoch 6653/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5038 - total_train_reward: -1179.3469\n",
      "Epoch 6654/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7240 - total_train_reward: -1794.6862\n",
      "Epoch 6655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2378 - total_train_reward: -925.5072\n",
      "Epoch 6656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4506 - total_train_reward: -1698.9297\n",
      "Epoch 6657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8422 - total_train_reward: -1068.4066\n",
      "Epoch 6658/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7183 - total_train_reward: -1728.6890\n",
      "Epoch 6659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.6778 - total_train_reward: -1080.0356\n",
      "Epoch 6660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0079 - total_train_reward: -1320.4608\n",
      "Epoch 6661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8002 - total_train_reward: -1082.5781\n",
      "Epoch 6662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0665 - total_train_reward: -1362.9978\n",
      "Epoch 6663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1958 - total_train_reward: -840.6988\n",
      "Epoch 6664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1022 - total_train_reward: -1754.0512\n",
      "Epoch 6665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3181 - total_train_reward: -1802.6855\n",
      "Epoch 6666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2682 - total_train_reward: -1341.4097\n",
      "Epoch 6667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1156 - total_train_reward: -1773.7631\n",
      "Epoch 6668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7149 - total_train_reward: -1513.1210\n",
      "Epoch 6669/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4556 - total_train_reward: -1388.5574\n",
      "Epoch 6670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0170 - total_train_reward: -1684.6978\n",
      "Epoch 6671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8622 - total_train_reward: -1105.1778\n",
      "Epoch 6672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3038 - total_train_reward: -1739.0606\n",
      "Epoch 6673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5780 - total_train_reward: -1202.2143\n",
      "Epoch 6674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5450 - total_train_reward: -960.6215\n",
      "Epoch 6675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3102 - total_train_reward: -1229.1454\n",
      "Epoch 6676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7847 - total_train_reward: -1844.1145\n",
      "Epoch 6677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4372 - total_train_reward: -1676.7499\n",
      "Epoch 6678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4190 - total_train_reward: -1360.7035\n",
      "Epoch 6679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2536 - total_train_reward: -1700.6404\n",
      "Epoch 6680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2527 - total_train_reward: -1041.3565\n",
      "Epoch 6681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7741 - total_train_reward: -1179.0721\n",
      "Epoch 6682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0072 - total_train_reward: -1624.6523\n",
      "Epoch 6683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1973 - total_train_reward: -859.8920\n",
      "Epoch 6684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6679 - total_train_reward: -1575.7049\n",
      "Epoch 6685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8287 - total_train_reward: -1197.9015\n",
      "Epoch 6686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1091 - total_train_reward: -959.9614\n",
      "Epoch 6687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7450 - total_train_reward: -1197.6638\n",
      "Epoch 6688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7300 - total_train_reward: -1448.4262\n",
      "Epoch 6689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2332 - total_train_reward: -1185.8962\n",
      "Epoch 6690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3793 - total_train_reward: -1691.8940\n",
      "Epoch 6691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1692 - total_train_reward: -1175.7207\n",
      "Epoch 6692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2058 - total_train_reward: -1582.4312\n",
      "Epoch 6693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5057 - total_train_reward: -1172.3209\n",
      "Epoch 6694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8676 - total_train_reward: -1191.2204\n",
      "Epoch 6695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1783 - total_train_reward: -1452.4453\n",
      "Epoch 6696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3681 - total_train_reward: -1197.6755\n",
      "Epoch 6697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5050 - total_train_reward: -1284.2398\n",
      "Epoch 6698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8542 - total_train_reward: -1201.5238\n",
      "Epoch 6699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6538 - total_train_reward: -1451.7468\n",
      "Epoch 6700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5802 - total_train_reward: -1319.3888\n",
      "Epoch 6701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4279 - total_train_reward: -1320.5029\n",
      "Epoch 6702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9764 - total_train_reward: -1703.7023\n",
      "Epoch 6703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6299 - total_train_reward: -1545.4154\n",
      "Epoch 6704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3491 - total_train_reward: -1339.2922\n",
      "Epoch 6705/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1896 - total_train_reward: -1299.9342\n",
      "Epoch 6706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2909 - total_train_reward: -1163.7204\n",
      "Epoch 6707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0222 - total_train_reward: -976.2986\n",
      "Epoch 6708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2040 - total_train_reward: -873.6985\n",
      "Epoch 6709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8488 - total_train_reward: -1806.9931\n",
      "Epoch 6710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6964 - total_train_reward: -1465.0051\n",
      "Epoch 6711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.4093 - total_train_reward: -1141.0243\n",
      "Epoch 6712/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8962 - total_train_reward: -960.9134\n",
      "Epoch 6713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0308 - total_train_reward: -1193.3529\n",
      "Epoch 6714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5681 - total_train_reward: -1145.8517\n",
      "Epoch 6715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8859 - total_train_reward: -958.0199\n",
      "Epoch 6716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7733 - total_train_reward: -1061.7224\n",
      "Epoch 6717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8958 - total_train_reward: -1162.7029\n",
      "Epoch 6718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9644 - total_train_reward: -1080.7787\n",
      "Epoch 6719/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5637 - total_train_reward: -1417.5916\n",
      "Epoch 6720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7830 - total_train_reward: -1401.7074\n",
      "Epoch 6721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9792 - total_train_reward: -1378.1216\n",
      "Epoch 6722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2137 - total_train_reward: -1670.6197\n",
      "Epoch 6723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5457 - total_train_reward: -973.6411\n",
      "Epoch 6724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5451 - total_train_reward: -1165.1312\n",
      "Epoch 6725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3030 - total_train_reward: -1174.6462\n",
      "Epoch 6726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4458 - total_train_reward: -1203.0715\n",
      "Epoch 6727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2961 - total_train_reward: -1349.9002\n",
      "Epoch 6728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8047 - total_train_reward: -1158.5089\n",
      "Epoch 6729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9102 - total_train_reward: -1489.1010\n",
      "Epoch 6730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6444 - total_train_reward: -1372.0754\n",
      "Epoch 6731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8567 - total_train_reward: -1778.5690\n",
      "Epoch 6732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7867 - total_train_reward: -1214.3801\n",
      "Epoch 6733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4716 - total_train_reward: -1211.4090\n",
      "Epoch 6734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8848 - total_train_reward: -1687.5649\n",
      "Epoch 6735/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5271 - total_train_reward: -1791.6257\n",
      "Epoch 6736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.2918 - total_train_reward: -1649.8459\n",
      "Epoch 6737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0943 - total_train_reward: -1791.3467\n",
      "Epoch 6738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1789 - total_train_reward: -1163.4541\n",
      "Epoch 6739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0619 - total_train_reward: -1004.5051\n",
      "Epoch 6740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6048 - total_train_reward: -1629.9073\n",
      "Epoch 6741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2628 - total_train_reward: -1445.9299\n",
      "Epoch 6742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3290 - total_train_reward: -845.4257\n",
      "Epoch 6743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3698 - total_train_reward: -1200.2111\n",
      "Epoch 6744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6518 - total_train_reward: -1343.4774\n",
      "Epoch 6745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0774 - total_train_reward: -1473.5689\n",
      "Epoch 6746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9246 - total_train_reward: -1061.1583\n",
      "Epoch 6747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2382 - total_train_reward: -1427.6326\n",
      "Epoch 6748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4125 - total_train_reward: -1400.3489\n",
      "Epoch 6749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5352 - total_train_reward: -862.3099\n",
      "Epoch 6750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5141 - total_train_reward: -1145.5710\n",
      "Epoch 6751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7022 - total_train_reward: -1070.5507\n",
      "Epoch 6752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8625 - total_train_reward: -1234.2507\n",
      "Epoch 6753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0339 - total_train_reward: -1576.8037\n",
      "Epoch 6754/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.8809 - total_train_reward: -1146.0534\n",
      "Epoch 6755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0825 - total_train_reward: -1320.8944\n",
      "Epoch 6756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3071 - total_train_reward: -1223.0870\n",
      "Epoch 6757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8807 - total_train_reward: -1189.4805\n",
      "Epoch 6758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6784 - total_train_reward: -961.1171\n",
      "Epoch 6759/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5585 - total_train_reward: -694.4651\n",
      "Epoch 6760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0302 - total_train_reward: -1073.5104\n",
      "Epoch 6761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5491 - total_train_reward: -1787.7974\n",
      "Epoch 6762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5177 - total_train_reward: -1063.2529\n",
      "Epoch 6763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2491 - total_train_reward: -1482.2133\n",
      "Epoch 6764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8562 - total_train_reward: -1344.1501\n",
      "Epoch 6765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4647 - total_train_reward: -1164.5796\n",
      "Epoch 6766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4973 - total_train_reward: -1285.4141\n",
      "Epoch 6767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2814 - total_train_reward: -1193.2653\n",
      "Epoch 6768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9021 - total_train_reward: -1176.5584\n",
      "Epoch 6769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7060 - total_train_reward: -1194.6080\n",
      "Epoch 6770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9868 - total_train_reward: -1824.9650\n",
      "Epoch 6771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1745 - total_train_reward: -966.2536\n",
      "Epoch 6772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7949 - total_train_reward: -1796.7265\n",
      "Epoch 6773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2786 - total_train_reward: -1807.9995\n",
      "Epoch 6774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8098 - total_train_reward: -730.2140\n",
      "Epoch 6775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9267 - total_train_reward: -1065.4908\n",
      "Epoch 6776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.3253 - total_train_reward: -1179.1358\n",
      "Epoch 6777/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4135 - total_train_reward: -1176.5641\n",
      "Epoch 6778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.4474 - total_train_reward: -1749.1644\n",
      "Epoch 6779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1632 - total_train_reward: -1637.2148\n",
      "Epoch 6780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8766 - total_train_reward: -967.8168\n",
      "Epoch 6781/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4015 - total_train_reward: -1236.0930\n",
      "Epoch 6782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7646 - total_train_reward: -1351.8884\n",
      "Epoch 6783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.1541 - total_train_reward: -1561.2309\n",
      "Epoch 6784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9820 - total_train_reward: -1264.5724\n",
      "Epoch 6785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4424 - total_train_reward: -1624.5933\n",
      "Epoch 6786/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6782 - total_train_reward: -1516.0241\n",
      "Epoch 6787/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9244 - total_train_reward: -1543.0520\n",
      "Epoch 6788/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0912 - total_train_reward: -1065.1428\n",
      "Epoch 6789/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9228 - total_train_reward: -1355.2563\n",
      "Epoch 6790/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1405 - total_train_reward: -1163.4886\n",
      "Epoch 6791/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1187 - total_train_reward: -1578.4243\n",
      "Epoch 6792/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8324 - total_train_reward: -1230.5363\n",
      "Epoch 6793/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7782 - total_train_reward: -1719.0996\n",
      "Epoch 6794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0430 - total_train_reward: -1176.1115\n",
      "Epoch 6795/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4922 - total_train_reward: -1542.7878\n",
      "Epoch 6796/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1542 - total_train_reward: -730.0303\n",
      "Epoch 6797/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7009 - total_train_reward: -1410.7801\n",
      "Epoch 6798/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5973 - total_train_reward: -1291.6013\n",
      "Epoch 6799/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4970 - total_train_reward: -1341.3343\n",
      "Epoch 6800/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7062 - total_train_reward: -1270.5749\n",
      "Epoch 6801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3809 - total_train_reward: -1760.0328\n",
      "Epoch 6802/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1327 - total_train_reward: -1583.6052\n",
      "Epoch 6803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5143 - total_train_reward: -1401.1483\n",
      "Epoch 6804/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.0038 - total_train_reward: -738.0953\n",
      "Epoch 6805/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3535 - total_train_reward: -1495.5108\n",
      "Epoch 6806/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9217 - total_train_reward: -1427.9546\n",
      "Epoch 6807/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8073 - total_train_reward: -1820.2433\n",
      "Epoch 6808/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5950 - total_train_reward: -1545.9129\n",
      "Epoch 6809/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5629 - total_train_reward: -1777.9094\n",
      "Epoch 6810/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4517 - total_train_reward: -1664.0991\n",
      "Epoch 6811/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8598 - total_train_reward: -1605.8338\n",
      "Epoch 6812/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4784 - total_train_reward: -1200.0769\n",
      "Epoch 6813/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2479 - total_train_reward: -1777.2728\n",
      "Epoch 6814/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7156 - total_train_reward: -1134.1422\n",
      "Epoch 6815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5885 - total_train_reward: -972.0710\n",
      "Epoch 6816/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7226 - total_train_reward: -1047.3690\n",
      "Epoch 6817/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9887 - total_train_reward: -1510.3203\n",
      "Epoch 6818/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8262 - total_train_reward: -1198.7879\n",
      "Epoch 6819/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7724 - total_train_reward: -1313.7692\n",
      "Epoch 6820/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4379 - total_train_reward: -1414.9370\n",
      "Epoch 6821/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1050 - total_train_reward: -1171.5727\n",
      "Epoch 6822/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4499 - total_train_reward: -1068.7609\n",
      "Epoch 6823/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0680 - total_train_reward: -846.2365\n",
      "Epoch 6824/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3506 - total_train_reward: -847.5980\n",
      "Epoch 6825/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.0483 - total_train_reward: -1193.3707\n",
      "Epoch 6826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1434 - total_train_reward: -1719.2494\n",
      "Epoch 6827/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2503 - total_train_reward: -1192.3694\n",
      "Epoch 6828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3886 - total_train_reward: -1073.5467\n",
      "Epoch 6829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7859 - total_train_reward: -1398.0426\n",
      "Epoch 6830/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4649 - total_train_reward: -963.1313\n",
      "Epoch 6831/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9401 - total_train_reward: -1183.5266\n",
      "Epoch 6832/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2239 - total_train_reward: -1059.1634\n",
      "Epoch 6833/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2778 - total_train_reward: -853.5511\n",
      "Epoch 6834/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7398 - total_train_reward: -1806.7442\n",
      "Epoch 6835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4249 - total_train_reward: -1159.2080\n",
      "Epoch 6836/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6991 - total_train_reward: -1069.7261\n",
      "Epoch 6837/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8531 - total_train_reward: -1331.0485\n",
      "Epoch 6838/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9987 - total_train_reward: -1010.5550\n",
      "Epoch 6839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5556 - total_train_reward: -1312.9545\n",
      "Epoch 6840/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5069 - total_train_reward: -847.9501\n",
      "Epoch 6841/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5000 - total_train_reward: -1549.6091\n",
      "Epoch 6842/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3142 - total_train_reward: -1103.1034\n",
      "Epoch 6843/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6247 - total_train_reward: -1195.3984\n",
      "Epoch 6844/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2097 - total_train_reward: -772.1830\n",
      "Epoch 6845/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6302 - total_train_reward: -1074.9113\n",
      "Epoch 6846/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5098 - total_train_reward: -1172.3974\n",
      "Epoch 6847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4455 - total_train_reward: -1071.6660\n",
      "Epoch 6848/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4848 - total_train_reward: -1742.0488\n",
      "Epoch 6849/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6793 - total_train_reward: -1254.5195\n",
      "Epoch 6850/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1433 - total_train_reward: -1189.6767\n",
      "Epoch 6851/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0763 - total_train_reward: -1048.3887\n",
      "Epoch 6852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3098 - total_train_reward: -834.0700\n",
      "Epoch 6853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2469 - total_train_reward: -1461.9430\n",
      "Epoch 6854/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7328 - total_train_reward: -1618.4107\n",
      "Epoch 6855/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7225 - total_train_reward: -1159.8333\n",
      "Epoch 6856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9162 - total_train_reward: -1388.8016\n",
      "Epoch 6857/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8367 - total_train_reward: -1582.2982\n",
      "Epoch 6858/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4497 - total_train_reward: -863.6275\n",
      "Epoch 6859/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5514 - total_train_reward: -1011.2426\n",
      "Epoch 6860/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2846 - total_train_reward: -1845.2983\n",
      "Epoch 6861/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6842 - total_train_reward: -983.9109\n",
      "Epoch 6862/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3161 - total_train_reward: -1157.1585\n",
      "Epoch 6863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5020 - total_train_reward: -1670.3129\n",
      "Epoch 6864/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.6332 - total_train_reward: -1198.6585\n",
      "Epoch 6865/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2743 - total_train_reward: -1355.0931\n",
      "Epoch 6866/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2116 - total_train_reward: -960.2860\n",
      "Epoch 6867/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2683 - total_train_reward: -1201.9802\n",
      "Epoch 6868/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1293 - total_train_reward: -1423.4570\n",
      "Epoch 6869/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4960 - total_train_reward: -1252.6006\n",
      "Epoch 6870/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0405 - total_train_reward: -1725.6681\n",
      "Epoch 6871/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5603 - total_train_reward: -1217.9658\n",
      "Epoch 6872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0594 - total_train_reward: -1005.7186\n",
      "Epoch 6873/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7064 - total_train_reward: -1074.6036\n",
      "Epoch 6874/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7312 - total_train_reward: -732.2130\n",
      "Epoch 6875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1107 - total_train_reward: -1786.9305\n",
      "Epoch 6876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3744 - total_train_reward: -1249.0491\n",
      "Epoch 6877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6153 - total_train_reward: -1152.4035\n",
      "Epoch 6878/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7538 - total_train_reward: -854.4825\n",
      "Epoch 6879/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6612 - total_train_reward: -929.6595\n",
      "Epoch 6880/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0860 - total_train_reward: -1237.0377\n",
      "Epoch 6881/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3556 - total_train_reward: -854.7181\n",
      "Epoch 6882/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9744 - total_train_reward: -1183.5218\n",
      "Epoch 6883/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6143 - total_train_reward: -1052.6000\n",
      "Epoch 6884/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0820 - total_train_reward: -1422.1594\n",
      "Epoch 6885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.4114 - total_train_reward: -1202.8663\n",
      "Epoch 6886/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9579 - total_train_reward: -1716.7454\n",
      "Epoch 6887/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2097 - total_train_reward: -1156.3690\n",
      "Epoch 6888/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2399 - total_train_reward: -1484.0075\n",
      "Epoch 6889/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3566 - total_train_reward: -1167.6300\n",
      "Epoch 6890/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7274 - total_train_reward: -1621.0793\n",
      "Epoch 6891/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8522 - total_train_reward: -1796.2671\n",
      "Epoch 6892/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7589 - total_train_reward: -1063.4003\n",
      "Epoch 6893/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0819 - total_train_reward: -1725.2551\n",
      "Epoch 6894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4221 - total_train_reward: -1056.9759\n",
      "Epoch 6895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0727 - total_train_reward: -1113.2651\n",
      "Epoch 6896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9669 - total_train_reward: -1689.4823\n",
      "Epoch 6897/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6166 - total_train_reward: -1198.8164\n",
      "Epoch 6898/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0711 - total_train_reward: -1198.9555\n",
      "Epoch 6899/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2384 - total_train_reward: -1041.4205\n",
      "Epoch 6900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6048 - total_train_reward: -1056.7836\n",
      "Epoch 6901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5620 - total_train_reward: -1098.1210\n",
      "Epoch 6902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1275 - total_train_reward: -1207.5170\n",
      "Epoch 6903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3787 - total_train_reward: -1839.4334\n",
      "Epoch 6904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3900 - total_train_reward: -1199.1566\n",
      "Epoch 6905/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6471 - total_train_reward: -1176.8819\n",
      "Epoch 6906/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6491 - total_train_reward: -1310.3665\n",
      "Epoch 6907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8718 - total_train_reward: -756.4199\n",
      "Epoch 6908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3269 - total_train_reward: -1063.8925\n",
      "Epoch 6909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1260 - total_train_reward: -1776.4506\n",
      "Epoch 6910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5955 - total_train_reward: -1211.8783\n",
      "Epoch 6911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2937 - total_train_reward: -1369.5895\n",
      "Epoch 6912/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7879 - total_train_reward: -1201.8966\n",
      "Epoch 6913/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7261 - total_train_reward: -1799.4374\n",
      "Epoch 6914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5336 - total_train_reward: -1137.9543\n",
      "Epoch 6915/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0460 - total_train_reward: -1447.1321\n",
      "Epoch 6916/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7893 - total_train_reward: -1044.9426\n",
      "Epoch 6917/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6167 - total_train_reward: -1321.4874\n",
      "Epoch 6918/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1463 - total_train_reward: -846.5050\n",
      "Epoch 6919/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.0438 - total_train_reward: -1208.9326\n",
      "Epoch 6920/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1222 - total_train_reward: -852.5289\n",
      "Epoch 6921/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4653 - total_train_reward: -1064.6325\n",
      "Epoch 6922/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3311 - total_train_reward: -1393.4273\n",
      "Epoch 6923/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8431 - total_train_reward: -1171.8193\n",
      "Epoch 6924/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5377 - total_train_reward: -1256.0334\n",
      "Epoch 6925/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2843 - total_train_reward: -1747.5733\n",
      "Epoch 6926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5338 - total_train_reward: -1251.1110\n",
      "Epoch 6927/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2645 - total_train_reward: -1214.0304\n",
      "Epoch 6928/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8616 - total_train_reward: -1208.1647\n",
      "Epoch 6929/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8114 - total_train_reward: -1576.2411\n",
      "Epoch 6930/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8730 - total_train_reward: -1176.9970\n",
      "Epoch 6931/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0937 - total_train_reward: -1047.2269\n",
      "Epoch 6932/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1340 - total_train_reward: -1521.4141\n",
      "Epoch 6933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2238 - total_train_reward: -1198.9556\n",
      "Epoch 6934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.5518 - total_train_reward: -995.7912\n",
      "Epoch 6935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5806 - total_train_reward: -1391.7629\n",
      "Epoch 6936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2974 - total_train_reward: -1251.0831\n",
      "Epoch 6937/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5806 - total_train_reward: -744.8377\n",
      "Epoch 6938/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2585 - total_train_reward: -1168.7204\n",
      "Epoch 6939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0447 - total_train_reward: -1630.9430\n",
      "Epoch 6940/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5632 - total_train_reward: -1068.0754\n",
      "Epoch 6941/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5011 - total_train_reward: -1330.2127\n",
      "Epoch 6942/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0326 - total_train_reward: -1209.4527\n",
      "Epoch 6943/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1986 - total_train_reward: -1835.2357\n",
      "Epoch 6944/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1946 - total_train_reward: -1214.2765\n",
      "Epoch 6945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8559 - total_train_reward: -1210.2728\n",
      "Epoch 6946/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2177 - total_train_reward: -1600.8962\n",
      "Epoch 6947/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9463 - total_train_reward: -1216.6414\n",
      "Epoch 6948/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8012 - total_train_reward: -1449.0853\n",
      "Epoch 6949/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7784 - total_train_reward: -1156.0861\n",
      "Epoch 6950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9809 - total_train_reward: -1213.6660\n",
      "Epoch 6951/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1780 - total_train_reward: -1065.8694\n",
      "Epoch 6952/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2433 - total_train_reward: -1584.3686\n",
      "Epoch 6953/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3444 - total_train_reward: -1647.6370\n",
      "Epoch 6954/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3382 - total_train_reward: -1413.2105\n",
      "Epoch 6955/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6531 - total_train_reward: -1064.4852\n",
      "Epoch 6956/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8466 - total_train_reward: -1509.6685\n",
      "Epoch 6957/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3172 - total_train_reward: -1229.0834\n",
      "Epoch 6958/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8210 - total_train_reward: -1126.7100\n",
      "Epoch 6959/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9405 - total_train_reward: -1785.3507\n",
      "Epoch 6960/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8778 - total_train_reward: -1833.2208\n",
      "Epoch 6961/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4762 - total_train_reward: -1778.2490\n",
      "Epoch 6962/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0230 - total_train_reward: -1153.8795\n",
      "Epoch 6963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0154 - total_train_reward: -1831.1748\n",
      "Epoch 6964/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2405 - total_train_reward: -1141.3829\n",
      "Epoch 6965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4058 - total_train_reward: -1212.1204\n",
      "Epoch 6966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5306 - total_train_reward: -1378.7221\n",
      "Epoch 6967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8745 - total_train_reward: -1527.1792\n",
      "Epoch 6968/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5921 - total_train_reward: -1501.3475\n",
      "Epoch 6969/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1113 - total_train_reward: -958.6689\n",
      "Epoch 6970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4900 - total_train_reward: -1426.6182\n",
      "Epoch 6971/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7249 - total_train_reward: -958.2446\n",
      "Epoch 6972/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1123 - total_train_reward: -1280.9945\n",
      "Epoch 6973/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9026 - total_train_reward: -1216.6792\n",
      "Epoch 6974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2954 - total_train_reward: -1679.2440\n",
      "Epoch 6975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6895 - total_train_reward: -1447.1201\n",
      "Epoch 6976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3462 - total_train_reward: -1796.5325\n",
      "Epoch 6977/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9688 - total_train_reward: -1679.8406\n",
      "Epoch 6978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2838 - total_train_reward: -1805.2392\n",
      "Epoch 6979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4487 - total_train_reward: -1354.1737\n",
      "Epoch 6980/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9277 - total_train_reward: -1099.1690\n",
      "Epoch 6981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2896 - total_train_reward: -1440.0648\n",
      "Epoch 6982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7568 - total_train_reward: -958.3176\n",
      "Epoch 6983/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8873 - total_train_reward: -1162.9404\n",
      "Epoch 6984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7224 - total_train_reward: -1742.8672\n",
      "Epoch 6985/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5955 - total_train_reward: -1791.3312\n",
      "Epoch 6986/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2881 - total_train_reward: -1183.6656\n",
      "Epoch 6987/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2909 - total_train_reward: -1398.4353\n",
      "Epoch 6988/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2716 - total_train_reward: -726.9036\n",
      "Epoch 6989/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6669 - total_train_reward: -1197.2203\n",
      "Epoch 6990/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6281 - total_train_reward: -1572.6056\n",
      "Epoch 6991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9395 - total_train_reward: -1639.2430\n",
      "Epoch 6992/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4946 - total_train_reward: -1050.7120\n",
      "Epoch 6993/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7137 - total_train_reward: -1194.7163\n",
      "Epoch 6994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5869 - total_train_reward: -1831.6022\n",
      "Epoch 6995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0096 - total_train_reward: -1411.5340\n",
      "Epoch 6996/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2294 - total_train_reward: -1372.3527\n",
      "Epoch 6997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4996 - total_train_reward: -1207.4530\n",
      "Epoch 6998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6300 - total_train_reward: -1559.7478\n",
      "Epoch 6999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5775 - total_train_reward: -1205.1255\n",
      "Epoch 7000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7091 - total_train_reward: -1499.5929\n",
      "Epoch 7001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0737 - total_train_reward: -1207.2988\n",
      "Epoch 7002/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9379 - total_train_reward: -1200.8524\n",
      "Epoch 7003/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8688 - total_train_reward: -1529.6915\n",
      "Epoch 7004/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5273 - total_train_reward: -970.7193\n",
      "Epoch 7005/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9903 - total_train_reward: -1204.7355\n",
      "Epoch 7006/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4144 - total_train_reward: -1231.8002\n",
      "Epoch 7007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 28.2251 - total_train_reward: -911.9540\n",
      "Epoch 7008/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.3404 - total_train_reward: -1512.0482\n",
      "Epoch 7009/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4441 - total_train_reward: -1171.4141\n",
      "Epoch 7010/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.4930 - total_train_reward: -1232.1621\n",
      "Epoch 7011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8030 - total_train_reward: -1176.7214\n",
      "Epoch 7012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7934 - total_train_reward: -1219.9540\n",
      "Epoch 7013/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0966 - total_train_reward: -1066.3031\n",
      "Epoch 7014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2771 - total_train_reward: -1259.3220\n",
      "Epoch 7015/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1326 - total_train_reward: -1012.1927\n",
      "Epoch 7016/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9830 - total_train_reward: -1224.6287\n",
      "Epoch 7017/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9078 - total_train_reward: -957.3376\n",
      "Epoch 7018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.6356 - total_train_reward: -1776.9153\n",
      "Epoch 7019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0585 - total_train_reward: -1304.0445\n",
      "Epoch 7020/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3651 - total_train_reward: -1211.3737\n",
      "Epoch 7021/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5317 - total_train_reward: -1083.4691\n",
      "Epoch 7022/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4339 - total_train_reward: -1065.6028\n",
      "Epoch 7023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1849 - total_train_reward: -1397.9426\n",
      "Epoch 7024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4835 - total_train_reward: -1206.5620\n",
      "Epoch 7025/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8931 - total_train_reward: -973.8625\n",
      "Epoch 7026/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6943 - total_train_reward: -1068.6338\n",
      "Epoch 7027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2318 - total_train_reward: -993.3931\n",
      "Epoch 7028/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0097 - total_train_reward: -1248.1674\n",
      "Epoch 7029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0763 - total_train_reward: -1282.5837\n",
      "Epoch 7030/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1086 - total_train_reward: -1541.7223\n",
      "Epoch 7031/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0363 - total_train_reward: -1065.6930\n",
      "Epoch 7032/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0204 - total_train_reward: -1452.2718\n",
      "Epoch 7033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5660 - total_train_reward: -1030.0609\n",
      "Epoch 7034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9712 - total_train_reward: -1369.7421\n",
      "Epoch 7035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2759 - total_train_reward: -1695.3018\n",
      "Epoch 7036/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6275 - total_train_reward: -1803.6415\n",
      "Epoch 7037/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5863 - total_train_reward: -1532.8231\n",
      "Epoch 7038/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0429 - total_train_reward: -1227.1713\n",
      "Epoch 7039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6911 - total_train_reward: -1206.6628\n",
      "Epoch 7040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3095 - total_train_reward: -1057.3882\n",
      "Epoch 7041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0123 - total_train_reward: -1200.6020\n",
      "Epoch 7042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3798 - total_train_reward: -1174.2615\n",
      "Epoch 7043/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0214 - total_train_reward: -1640.8300\n",
      "Epoch 7044/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3520 - total_train_reward: -1233.5114\n",
      "Epoch 7045/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8591 - total_train_reward: -1170.1646\n",
      "Epoch 7046/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0274 - total_train_reward: -1550.9671\n",
      "Epoch 7047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5608 - total_train_reward: -1268.0176\n",
      "Epoch 7048/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8469 - total_train_reward: -1163.5239\n",
      "Epoch 7049/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9813 - total_train_reward: -1204.3567\n",
      "Epoch 7050/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4815 - total_train_reward: -1519.1527\n",
      "Epoch 7051/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9578 - total_train_reward: -1062.0828\n",
      "Epoch 7052/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2653 - total_train_reward: -1178.1995\n",
      "Epoch 7053/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7354 - total_train_reward: -1539.2312\n",
      "Epoch 7054/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4715 - total_train_reward: -1385.0922\n",
      "Epoch 7055/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8633 - total_train_reward: -930.9673\n",
      "Epoch 7056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1091 - total_train_reward: -1238.7732\n",
      "Epoch 7057/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8989 - total_train_reward: -1164.7698\n",
      "Epoch 7058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9140 - total_train_reward: -1246.4324\n",
      "Epoch 7059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1501 - total_train_reward: -1065.9810\n",
      "Epoch 7060/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9684 - total_train_reward: -1229.5614\n",
      "Epoch 7061/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8646 - total_train_reward: -1545.2881\n",
      "Epoch 7062/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9221 - total_train_reward: -1237.1234\n",
      "Epoch 7063/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6070 - total_train_reward: -1822.1567\n",
      "Epoch 7064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3919 - total_train_reward: -817.1361\n",
      "Epoch 7065/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0431 - total_train_reward: -1785.0628\n",
      "Epoch 7066/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6042 - total_train_reward: -957.6728\n",
      "Epoch 7067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7279 - total_train_reward: -1299.7473\n",
      "Epoch 7068/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7929 - total_train_reward: -1086.1539\n",
      "Epoch 7069/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7975 - total_train_reward: -1053.6604\n",
      "Epoch 7070/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6696 - total_train_reward: -957.5591\n",
      "Epoch 7071/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7835 - total_train_reward: -1719.2176\n",
      "Epoch 7072/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1295 - total_train_reward: -1828.6253\n",
      "Epoch 7073/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5122 - total_train_reward: -1427.3596\n",
      "Epoch 7074/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6450 - total_train_reward: -958.9578\n",
      "Epoch 7075/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3542 - total_train_reward: -1225.4682\n",
      "Epoch 7076/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1689 - total_train_reward: -1258.7388\n",
      "Epoch 7077/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6735 - total_train_reward: -1613.8338\n",
      "Epoch 7078/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7031 - total_train_reward: -1661.1880\n",
      "Epoch 7079/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4181 - total_train_reward: -1779.4252\n",
      "Epoch 7080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7405 - total_train_reward: -949.3490\n",
      "Epoch 7081/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0079 - total_train_reward: -1304.8923\n",
      "Epoch 7082/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4857 - total_train_reward: -1066.0127\n",
      "Epoch 7083/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1360 - total_train_reward: -1221.0665\n",
      "Epoch 7084/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7432 - total_train_reward: -1723.2730\n",
      "Epoch 7085/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4056 - total_train_reward: -957.2719\n",
      "Epoch 7086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5442 - total_train_reward: -1170.9157\n",
      "Epoch 7087/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2108 - total_train_reward: -1416.2849\n",
      "Epoch 7088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4888 - total_train_reward: -1531.1538\n",
      "Epoch 7089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0533 - total_train_reward: -1228.3049\n",
      "Epoch 7090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2133 - total_train_reward: -1260.5352\n",
      "Epoch 7091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4930 - total_train_reward: -726.4089\n",
      "Epoch 7092/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1904 - total_train_reward: -1411.8059\n",
      "Epoch 7093/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.9109 - total_train_reward: -1055.2017\n",
      "Epoch 7094/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3779 - total_train_reward: -1215.8629\n",
      "Epoch 7095/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2599 - total_train_reward: -1230.1096\n",
      "Epoch 7096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7030 - total_train_reward: -1457.0662\n",
      "Epoch 7097/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9556 - total_train_reward: -1133.9975\n",
      "Epoch 7098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2011 - total_train_reward: -1571.9969\n",
      "Epoch 7099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5317 - total_train_reward: -1381.7311\n",
      "Epoch 7100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9670 - total_train_reward: -958.9502\n",
      "Epoch 7101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8574 - total_train_reward: -1216.6154\n",
      "Epoch 7102/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6305 - total_train_reward: -1569.1271\n",
      "Epoch 7103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2849 - total_train_reward: -1682.8627\n",
      "Epoch 7104/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9691 - total_train_reward: -1304.2653\n",
      "Epoch 7105/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1479 - total_train_reward: -1326.0288\n",
      "Epoch 7106/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2656 - total_train_reward: -1216.6136\n",
      "Epoch 7107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1731 - total_train_reward: -1176.3965\n",
      "Epoch 7108/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0796 - total_train_reward: -1193.5115\n",
      "Epoch 7109/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2020 - total_train_reward: -1179.5634\n",
      "Epoch 7110/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1174 - total_train_reward: -1832.0075\n",
      "Epoch 7111/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5560 - total_train_reward: -1215.7812\n",
      "Epoch 7112/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3982 - total_train_reward: -1154.4115\n",
      "Epoch 7113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4831 - total_train_reward: -1756.2765\n",
      "Epoch 7114/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1228 - total_train_reward: -1828.1683\n",
      "Epoch 7115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9470 - total_train_reward: -1206.0668\n",
      "Epoch 7116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.3272 - total_train_reward: -1622.9225\n",
      "Epoch 7117/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5619 - total_train_reward: -958.0918\n",
      "Epoch 7118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1845 - total_train_reward: -1216.8410\n",
      "Epoch 7119/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3332 - total_train_reward: -1216.3500\n",
      "Epoch 7120/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3475 - total_train_reward: -1459.5211\n",
      "Epoch 7121/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0775 - total_train_reward: -1243.5473\n",
      "Epoch 7122/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2020 - total_train_reward: -1212.7306\n",
      "Epoch 7123/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5150 - total_train_reward: -1552.6265\n",
      "Epoch 7124/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3202 - total_train_reward: -1379.9943\n",
      "Epoch 7125/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3290 - total_train_reward: -1484.0598\n",
      "Epoch 7126/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6969 - total_train_reward: -1489.7309\n",
      "Epoch 7127/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2556 - total_train_reward: -1505.5612\n",
      "Epoch 7128/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.4056 - total_train_reward: -1150.5135\n",
      "Epoch 7129/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2404 - total_train_reward: -1130.8809\n",
      "Epoch 7130/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1509 - total_train_reward: -1215.3834\n",
      "Epoch 7131/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7089 - total_train_reward: -984.8293\n",
      "Epoch 7132/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3817 - total_train_reward: -1422.4618\n",
      "Epoch 7133/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5110 - total_train_reward: -1238.0351\n",
      "Epoch 7134/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6966 - total_train_reward: -1777.9693\n",
      "Epoch 7135/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9941 - total_train_reward: -1276.9159\n",
      "Epoch 7136/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3993 - total_train_reward: -957.3524\n",
      "Epoch 7137/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3863 - total_train_reward: -1626.4055\n",
      "Epoch 7138/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0920 - total_train_reward: -1274.2799\n",
      "Epoch 7139/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1476 - total_train_reward: -1272.4662\n",
      "Epoch 7140/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4147 - total_train_reward: -1366.0511\n",
      "Epoch 7141/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8526 - total_train_reward: -1494.7059\n",
      "Epoch 7142/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9597 - total_train_reward: -763.8514\n",
      "Epoch 7143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1037 - total_train_reward: -974.6753\n",
      "Epoch 7144/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0679 - total_train_reward: -1802.6763\n",
      "Epoch 7145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0987 - total_train_reward: -1202.0935\n",
      "Epoch 7146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4973 - total_train_reward: -964.6985\n",
      "Epoch 7147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1888 - total_train_reward: -1205.3564\n",
      "Epoch 7148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7084 - total_train_reward: -1253.8184\n",
      "Epoch 7149/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4865 - total_train_reward: -962.6304\n",
      "Epoch 7150/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0294 - total_train_reward: -1668.7543\n",
      "Epoch 7151/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.9065 - total_train_reward: -1031.1728\n",
      "Epoch 7152/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9106 - total_train_reward: -1314.5572\n",
      "Epoch 7153/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9803 - total_train_reward: -1242.5147\n",
      "Epoch 7154/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7130 - total_train_reward: -1209.0139\n",
      "Epoch 7155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5169 - total_train_reward: -1735.8842\n",
      "Epoch 7156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3108 - total_train_reward: -1160.4723\n",
      "Epoch 7157/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2949 - total_train_reward: -1653.5768\n",
      "Epoch 7158/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4048 - total_train_reward: -1534.3538\n",
      "Epoch 7159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0311 - total_train_reward: -1370.4097\n",
      "Epoch 7160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7227 - total_train_reward: -1275.8779\n",
      "Epoch 7161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.0747 - total_train_reward: -1184.9428\n",
      "Epoch 7162/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9936 - total_train_reward: -1197.6374\n",
      "Epoch 7163/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0629 - total_train_reward: -1244.5437\n",
      "Epoch 7164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7380 - total_train_reward: -1660.9948\n",
      "Epoch 7165/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3247 - total_train_reward: -1229.7425\n",
      "Epoch 7166/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9394 - total_train_reward: -859.4423\n",
      "Epoch 7167/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8320 - total_train_reward: -1331.7185\n",
      "Epoch 7168/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9543 - total_train_reward: -1784.4470\n",
      "Epoch 7169/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2676 - total_train_reward: -1738.3186\n",
      "Epoch 7170/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5799 - total_train_reward: -1445.3601\n",
      "Epoch 7171/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1310 - total_train_reward: -1636.9695\n",
      "Epoch 7172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6729 - total_train_reward: -1012.2409\n",
      "Epoch 7173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1490 - total_train_reward: -1077.9187\n",
      "Epoch 7174/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4635 - total_train_reward: -1063.9087\n",
      "Epoch 7175/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9106 - total_train_reward: -908.5643\n",
      "Epoch 7176/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3370 - total_train_reward: -1172.7132\n",
      "Epoch 7177/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5237 - total_train_reward: -1786.8395\n",
      "Epoch 7178/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8488 - total_train_reward: -1729.5459\n",
      "Epoch 7179/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6208 - total_train_reward: -1066.9396\n",
      "Epoch 7180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6569 - total_train_reward: -1530.0045\n",
      "Epoch 7181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2114 - total_train_reward: -958.6208\n",
      "Epoch 7182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7715 - total_train_reward: -1480.0100\n",
      "Epoch 7183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1001 - total_train_reward: -1679.1303\n",
      "Epoch 7184/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1274 - total_train_reward: -1829.4629\n",
      "Epoch 7185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5988 - total_train_reward: -1769.4985\n",
      "Epoch 7186/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5484 - total_train_reward: -1214.5664\n",
      "Epoch 7187/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4807 - total_train_reward: -1216.6918\n",
      "Epoch 7188/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2968 - total_train_reward: -1294.9319\n",
      "Epoch 7189/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7445 - total_train_reward: -1223.8163\n",
      "Epoch 7190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5690 - total_train_reward: -1795.7878\n",
      "Epoch 7191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8027 - total_train_reward: -1218.6244\n",
      "Epoch 7192/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9318 - total_train_reward: -1291.7052\n",
      "Epoch 7193/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4522 - total_train_reward: -1053.4496\n",
      "Epoch 7194/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3161 - total_train_reward: -1293.2504\n",
      "Epoch 7195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1146 - total_train_reward: -1557.2810\n",
      "Epoch 7196/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7071 - total_train_reward: -1229.4988\n",
      "Epoch 7197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6015 - total_train_reward: -1153.6754\n",
      "Epoch 7198/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1579 - total_train_reward: -1501.1566\n",
      "Epoch 7199/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9950 - total_train_reward: -1176.5755\n",
      "Epoch 7200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9689 - total_train_reward: -1499.9564\n",
      "Epoch 7201/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2078 - total_train_reward: -1187.6427\n",
      "Epoch 7202/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4258 - total_train_reward: -1583.0443\n",
      "Epoch 7203/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4473 - total_train_reward: -1209.4071\n",
      "Epoch 7204/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2865 - total_train_reward: -1194.0808\n",
      "Epoch 7205/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9377 - total_train_reward: -725.9697\n",
      "Epoch 7206/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1875 - total_train_reward: -1770.7956\n",
      "Epoch 7207/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1484 - total_train_reward: -1537.1794\n",
      "Epoch 7208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6680 - total_train_reward: -1166.3000\n",
      "Epoch 7209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7890 - total_train_reward: -1061.0366\n",
      "Epoch 7210/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3006 - total_train_reward: -842.0103\n",
      "Epoch 7211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8748 - total_train_reward: -1830.2102\n",
      "Epoch 7212/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0091 - total_train_reward: -1262.6784\n",
      "Epoch 7213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9251 - total_train_reward: -1711.2013\n",
      "Epoch 7214/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6433 - total_train_reward: -1174.8451\n",
      "Epoch 7215/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.9294 - total_train_reward: -1219.8689\n",
      "Epoch 7216/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7417 - total_train_reward: -1238.8219\n",
      "Epoch 7217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1015 - total_train_reward: -1692.8660\n",
      "Epoch 7218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5790 - total_train_reward: -1288.4105\n",
      "Epoch 7219/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2473 - total_train_reward: -1212.5915\n",
      "Epoch 7220/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3463 - total_train_reward: -1233.2605\n",
      "Epoch 7221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8996 - total_train_reward: -1486.6793\n",
      "Epoch 7222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2051 - total_train_reward: -1073.0380\n",
      "Epoch 7223/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1114 - total_train_reward: -1276.4544\n",
      "Epoch 7224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9318 - total_train_reward: -1235.5729\n",
      "Epoch 7225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7148 - total_train_reward: -1031.4144\n",
      "Epoch 7226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6092 - total_train_reward: -1171.9155\n",
      "Epoch 7227/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8297 - total_train_reward: -1332.9157\n",
      "Epoch 7228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2917 - total_train_reward: -1337.8229\n",
      "Epoch 7229/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9509 - total_train_reward: -1389.1705\n",
      "Epoch 7230/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6874 - total_train_reward: -960.7575\n",
      "Epoch 7231/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5739 - total_train_reward: -1205.1773\n",
      "Epoch 7232/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7032 - total_train_reward: -1213.7142\n",
      "Epoch 7233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0186 - total_train_reward: -1815.1098\n",
      "Epoch 7234/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3916 - total_train_reward: -839.9808\n",
      "Epoch 7235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8638 - total_train_reward: -1228.3505\n",
      "Epoch 7236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8196 - total_train_reward: -1296.0329\n",
      "Epoch 7237/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7310 - total_train_reward: -1534.8194\n",
      "Epoch 7238/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6841 - total_train_reward: -1264.3872\n",
      "Epoch 7239/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5316 - total_train_reward: -1738.5364\n",
      "Epoch 7240/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0684 - total_train_reward: -1083.9435\n",
      "Epoch 7241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6837 - total_train_reward: -1460.1991\n",
      "Epoch 7242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2924 - total_train_reward: -1482.7928\n",
      "Epoch 7243/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1112 - total_train_reward: -1696.5746\n",
      "Epoch 7244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8724 - total_train_reward: -977.1636\n",
      "Epoch 7245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4382 - total_train_reward: -1233.3372\n",
      "Epoch 7246/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7730 - total_train_reward: -1383.3502\n",
      "Epoch 7247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5046 - total_train_reward: -1669.3560\n",
      "Epoch 7248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6024 - total_train_reward: -1296.6711\n",
      "Epoch 7249/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4268 - total_train_reward: -1234.9070\n",
      "Epoch 7250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7816 - total_train_reward: -1315.3094\n",
      "Epoch 7251/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4063 - total_train_reward: -1082.8992\n",
      "Epoch 7252/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8997 - total_train_reward: -1221.9355\n",
      "Epoch 7253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1251 - total_train_reward: -1771.3617\n",
      "Epoch 7254/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0610 - total_train_reward: -1506.9475\n",
      "Epoch 7255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1063 - total_train_reward: -1194.1867\n",
      "Epoch 7256/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8304 - total_train_reward: -1055.5328\n",
      "Epoch 7257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8458 - total_train_reward: -1583.7195\n",
      "Epoch 7258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5756 - total_train_reward: -1749.8787\n",
      "Epoch 7259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6756 - total_train_reward: -1656.8668\n",
      "Epoch 7260/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2695 - total_train_reward: -1273.7570\n",
      "Epoch 7261/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8419 - total_train_reward: -1201.6209\n",
      "Epoch 7262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1006 - total_train_reward: -1300.1785\n",
      "Epoch 7263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2654 - total_train_reward: -1349.4497\n",
      "Epoch 7264/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4940 - total_train_reward: -1238.3898\n",
      "Epoch 7265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8487 - total_train_reward: -1447.4740\n",
      "Epoch 7266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0081 - total_train_reward: -1306.9126\n",
      "Epoch 7267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1393 - total_train_reward: -961.7721\n",
      "Epoch 7268/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7793 - total_train_reward: -1678.8865\n",
      "Epoch 7269/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3687 - total_train_reward: -1219.6560\n",
      "Epoch 7270/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2385 - total_train_reward: -1073.1785\n",
      "Epoch 7271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6153 - total_train_reward: -1432.6854\n",
      "Epoch 7272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0118 - total_train_reward: -932.9608\n",
      "Epoch 7273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7516 - total_train_reward: -1224.8960\n",
      "Epoch 7274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9623 - total_train_reward: -1223.6781\n",
      "Epoch 7275/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1728 - total_train_reward: -1492.5606\n",
      "Epoch 7276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1784 - total_train_reward: -1064.4286\n",
      "Epoch 7277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6236 - total_train_reward: -1406.5692\n",
      "Epoch 7278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4733 - total_train_reward: -1167.8903\n",
      "Epoch 7279/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0438 - total_train_reward: -1511.6783\n",
      "Epoch 7280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0813 - total_train_reward: -1272.5298\n",
      "Epoch 7281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9097 - total_train_reward: -1326.1993\n",
      "Epoch 7282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7585 - total_train_reward: -1246.5064\n",
      "Epoch 7283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9982 - total_train_reward: -1822.0350\n",
      "Epoch 7284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5174 - total_train_reward: -1399.9302\n",
      "Epoch 7285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3648 - total_train_reward: -1137.5487\n",
      "Epoch 7286/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0543 - total_train_reward: -1616.3326\n",
      "Epoch 7287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8679 - total_train_reward: -1773.8150\n",
      "Epoch 7288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4336 - total_train_reward: -1474.8556\n",
      "Epoch 7289/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9329 - total_train_reward: -1273.4762\n",
      "Epoch 7290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8116 - total_train_reward: -725.9018\n",
      "Epoch 7291/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4799 - total_train_reward: -1814.9561\n",
      "Epoch 7292/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4695 - total_train_reward: -1128.6363\n",
      "Epoch 7293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.1245 - total_train_reward: -1224.2528\n",
      "Epoch 7294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.3764 - total_train_reward: -1603.6411\n",
      "Epoch 7295/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5753 - total_train_reward: -1637.5694\n",
      "Epoch 7296/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8448 - total_train_reward: -1744.5511\n",
      "Epoch 7297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3290 - total_train_reward: -1486.6705\n",
      "Epoch 7298/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8416 - total_train_reward: -1236.3271\n",
      "Epoch 7299/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1507 - total_train_reward: -1180.5110\n",
      "Epoch 7300/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7855 - total_train_reward: -1646.7946\n",
      "Epoch 7301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2660 - total_train_reward: -1062.6248\n",
      "Epoch 7302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4593 - total_train_reward: -1652.2690\n",
      "Epoch 7303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1460 - total_train_reward: -1161.8120\n",
      "Epoch 7304/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7345 - total_train_reward: -944.0639\n",
      "Epoch 7305/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4226 - total_train_reward: -1290.2171\n",
      "Epoch 7306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6030 - total_train_reward: -1210.4341\n",
      "Epoch 7307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8377 - total_train_reward: -1116.4610\n",
      "Epoch 7308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6668 - total_train_reward: -1209.3670\n",
      "Epoch 7309/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5989 - total_train_reward: -1326.8937\n",
      "Epoch 7310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7664 - total_train_reward: -1283.2563\n",
      "Epoch 7311/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9205 - total_train_reward: -893.3729\n",
      "Epoch 7312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5069 - total_train_reward: -1591.2739\n",
      "Epoch 7313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6839 - total_train_reward: -1218.1082\n",
      "Epoch 7314/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7261 - total_train_reward: -1553.9751\n",
      "Epoch 7315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6970 - total_train_reward: -1367.1132\n",
      "Epoch 7316/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2543 - total_train_reward: -1197.5146\n",
      "Epoch 7317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8473 - total_train_reward: -1681.4377\n",
      "Epoch 7318/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9745 - total_train_reward: -1378.8056\n",
      "Epoch 7319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8411 - total_train_reward: -1699.4363\n",
      "Epoch 7320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1745 - total_train_reward: -1148.3967\n",
      "Epoch 7321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7127 - total_train_reward: -1488.7238\n",
      "Epoch 7322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1494 - total_train_reward: -1746.2300\n",
      "Epoch 7323/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8022 - total_train_reward: -1165.6406\n",
      "Epoch 7324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6869 - total_train_reward: -1322.2296\n",
      "Epoch 7325/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6986 - total_train_reward: -936.7252\n",
      "Epoch 7326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5233 - total_train_reward: -1260.9803\n",
      "Epoch 7327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3494 - total_train_reward: -1289.4379\n",
      "Epoch 7328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1967 - total_train_reward: -1226.8874\n",
      "Epoch 7329/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2651 - total_train_reward: -961.3389\n",
      "Epoch 7330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7009 - total_train_reward: -1726.7506\n",
      "Epoch 7331/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.3682 - total_train_reward: -1208.5184\n",
      "Epoch 7332/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8015 - total_train_reward: -1606.0585\n",
      "Epoch 7333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4099 - total_train_reward: -1497.8010\n",
      "Epoch 7334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3952 - total_train_reward: -1525.6599\n",
      "Epoch 7335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5053 - total_train_reward: -1255.8846\n",
      "Epoch 7336/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7046 - total_train_reward: -956.9095\n",
      "Epoch 7337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2244 - total_train_reward: -1356.8062\n",
      "Epoch 7338/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3809 - total_train_reward: -1214.3135\n",
      "Epoch 7339/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6952 - total_train_reward: -1237.3124\n",
      "Epoch 7340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1390 - total_train_reward: -1426.3885\n",
      "Epoch 7341/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2175 - total_train_reward: -1195.2521\n",
      "Epoch 7342/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9960 - total_train_reward: -1243.8659\n",
      "Epoch 7343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7898 - total_train_reward: -1468.4426\n",
      "Epoch 7344/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7474 - total_train_reward: -1265.1651\n",
      "Epoch 7345/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4543 - total_train_reward: -1651.2082\n",
      "Epoch 7346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6444 - total_train_reward: -1396.7957\n",
      "Epoch 7347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4923 - total_train_reward: -1699.0165\n",
      "Epoch 7348/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1824 - total_train_reward: -1465.1752\n",
      "Epoch 7349/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7150 - total_train_reward: -1258.6741\n",
      "Epoch 7350/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7218 - total_train_reward: -1221.6363\n",
      "Epoch 7351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5247 - total_train_reward: -1777.7549\n",
      "Epoch 7352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6460 - total_train_reward: -949.6081\n",
      "Epoch 7353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5110 - total_train_reward: -1648.4730\n",
      "Epoch 7354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6447 - total_train_reward: -1315.7195\n",
      "Epoch 7355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5489 - total_train_reward: -1682.0240\n",
      "Epoch 7356/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0736 - total_train_reward: -1250.3149\n",
      "Epoch 7357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3943 - total_train_reward: -1383.4986\n",
      "Epoch 7358/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8414 - total_train_reward: -1269.5521\n",
      "Epoch 7359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7001 - total_train_reward: -1558.3399\n",
      "Epoch 7360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0065 - total_train_reward: -1269.7527\n",
      "Epoch 7361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4568 - total_train_reward: -1215.7366\n",
      "Epoch 7362/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8353 - total_train_reward: -1085.3459\n",
      "Epoch 7363/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5136 - total_train_reward: -1507.0199\n",
      "Epoch 7364/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2272 - total_train_reward: -1234.1571\n",
      "Epoch 7365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1544 - total_train_reward: -1067.9874\n",
      "Epoch 7366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5884 - total_train_reward: -1257.9458\n",
      "Epoch 7367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5134 - total_train_reward: -1203.5314\n",
      "Epoch 7368/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3745 - total_train_reward: -954.6760\n",
      "Epoch 7369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1128 - total_train_reward: -1168.5829\n",
      "Epoch 7370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6810 - total_train_reward: -1581.2705\n",
      "Epoch 7371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7640 - total_train_reward: -1239.3185\n",
      "Epoch 7372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2763 - total_train_reward: -1199.9531\n",
      "Epoch 7373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2323 - total_train_reward: -1799.1362\n",
      "Epoch 7374/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1702 - total_train_reward: -1247.6125\n",
      "Epoch 7375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7230 - total_train_reward: -1372.0155\n",
      "Epoch 7376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2269 - total_train_reward: -1090.8648\n",
      "Epoch 7377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5757 - total_train_reward: -1756.6365\n",
      "Epoch 7378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1361 - total_train_reward: -1173.8201\n",
      "Epoch 7379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1482 - total_train_reward: -957.9242\n",
      "Epoch 7380/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0879 - total_train_reward: -1215.7822\n",
      "Epoch 7381/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5590 - total_train_reward: -1167.5540\n",
      "Epoch 7382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7508 - total_train_reward: -1251.3590\n",
      "Epoch 7383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9163 - total_train_reward: -939.0533\n",
      "Epoch 7384/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0852 - total_train_reward: -1285.0578\n",
      "Epoch 7385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8788 - total_train_reward: -1389.0734\n",
      "Epoch 7386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8634 - total_train_reward: -1530.1393\n",
      "Epoch 7387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1933 - total_train_reward: -1623.7085\n",
      "Epoch 7388/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4223 - total_train_reward: -955.2111\n",
      "Epoch 7389/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0673 - total_train_reward: -1546.7480\n",
      "Epoch 7390/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0958 - total_train_reward: -893.0580\n",
      "Epoch 7391/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1507 - total_train_reward: -1793.0122\n",
      "Epoch 7392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0565 - total_train_reward: -1254.8489\n",
      "Epoch 7393/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8696 - total_train_reward: -1268.3445\n",
      "Epoch 7394/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9441 - total_train_reward: -1788.6371\n",
      "Epoch 7395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0542 - total_train_reward: -1145.0251\n",
      "Epoch 7396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2744 - total_train_reward: -1453.1867\n",
      "Epoch 7397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7351 - total_train_reward: -1260.1279\n",
      "Epoch 7398/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0590 - total_train_reward: -1265.3288\n",
      "Epoch 7399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8291 - total_train_reward: -956.3523\n",
      "Epoch 7400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4715 - total_train_reward: -1072.0418\n",
      "Epoch 7401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5519 - total_train_reward: -1279.3094\n",
      "Epoch 7402/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1835 - total_train_reward: -1361.5324\n",
      "Epoch 7403/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7520 - total_train_reward: -1650.6063\n",
      "Epoch 7404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4317 - total_train_reward: -1739.1478\n",
      "Epoch 7405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3684 - total_train_reward: -1740.2733\n",
      "Epoch 7406/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5219 - total_train_reward: -1816.1026\n",
      "Epoch 7407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2275 - total_train_reward: -1320.9328\n",
      "Epoch 7408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3090 - total_train_reward: -1241.3368\n",
      "Epoch 7409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.1610 - total_train_reward: -1661.1967\n",
      "Epoch 7410/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8673 - total_train_reward: -1785.1283\n",
      "Epoch 7411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2801 - total_train_reward: -1534.7339\n",
      "Epoch 7412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7175 - total_train_reward: -1201.5528\n",
      "Epoch 7413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8597 - total_train_reward: -1584.3322\n",
      "Epoch 7414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6304 - total_train_reward: -1284.5488\n",
      "Epoch 7415/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9467 - total_train_reward: -1586.3959\n",
      "Epoch 7416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4272 - total_train_reward: -1238.1423\n",
      "Epoch 7417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1197 - total_train_reward: -1715.5775\n",
      "Epoch 7418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4931 - total_train_reward: -1040.4248\n",
      "Epoch 7419/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8903 - total_train_reward: -1337.3623\n",
      "Epoch 7420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 24.8523 - total_train_reward: -931.9404\n",
      "Epoch 7421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.8039 - total_train_reward: -1319.3502\n",
      "Epoch 7422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1641 - total_train_reward: -1167.5952\n",
      "Epoch 7423/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5779 - total_train_reward: -1664.7696\n",
      "Epoch 7424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1071 - total_train_reward: -1128.2212\n",
      "Epoch 7425/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 21.1161 - total_train_reward: -1215.0923\n",
      "Epoch 7426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1730 - total_train_reward: -1116.3729\n",
      "Epoch 7427/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5255 - total_train_reward: -1654.7969\n",
      "Epoch 7428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2190 - total_train_reward: -1444.4603\n",
      "Epoch 7429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9759 - total_train_reward: -1620.5400\n",
      "Epoch 7430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5452 - total_train_reward: -1256.3429\n",
      "Epoch 7431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9842 - total_train_reward: -1329.0036\n",
      "Epoch 7432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2044 - total_train_reward: -1523.1647\n",
      "Epoch 7433/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9621 - total_train_reward: -1246.2914\n",
      "Epoch 7434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7676 - total_train_reward: -1250.6370\n",
      "Epoch 7435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6519 - total_train_reward: -1766.8344\n",
      "Epoch 7436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5288 - total_train_reward: -1277.9738\n",
      "Epoch 7437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5308 - total_train_reward: -1650.7365\n",
      "Epoch 7438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5149 - total_train_reward: -725.1895\n",
      "Epoch 7439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4737 - total_train_reward: -1220.8663\n",
      "Epoch 7440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1820 - total_train_reward: -1270.5430\n",
      "Epoch 7441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5877 - total_train_reward: -1345.2804\n",
      "Epoch 7442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7144 - total_train_reward: -956.7397\n",
      "Epoch 7443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4181 - total_train_reward: -1457.3852\n",
      "Epoch 7444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7687 - total_train_reward: -1299.2315\n",
      "Epoch 7445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3615 - total_train_reward: -1713.4688\n",
      "Epoch 7446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2376 - total_train_reward: -1283.9256\n",
      "Epoch 7447/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8184 - total_train_reward: -1551.7950\n",
      "Epoch 7448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7755 - total_train_reward: -1814.1835\n",
      "Epoch 7449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5432 - total_train_reward: -1239.3860\n",
      "Epoch 7450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6733 - total_train_reward: -1781.9939\n",
      "Epoch 7451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5077 - total_train_reward: -1397.0585\n",
      "Epoch 7452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9847 - total_train_reward: -1557.5346\n",
      "Epoch 7453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4520 - total_train_reward: -1080.1669\n",
      "Epoch 7454/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7631 - total_train_reward: -977.7114\n",
      "Epoch 7455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9265 - total_train_reward: -1463.1583\n",
      "Epoch 7456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2343 - total_train_reward: -1233.7993\n",
      "Epoch 7457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9674 - total_train_reward: -1167.8547\n",
      "Epoch 7458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4711 - total_train_reward: -1039.2321\n",
      "Epoch 7459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6704 - total_train_reward: -1310.9213\n",
      "Epoch 7460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8209 - total_train_reward: -893.4903\n",
      "Epoch 7461/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6633 - total_train_reward: -982.3398\n",
      "Epoch 7462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9127 - total_train_reward: -1154.1149\n",
      "Epoch 7463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7332 - total_train_reward: -1141.6748\n",
      "Epoch 7464/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1037 - total_train_reward: -1326.5824\n",
      "Epoch 7465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2096 - total_train_reward: -1048.7809\n",
      "Epoch 7466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2866 - total_train_reward: -1031.8054\n",
      "Epoch 7467/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0383 - total_train_reward: -1218.6799\n",
      "Epoch 7468/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5451 - total_train_reward: -1354.1095\n",
      "Epoch 7469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2300 - total_train_reward: -1085.2357\n",
      "Epoch 7470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8425 - total_train_reward: -1420.0217\n",
      "Epoch 7471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7517 - total_train_reward: -1236.5236\n",
      "Epoch 7472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2737 - total_train_reward: -1236.5309\n",
      "Epoch 7473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2516 - total_train_reward: -1236.4940\n",
      "Epoch 7474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8130 - total_train_reward: -1533.4447\n",
      "Epoch 7475/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0968 - total_train_reward: -1084.7811\n",
      "Epoch 7476/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0282 - total_train_reward: -1332.2205\n",
      "Epoch 7477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0455 - total_train_reward: -1063.1490\n",
      "Epoch 7478/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0263 - total_train_reward: -1297.5014\n",
      "Epoch 7479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0078 - total_train_reward: -1045.9205\n",
      "Epoch 7480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1270 - total_train_reward: -1822.2573\n",
      "Epoch 7481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4546 - total_train_reward: -725.6659\n",
      "Epoch 7482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.0301 - total_train_reward: -1232.5437\n",
      "Epoch 7483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9853 - total_train_reward: -1455.9263\n",
      "Epoch 7484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1131 - total_train_reward: -1283.8990\n",
      "Epoch 7485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8003 - total_train_reward: -957.1311\n",
      "Epoch 7486/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3081 - total_train_reward: -1229.0543\n",
      "Epoch 7487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1129 - total_train_reward: -1210.8239\n",
      "Epoch 7488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5030 - total_train_reward: -1358.2886\n",
      "Epoch 7489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2614 - total_train_reward: -1166.7968\n",
      "Epoch 7490/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0713 - total_train_reward: -1813.1640\n",
      "Epoch 7491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7740 - total_train_reward: -1078.8677\n",
      "Epoch 7492/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4860 - total_train_reward: -1812.6546\n",
      "Epoch 7493/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4089 - total_train_reward: -1251.6365\n",
      "Epoch 7494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9245 - total_train_reward: -1220.9938\n",
      "Epoch 7495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4535 - total_train_reward: -1316.3022\n",
      "Epoch 7496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6808 - total_train_reward: -1225.4783\n",
      "Epoch 7497/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3714 - total_train_reward: -1200.1996\n",
      "Epoch 7498/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8121 - total_train_reward: -1551.0781\n",
      "Epoch 7499/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4958 - total_train_reward: -993.0427\n",
      "Epoch 7500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0647 - total_train_reward: -1173.0301\n",
      "Epoch 7501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2369 - total_train_reward: -957.9686\n",
      "Epoch 7502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4432 - total_train_reward: -1197.4778\n",
      "Epoch 7503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9891 - total_train_reward: -1226.4701\n",
      "Epoch 7504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1933 - total_train_reward: -1533.5975\n",
      "Epoch 7505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4052 - total_train_reward: -1764.7875\n",
      "Epoch 7506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3393 - total_train_reward: -1218.8456\n",
      "Epoch 7507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1951 - total_train_reward: -1245.0428\n",
      "Epoch 7508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5599 - total_train_reward: -1284.6482\n",
      "Epoch 7509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5092 - total_train_reward: -1044.6142\n",
      "Epoch 7510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3177 - total_train_reward: -958.0801\n",
      "Epoch 7511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3609 - total_train_reward: -1213.1909\n",
      "Epoch 7512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2842 - total_train_reward: -1210.6245\n",
      "Epoch 7513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4629 - total_train_reward: -1133.7873\n",
      "Epoch 7514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0633 - total_train_reward: -1235.3476\n",
      "Epoch 7515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2467 - total_train_reward: -940.8656\n",
      "Epoch 7516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9484 - total_train_reward: -1356.6641\n",
      "Epoch 7517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3784 - total_train_reward: -1375.5202\n",
      "Epoch 7518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0520 - total_train_reward: -1512.7169\n",
      "Epoch 7519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9343 - total_train_reward: -1243.1452\n",
      "Epoch 7520/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9723 - total_train_reward: -1281.0066\n",
      "Epoch 7521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0267 - total_train_reward: -1323.2588\n",
      "Epoch 7522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3144 - total_train_reward: -1217.8165\n",
      "Epoch 7523/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9676 - total_train_reward: -1493.6976\n",
      "Epoch 7524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9694 - total_train_reward: -1371.5774\n",
      "Epoch 7525/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1203 - total_train_reward: -1377.3200\n",
      "Epoch 7526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9035 - total_train_reward: -1712.7075\n",
      "Epoch 7527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3152 - total_train_reward: -1391.7381\n",
      "Epoch 7528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9473 - total_train_reward: -1633.7418\n",
      "Epoch 7529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3929 - total_train_reward: -1288.2325\n",
      "Epoch 7530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3679 - total_train_reward: -1735.2455\n",
      "Epoch 7531/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0664 - total_train_reward: -1222.7960\n",
      "Epoch 7532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1113 - total_train_reward: -1188.4845\n",
      "Epoch 7533/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5002 - total_train_reward: -1247.0000\n",
      "Epoch 7534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5033 - total_train_reward: -1168.4954\n",
      "Epoch 7535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7420 - total_train_reward: -1178.1927\n",
      "Epoch 7536/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8811 - total_train_reward: -1251.7299\n",
      "Epoch 7537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0616 - total_train_reward: -1224.3539\n",
      "Epoch 7538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8112 - total_train_reward: -1261.0583\n",
      "Epoch 7539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7698 - total_train_reward: -1819.3187\n",
      "Epoch 7540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.4734 - total_train_reward: -1588.7801\n",
      "Epoch 7541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1714 - total_train_reward: -1196.7901\n",
      "Epoch 7542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2662 - total_train_reward: -1695.3251\n",
      "Epoch 7543/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2061 - total_train_reward: -845.0484\n",
      "Epoch 7544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3639 - total_train_reward: -1186.0870\n",
      "Epoch 7545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5104 - total_train_reward: -845.0833\n",
      "Epoch 7546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6836 - total_train_reward: -859.5816\n",
      "Epoch 7547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5625 - total_train_reward: -1085.1650\n",
      "Epoch 7548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9280 - total_train_reward: -1404.7795\n",
      "Epoch 7549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6649 - total_train_reward: -1437.9979\n",
      "Epoch 7550/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2316 - total_train_reward: -1250.9845\n",
      "Epoch 7551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7407 - total_train_reward: -1224.5976\n",
      "Epoch 7552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9352 - total_train_reward: -1696.9156\n",
      "Epoch 7553/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2478 - total_train_reward: -1168.7323\n",
      "Epoch 7554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4134 - total_train_reward: -1070.1319\n",
      "Epoch 7555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8434 - total_train_reward: -1228.8032\n",
      "Epoch 7556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6041 - total_train_reward: -1633.2012\n",
      "Epoch 7557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2495 - total_train_reward: -1217.0312\n",
      "Epoch 7558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9223 - total_train_reward: -1595.3848\n",
      "Epoch 7559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4855 - total_train_reward: -1365.7491\n",
      "Epoch 7560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3785 - total_train_reward: -1009.7278\n",
      "Epoch 7561/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3583 - total_train_reward: -1684.9728\n",
      "Epoch 7562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1343 - total_train_reward: -1247.4124\n",
      "Epoch 7563/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8132 - total_train_reward: -1252.7373\n",
      "Epoch 7564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1852 - total_train_reward: -1073.9429\n",
      "Epoch 7565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7324 - total_train_reward: -1479.8560\n",
      "Epoch 7566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4204 - total_train_reward: -1214.4259\n",
      "Epoch 7567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8108 - total_train_reward: -1633.1603\n",
      "Epoch 7568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5427 - total_train_reward: -1426.9873\n",
      "Epoch 7569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5342 - total_train_reward: -1665.0449\n",
      "Epoch 7570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1518 - total_train_reward: -1434.3456\n",
      "Epoch 7571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.5098 - total_train_reward: -1200.6249\n",
      "Epoch 7572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8919 - total_train_reward: -1439.2984\n",
      "Epoch 7573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8274 - total_train_reward: -1237.7106\n",
      "Epoch 7574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7124 - total_train_reward: -1060.9132\n",
      "Epoch 7575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2286 - total_train_reward: -1484.0138\n",
      "Epoch 7576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1785 - total_train_reward: -953.5625\n",
      "Epoch 7577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9992 - total_train_reward: -1134.2678\n",
      "Epoch 7578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8449 - total_train_reward: -1262.3740\n",
      "Epoch 7579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1427 - total_train_reward: -1516.5516\n",
      "Epoch 7580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6499 - total_train_reward: -1696.2758\n",
      "Epoch 7581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4033 - total_train_reward: -1118.0639\n",
      "Epoch 7582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3823 - total_train_reward: -706.2769\n",
      "Epoch 7583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6411 - total_train_reward: -719.2498\n",
      "Epoch 7584/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.0173 - total_train_reward: -1203.2437\n",
      "Epoch 7585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1176 - total_train_reward: -1148.2335\n",
      "Epoch 7586/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0868 - total_train_reward: -1661.2692\n",
      "Epoch 7587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0588 - total_train_reward: -1224.4703\n",
      "Epoch 7588/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.5566 - total_train_reward: -1201.7370\n",
      "Epoch 7589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8039 - total_train_reward: -1235.5804\n",
      "Epoch 7590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.6215 - total_train_reward: -1687.1607\n",
      "Epoch 7591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2306 - total_train_reward: -956.4624\n",
      "Epoch 7592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2882 - total_train_reward: -1342.8015\n",
      "Epoch 7593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0205 - total_train_reward: -1254.1357\n",
      "Epoch 7594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4280 - total_train_reward: -1160.7986\n",
      "Epoch 7595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5233 - total_train_reward: -1008.3525\n",
      "Epoch 7596/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5016 - total_train_reward: -1270.5979\n",
      "Epoch 7597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3781 - total_train_reward: -1667.6291\n",
      "Epoch 7598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8248 - total_train_reward: -1356.3738\n",
      "Epoch 7599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6776 - total_train_reward: -1175.4714\n",
      "Epoch 7600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9840 - total_train_reward: -1029.2913\n",
      "Epoch 7601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0274 - total_train_reward: -1584.2439\n",
      "Epoch 7602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5568 - total_train_reward: -1236.3999\n",
      "Epoch 7603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9649 - total_train_reward: -1777.1361\n",
      "Epoch 7604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1390 - total_train_reward: -1256.7019\n",
      "Epoch 7605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9841 - total_train_reward: -1239.2418\n",
      "Epoch 7606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0288 - total_train_reward: -1236.5542\n",
      "Epoch 7607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3239 - total_train_reward: -903.7358\n",
      "Epoch 7608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0506 - total_train_reward: -1725.2618\n",
      "Epoch 7609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1777 - total_train_reward: -841.7593\n",
      "Epoch 7610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2748 - total_train_reward: -1774.0680\n",
      "Epoch 7611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8571 - total_train_reward: -1557.2630\n",
      "Epoch 7612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4766 - total_train_reward: -1231.3919\n",
      "Epoch 7613/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0674 - total_train_reward: -1786.0903\n",
      "Epoch 7614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9225 - total_train_reward: -1310.8382\n",
      "Epoch 7615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1398 - total_train_reward: -1519.0961\n",
      "Epoch 7616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1861 - total_train_reward: -1227.3153\n",
      "Epoch 7617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5476 - total_train_reward: -1816.7159\n",
      "Epoch 7618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4157 - total_train_reward: -1060.5986\n",
      "Epoch 7619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4116 - total_train_reward: -1062.3591\n",
      "Epoch 7620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4754 - total_train_reward: -1251.1433\n",
      "Epoch 7621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1047 - total_train_reward: -1143.1036\n",
      "Epoch 7622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4457 - total_train_reward: -1262.7426\n",
      "Epoch 7623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5181 - total_train_reward: -1646.8572\n",
      "Epoch 7624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3886 - total_train_reward: -1467.5027\n",
      "Epoch 7625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2398 - total_train_reward: -1357.0565\n",
      "Epoch 7626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1581 - total_train_reward: -1404.0891\n",
      "Epoch 7627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6634 - total_train_reward: -1229.7923\n",
      "Epoch 7628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1451 - total_train_reward: -1327.2046\n",
      "Epoch 7629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4794 - total_train_reward: -1240.3917\n",
      "Epoch 7630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6690 - total_train_reward: -1275.3825\n",
      "Epoch 7631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9497 - total_train_reward: -1169.7643\n",
      "Epoch 7632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8610 - total_train_reward: -1272.3371\n",
      "Epoch 7633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6084 - total_train_reward: -1109.7840\n",
      "Epoch 7634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0632 - total_train_reward: -1307.8447\n",
      "Epoch 7635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5613 - total_train_reward: -1707.8616\n",
      "Epoch 7636/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4533 - total_train_reward: -1272.9412\n",
      "Epoch 7637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7143 - total_train_reward: -1439.0032\n",
      "Epoch 7638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2639 - total_train_reward: -1264.9809\n",
      "Epoch 7639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4521 - total_train_reward: -1398.7091\n",
      "Epoch 7640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2243 - total_train_reward: -1116.9499\n",
      "Epoch 7641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9305 - total_train_reward: -1733.5441\n",
      "Epoch 7642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0504 - total_train_reward: -1729.1665\n",
      "Epoch 7643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0128 - total_train_reward: -1192.9519\n",
      "Epoch 7644/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9977 - total_train_reward: -1666.5909\n",
      "Epoch 7645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9003 - total_train_reward: -1331.0586\n",
      "Epoch 7646/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9272 - total_train_reward: -1242.3139\n",
      "Epoch 7647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1460 - total_train_reward: -1235.2282\n",
      "Epoch 7648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9405 - total_train_reward: -1677.0755\n",
      "Epoch 7649/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7131 - total_train_reward: -1273.5151\n",
      "Epoch 7650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1051 - total_train_reward: -1513.4350\n",
      "Epoch 7651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9370 - total_train_reward: -1271.5691\n",
      "Epoch 7652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8017 - total_train_reward: -1789.4721\n",
      "Epoch 7653/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3911 - total_train_reward: -1609.4276\n",
      "Epoch 7654/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8929 - total_train_reward: -1683.8618\n",
      "Epoch 7655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8480 - total_train_reward: -1221.0998\n",
      "Epoch 7656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2135 - total_train_reward: -1637.0937\n",
      "Epoch 7657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1443 - total_train_reward: -1228.8462\n",
      "Epoch 7658/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0072 - total_train_reward: -1257.1615\n",
      "Epoch 7659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2329 - total_train_reward: -1810.1671\n",
      "Epoch 7660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9004 - total_train_reward: -1280.1524\n",
      "Epoch 7661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6892 - total_train_reward: -1441.4553\n",
      "Epoch 7662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2178 - total_train_reward: -961.5129\n",
      "Epoch 7663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3653 - total_train_reward: -1500.6125\n",
      "Epoch 7664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6636 - total_train_reward: -1588.8159\n",
      "Epoch 7665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0805 - total_train_reward: -1669.2447\n",
      "Epoch 7666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3875 - total_train_reward: -976.2436\n",
      "Epoch 7667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9576 - total_train_reward: -1122.8818\n",
      "Epoch 7668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4191 - total_train_reward: -1770.3502\n",
      "Epoch 7669/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3942 - total_train_reward: -1279.3247\n",
      "Epoch 7670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7314 - total_train_reward: -1257.4021\n",
      "Epoch 7671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2379 - total_train_reward: -1275.9617\n",
      "Epoch 7672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3827 - total_train_reward: -1700.7770\n",
      "Epoch 7673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3160 - total_train_reward: -1080.9896\n",
      "Epoch 7674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5423 - total_train_reward: -1073.3924\n",
      "Epoch 7675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0212 - total_train_reward: -1244.7228\n",
      "Epoch 7676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5157 - total_train_reward: -1279.2821\n",
      "Epoch 7677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8639 - total_train_reward: -1158.0602\n",
      "Epoch 7678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1837 - total_train_reward: -714.1251\n",
      "Epoch 7679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5645 - total_train_reward: -1461.2436\n",
      "Epoch 7680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.0338 - total_train_reward: -1279.0470\n",
      "Epoch 7681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4626 - total_train_reward: -1302.1727\n",
      "Epoch 7682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9151 - total_train_reward: -1062.3584\n",
      "Epoch 7683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3108 - total_train_reward: -1261.8871\n",
      "Epoch 7684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7892 - total_train_reward: -1470.0870\n",
      "Epoch 7685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8218 - total_train_reward: -1452.4346\n",
      "Epoch 7686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3849 - total_train_reward: -1593.3606\n",
      "Epoch 7687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3838 - total_train_reward: -1142.0335\n",
      "Epoch 7688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2888 - total_train_reward: -1764.4215\n",
      "Epoch 7689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2246 - total_train_reward: -1281.6854\n",
      "Epoch 7690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5975 - total_train_reward: -1335.2539\n",
      "Epoch 7691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3634 - total_train_reward: -940.9402\n",
      "Epoch 7692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 28.9947 - total_train_reward: -1214.2362\n",
      "Epoch 7693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7756 - total_train_reward: -1277.3296\n",
      "Epoch 7694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3746 - total_train_reward: -1491.5964\n",
      "Epoch 7695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7878 - total_train_reward: -1791.3475\n",
      "Epoch 7696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7212 - total_train_reward: -1340.2808\n",
      "Epoch 7697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3151 - total_train_reward: -1333.3298\n",
      "Epoch 7698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7177 - total_train_reward: -1515.7476\n",
      "Epoch 7699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2631 - total_train_reward: -1811.1839\n",
      "Epoch 7700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1340 - total_train_reward: -1355.9783\n",
      "Epoch 7701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1441 - total_train_reward: -1636.7897\n",
      "Epoch 7702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0064 - total_train_reward: -1116.4887\n",
      "Epoch 7703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9700 - total_train_reward: -1131.9836\n",
      "Epoch 7704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3735 - total_train_reward: -1103.5752\n",
      "Epoch 7705/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6181 - total_train_reward: -1250.2630\n",
      "Epoch 7706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4715 - total_train_reward: -1408.4685\n",
      "Epoch 7707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1433 - total_train_reward: -1575.1423\n",
      "Epoch 7708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2403 - total_train_reward: -1397.3586\n",
      "Epoch 7709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8386 - total_train_reward: -1259.5341\n",
      "Epoch 7710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3566 - total_train_reward: -1082.9673\n",
      "Epoch 7711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5685 - total_train_reward: -1715.7365\n",
      "Epoch 7712/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3230 - total_train_reward: -1167.7398\n",
      "Epoch 7713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5334 - total_train_reward: -1192.6522\n",
      "Epoch 7714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.9862 - total_train_reward: -1465.7040\n",
      "Epoch 7715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2111 - total_train_reward: -1203.8170\n",
      "Epoch 7716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0912 - total_train_reward: -1677.6605\n",
      "Epoch 7717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4862 - total_train_reward: -922.3316\n",
      "Epoch 7718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5040 - total_train_reward: -1260.0456\n",
      "Epoch 7719/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2696 - total_train_reward: -1394.6225\n",
      "Epoch 7720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4443 - total_train_reward: -967.9090\n",
      "Epoch 7721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2942 - total_train_reward: -956.0875\n",
      "Epoch 7722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3520 - total_train_reward: -1286.5758\n",
      "Epoch 7723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2673 - total_train_reward: -1299.8162\n",
      "Epoch 7724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5776 - total_train_reward: -1002.9586\n",
      "Epoch 7725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1242 - total_train_reward: -1249.1471\n",
      "Epoch 7726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5184 - total_train_reward: -1203.9025\n",
      "Epoch 7727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5313 - total_train_reward: -1550.2819\n",
      "Epoch 7728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5706 - total_train_reward: -1466.1115\n",
      "Epoch 7729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7059 - total_train_reward: -848.9594\n",
      "Epoch 7730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3751 - total_train_reward: -946.2570\n",
      "Epoch 7731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5355 - total_train_reward: -1199.8740\n",
      "Epoch 7732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4985 - total_train_reward: -1220.8243\n",
      "Epoch 7733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3406 - total_train_reward: -1259.3957\n",
      "Epoch 7734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4320 - total_train_reward: -1804.0084\n",
      "Epoch 7735/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8501 - total_train_reward: -1755.8957\n",
      "Epoch 7736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4440 - total_train_reward: -1081.2887\n",
      "Epoch 7737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7832 - total_train_reward: -1285.6668\n",
      "Epoch 7738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4350 - total_train_reward: -1738.6064\n",
      "Epoch 7739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0825 - total_train_reward: -1303.2868\n",
      "Epoch 7740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3725 - total_train_reward: -1509.1030\n",
      "Epoch 7741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7671 - total_train_reward: -1046.9238\n",
      "Epoch 7742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6955 - total_train_reward: -1812.4887\n",
      "Epoch 7743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8835 - total_train_reward: -1302.6028\n",
      "Epoch 7744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5384 - total_train_reward: -1563.0133\n",
      "Epoch 7745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8178 - total_train_reward: -1265.5307\n",
      "Epoch 7746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3112 - total_train_reward: -1649.6202\n",
      "Epoch 7747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2710 - total_train_reward: -1077.9838\n",
      "Epoch 7748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5459 - total_train_reward: -1809.6921\n",
      "Epoch 7749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3805 - total_train_reward: -1273.5152\n",
      "Epoch 7750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3754 - total_train_reward: -1295.3174\n",
      "Epoch 7751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5150 - total_train_reward: -1797.9828\n",
      "Epoch 7752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9339 - total_train_reward: -1288.2266\n",
      "Epoch 7753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9955 - total_train_reward: -1219.6784\n",
      "Epoch 7754/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6934 - total_train_reward: -1161.1957\n",
      "Epoch 7755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8711 - total_train_reward: -1794.8830\n",
      "Epoch 7756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.9941 - total_train_reward: -915.5808\n",
      "Epoch 7757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1222 - total_train_reward: -1803.3818\n",
      "Epoch 7758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0362 - total_train_reward: -1469.3543\n",
      "Epoch 7759/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 32.7951 - total_train_reward: -1282.6677\n",
      "Epoch 7760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1249 - total_train_reward: -1244.2528\n",
      "Epoch 7761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5971 - total_train_reward: -1206.2096\n",
      "Epoch 7762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7998 - total_train_reward: -1164.9454\n",
      "Epoch 7763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.4640 - total_train_reward: -1675.5309\n",
      "Epoch 7764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0102 - total_train_reward: -1221.6716\n",
      "Epoch 7765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4925 - total_train_reward: -1142.1927\n",
      "Epoch 7766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9226 - total_train_reward: -1318.0893\n",
      "Epoch 7767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7743 - total_train_reward: -956.1581\n",
      "Epoch 7768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7486 - total_train_reward: -1460.3213\n",
      "Epoch 7769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4381 - total_train_reward: -1590.7558\n",
      "Epoch 7770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8321 - total_train_reward: -1749.8097\n",
      "Epoch 7771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9329 - total_train_reward: -1158.1743\n",
      "Epoch 7772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0242 - total_train_reward: -1493.5722\n",
      "Epoch 7773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4204 - total_train_reward: -1362.0514\n",
      "Epoch 7774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.4525 - total_train_reward: -1103.3676\n",
      "Epoch 7775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8737 - total_train_reward: -908.8098\n",
      "Epoch 7776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8190 - total_train_reward: -1556.0855\n",
      "Epoch 7777/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6186 - total_train_reward: -1236.4162\n",
      "Epoch 7778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4683 - total_train_reward: -1268.4493\n",
      "Epoch 7779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0992 - total_train_reward: -1706.9570\n",
      "Epoch 7780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6971 - total_train_reward: -1315.4736\n",
      "Epoch 7781/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3955 - total_train_reward: -1269.5255\n",
      "Epoch 7782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8015 - total_train_reward: -1758.7381\n",
      "Epoch 7783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7178 - total_train_reward: -854.5397\n",
      "Epoch 7784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7464 - total_train_reward: -1806.3661\n",
      "Epoch 7785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0499 - total_train_reward: -1212.1681\n",
      "Epoch 7786/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7730 - total_train_reward: -1239.8025\n",
      "Epoch 7787/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9994 - total_train_reward: -1288.6039\n",
      "Epoch 7788/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6695 - total_train_reward: -1321.5363\n",
      "Epoch 7789/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8290 - total_train_reward: -1048.7494\n",
      "Epoch 7790/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2039 - total_train_reward: -1173.4893\n",
      "Epoch 7791/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7800 - total_train_reward: -1290.9445\n",
      "Epoch 7792/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2311 - total_train_reward: -1480.1577\n",
      "Epoch 7793/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5405 - total_train_reward: -1511.0615\n",
      "Epoch 7794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0640 - total_train_reward: -951.7722\n",
      "Epoch 7795/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6089 - total_train_reward: -1719.6972\n",
      "Epoch 7796/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0835 - total_train_reward: -1479.3420\n",
      "Epoch 7797/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3036 - total_train_reward: -1270.0624\n",
      "Epoch 7798/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1096 - total_train_reward: -1271.9220\n",
      "Epoch 7799/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1818 - total_train_reward: -1262.0587\n",
      "Epoch 7800/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8300 - total_train_reward: -1591.1196\n",
      "Epoch 7801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3448 - total_train_reward: -1275.1969\n",
      "Epoch 7802/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4807 - total_train_reward: -1281.1397\n",
      "Epoch 7803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1577 - total_train_reward: -1234.9903\n",
      "Epoch 7804/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4632 - total_train_reward: -1645.8647\n",
      "Epoch 7805/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9038 - total_train_reward: -1064.5635\n",
      "Epoch 7806/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1118 - total_train_reward: -1227.6918\n",
      "Epoch 7807/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8847 - total_train_reward: -1405.1594\n",
      "Epoch 7808/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9628 - total_train_reward: -1198.2738\n",
      "Epoch 7809/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3051 - total_train_reward: -1240.3582\n",
      "Epoch 7810/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0484 - total_train_reward: -1202.6133\n",
      "Epoch 7811/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1083 - total_train_reward: -1669.8938\n",
      "Epoch 7812/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2970 - total_train_reward: -1313.7787\n",
      "Epoch 7813/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5324 - total_train_reward: -1277.4608\n",
      "Epoch 7814/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3411 - total_train_reward: -927.0958\n",
      "Epoch 7815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2300 - total_train_reward: -1150.8637\n",
      "Epoch 7816/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5586 - total_train_reward: -1294.1174\n",
      "Epoch 7817/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.5926 - total_train_reward: -1235.9017\n",
      "Epoch 7818/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2385 - total_train_reward: -1286.2216\n",
      "Epoch 7819/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3546 - total_train_reward: -1173.1806\n",
      "Epoch 7820/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0976 - total_train_reward: -1629.7904\n",
      "Epoch 7821/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1438 - total_train_reward: -1330.4810\n",
      "Epoch 7822/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5815 - total_train_reward: -1296.5747\n",
      "Epoch 7823/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6247 - total_train_reward: -1174.9393\n",
      "Epoch 7824/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3337 - total_train_reward: -1350.5163\n",
      "Epoch 7825/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0709 - total_train_reward: -1417.7394\n",
      "Epoch 7826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9266 - total_train_reward: -1299.8513\n",
      "Epoch 7827/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3326 - total_train_reward: -822.7177\n",
      "Epoch 7828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3092 - total_train_reward: -848.6008\n",
      "Epoch 7829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2905 - total_train_reward: -864.3707\n",
      "Epoch 7830/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5384 - total_train_reward: -1483.9488\n",
      "Epoch 7831/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3443 - total_train_reward: -1705.4955\n",
      "Epoch 7832/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6792 - total_train_reward: -960.5886\n",
      "Epoch 7833/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3454 - total_train_reward: -1263.3850\n",
      "Epoch 7834/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5163 - total_train_reward: -1318.6874\n",
      "Epoch 7835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3443 - total_train_reward: -1467.8945\n",
      "Epoch 7836/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4683 - total_train_reward: -1300.6882\n",
      "Epoch 7837/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8111 - total_train_reward: -1223.7816\n",
      "Epoch 7838/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1759 - total_train_reward: -1068.6536\n",
      "Epoch 7839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5865 - total_train_reward: -1657.0825\n",
      "Epoch 7840/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1252 - total_train_reward: -1229.0723\n",
      "Epoch 7841/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8448 - total_train_reward: -1674.5475\n",
      "Epoch 7842/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9113 - total_train_reward: -1160.3939\n",
      "Epoch 7843/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9703 - total_train_reward: -1276.4552\n",
      "Epoch 7844/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9198 - total_train_reward: -1382.1946\n",
      "Epoch 7845/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7497 - total_train_reward: -1315.0103\n",
      "Epoch 7846/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8885 - total_train_reward: -1428.6189\n",
      "Epoch 7847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6973 - total_train_reward: -1219.8250\n",
      "Epoch 7848/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0931 - total_train_reward: -1710.4478\n",
      "Epoch 7849/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7171 - total_train_reward: -1763.3751\n",
      "Epoch 7850/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1374 - total_train_reward: -1667.6630\n",
      "Epoch 7851/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8389 - total_train_reward: -1339.7647\n",
      "Epoch 7852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0841 - total_train_reward: -1488.9159\n",
      "Epoch 7853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6562 - total_train_reward: -1236.0891\n",
      "Epoch 7854/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7840 - total_train_reward: -1064.6786\n",
      "Epoch 7855/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9623 - total_train_reward: -1284.7603\n",
      "Epoch 7856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8808 - total_train_reward: -1155.2532\n",
      "Epoch 7857/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0518 - total_train_reward: -1491.8677\n",
      "Epoch 7858/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7649 - total_train_reward: -1250.8962\n",
      "Epoch 7859/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2482 - total_train_reward: -1125.2484\n",
      "Epoch 7860/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3650 - total_train_reward: -1376.6967\n",
      "Epoch 7861/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1074 - total_train_reward: -723.6828\n",
      "Epoch 7862/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8263 - total_train_reward: -1271.4934\n",
      "Epoch 7863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8274 - total_train_reward: -1296.7715\n",
      "Epoch 7864/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.6447 - total_train_reward: -1562.9630\n",
      "Epoch 7865/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2181 - total_train_reward: -988.9324\n",
      "Epoch 7866/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5294 - total_train_reward: -1197.9907\n",
      "Epoch 7867/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6989 - total_train_reward: -1787.1287\n",
      "Epoch 7868/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1684 - total_train_reward: -1394.7013\n",
      "Epoch 7869/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5941 - total_train_reward: -967.0526\n",
      "Epoch 7870/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6483 - total_train_reward: -1249.5965\n",
      "Epoch 7871/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1933 - total_train_reward: -1762.5954\n",
      "Epoch 7872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8055 - total_train_reward: -1210.6325\n",
      "Epoch 7873/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7652 - total_train_reward: -1198.1324\n",
      "Epoch 7874/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7139 - total_train_reward: -1038.8936\n",
      "Epoch 7875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1656 - total_train_reward: -1463.0708\n",
      "Epoch 7876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1213 - total_train_reward: -1766.0955\n",
      "Epoch 7877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5741 - total_train_reward: -1361.6330\n",
      "Epoch 7878/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7069 - total_train_reward: -1482.6411\n",
      "Epoch 7879/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5459 - total_train_reward: -1289.5496\n",
      "Epoch 7880/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9828 - total_train_reward: -956.4943\n",
      "Epoch 7881/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5893 - total_train_reward: -1796.1723\n",
      "Epoch 7882/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9903 - total_train_reward: -1471.1864\n",
      "Epoch 7883/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1478 - total_train_reward: -1814.6530\n",
      "Epoch 7884/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4252 - total_train_reward: -775.8612\n",
      "Epoch 7885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9745 - total_train_reward: -1494.4082\n",
      "Epoch 7886/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4940 - total_train_reward: -683.1659\n",
      "Epoch 7887/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3751 - total_train_reward: -1271.7251\n",
      "Epoch 7888/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2850 - total_train_reward: -1163.4157\n",
      "Epoch 7889/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3737 - total_train_reward: -1414.2057\n",
      "Epoch 7890/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0454 - total_train_reward: -1813.7642\n",
      "Epoch 7891/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3089 - total_train_reward: -1210.8278\n",
      "Epoch 7892/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2809 - total_train_reward: -1299.0232\n",
      "Epoch 7893/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8943 - total_train_reward: -1291.3922\n",
      "Epoch 7894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0006 - total_train_reward: -1183.4622\n",
      "Epoch 7895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4429 - total_train_reward: -1797.5521\n",
      "Epoch 7896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0424 - total_train_reward: -1262.3529\n",
      "Epoch 7897/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4240 - total_train_reward: -1253.1106\n",
      "Epoch 7898/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6119 - total_train_reward: -1063.0102\n",
      "Epoch 7899/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0927 - total_train_reward: -878.0331\n",
      "Epoch 7900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6265 - total_train_reward: -1083.2264\n",
      "Epoch 7901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4093 - total_train_reward: -1026.5036\n",
      "Epoch 7902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7313 - total_train_reward: -1345.5224\n",
      "Epoch 7903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8034 - total_train_reward: -1525.4494\n",
      "Epoch 7904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.6254 - total_train_reward: -1028.8204\n",
      "Epoch 7905/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8375 - total_train_reward: -1556.5557\n",
      "Epoch 7906/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4375 - total_train_reward: -961.9254\n",
      "Epoch 7907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9769 - total_train_reward: -1171.5597\n",
      "Epoch 7908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0445 - total_train_reward: -1312.1460\n",
      "Epoch 7909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.2466 - total_train_reward: -1057.6564\n",
      "Epoch 7910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5854 - total_train_reward: -1124.7471\n",
      "Epoch 7911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7769 - total_train_reward: -1047.2131\n",
      "Epoch 7912/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8538 - total_train_reward: -1753.0213\n",
      "Epoch 7913/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3969 - total_train_reward: -1819.1110\n",
      "Epoch 7914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1497 - total_train_reward: -1755.5530\n",
      "Epoch 7915/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0390 - total_train_reward: -1690.0777\n",
      "Epoch 7916/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0178 - total_train_reward: -1299.0870\n",
      "Epoch 7917/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3133 - total_train_reward: -1361.8168\n",
      "Epoch 7918/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7839 - total_train_reward: -1277.9646\n",
      "Epoch 7919/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7949 - total_train_reward: -859.7699\n",
      "Epoch 7920/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7668 - total_train_reward: -1599.5773\n",
      "Epoch 7921/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.2292 - total_train_reward: -1225.7321\n",
      "Epoch 7922/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4964 - total_train_reward: -1787.3475\n",
      "Epoch 7923/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8255 - total_train_reward: -1296.8218\n",
      "Epoch 7924/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0175 - total_train_reward: -1153.9400\n",
      "Epoch 7925/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5907 - total_train_reward: -1769.9445\n",
      "Epoch 7926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8769 - total_train_reward: -1215.8836\n",
      "Epoch 7927/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4897 - total_train_reward: -1236.5048\n",
      "Epoch 7928/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1204 - total_train_reward: -1776.6183\n",
      "Epoch 7929/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3381 - total_train_reward: -1217.0882\n",
      "Epoch 7930/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0959 - total_train_reward: -1169.2226\n",
      "Epoch 7931/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9616 - total_train_reward: -1452.7187\n",
      "Epoch 7932/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5997 - total_train_reward: -1727.8364\n",
      "Epoch 7933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8993 - total_train_reward: -1713.3657\n",
      "Epoch 7934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6597 - total_train_reward: -1481.8444\n",
      "Epoch 7935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7671 - total_train_reward: -1795.0592\n",
      "Epoch 7936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1250 - total_train_reward: -1716.4866\n",
      "Epoch 7937/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5678 - total_train_reward: -1189.2254\n",
      "Epoch 7938/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8916 - total_train_reward: -1773.8846\n",
      "Epoch 7939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9165 - total_train_reward: -1477.0302\n",
      "Epoch 7940/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8992 - total_train_reward: -1592.0990\n",
      "Epoch 7941/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6848 - total_train_reward: -1482.8275\n",
      "Epoch 7942/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9909 - total_train_reward: -1189.4864\n",
      "Epoch 7943/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4489 - total_train_reward: -1593.1256\n",
      "Epoch 7944/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3125 - total_train_reward: -1072.0878\n",
      "Epoch 7945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3299 - total_train_reward: -1206.4426\n",
      "Epoch 7946/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4130 - total_train_reward: -1280.0537\n",
      "Epoch 7947/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8598 - total_train_reward: -1241.3788\n",
      "Epoch 7948/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2645 - total_train_reward: -1439.3988\n",
      "Epoch 7949/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9910 - total_train_reward: -1262.2358\n",
      "Epoch 7950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1202 - total_train_reward: -1280.6935\n",
      "Epoch 7951/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3191 - total_train_reward: -1670.4306\n",
      "Epoch 7952/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8474 - total_train_reward: -1379.6826\n",
      "Epoch 7953/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4572 - total_train_reward: -1341.3337\n",
      "Epoch 7954/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8407 - total_train_reward: -1224.6167\n",
      "Epoch 7955/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0027 - total_train_reward: -1297.5659\n",
      "Epoch 7956/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8044 - total_train_reward: -1421.5720\n",
      "Epoch 7957/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9034 - total_train_reward: -1069.0331\n",
      "Epoch 7958/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4561 - total_train_reward: -1262.5010\n",
      "Epoch 7959/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2082 - total_train_reward: -1173.1369\n",
      "Epoch 7960/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7472 - total_train_reward: -966.0684\n",
      "Epoch 7961/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2251 - total_train_reward: -1434.5755\n",
      "Epoch 7962/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9179 - total_train_reward: -1358.2730\n",
      "Epoch 7963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0033 - total_train_reward: -703.8657\n",
      "Epoch 7964/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.9888 - total_train_reward: -1201.3768\n",
      "Epoch 7965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6491 - total_train_reward: -1662.8202\n",
      "Epoch 7966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8786 - total_train_reward: -1709.9201\n",
      "Epoch 7967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5785 - total_train_reward: -1240.8204\n",
      "Epoch 7968/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7150 - total_train_reward: -1312.8787\n",
      "Epoch 7969/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6877 - total_train_reward: -1091.9431\n",
      "Epoch 7970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9118 - total_train_reward: -1164.4263\n",
      "Epoch 7971/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9070 - total_train_reward: -1282.3252\n",
      "Epoch 7972/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9275 - total_train_reward: -1252.6703\n",
      "Epoch 7973/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3813 - total_train_reward: -1325.9494\n",
      "Epoch 7974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3029 - total_train_reward: -1544.0287\n",
      "Epoch 7975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5151 - total_train_reward: -1476.0114\n",
      "Epoch 7976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9363 - total_train_reward: -1178.1404\n",
      "Epoch 7977/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0382 - total_train_reward: -1243.8428\n",
      "Epoch 7978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9604 - total_train_reward: -1027.4977\n",
      "Epoch 7979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0862 - total_train_reward: -956.3783\n",
      "Epoch 7980/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1059 - total_train_reward: -1229.7237\n",
      "Epoch 7981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0707 - total_train_reward: -1263.1590\n",
      "Epoch 7982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9718 - total_train_reward: -1236.9075\n",
      "Epoch 7983/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3063 - total_train_reward: -1268.9955\n",
      "Epoch 7984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9518 - total_train_reward: -1166.3567\n",
      "Epoch 7985/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4532 - total_train_reward: -1790.5070\n",
      "Epoch 7986/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7022 - total_train_reward: -1159.5785\n",
      "Epoch 7987/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0126 - total_train_reward: -1624.5408\n",
      "Epoch 7988/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6271 - total_train_reward: -1214.5308\n",
      "Epoch 7989/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4123 - total_train_reward: -1049.9786\n",
      "Epoch 7990/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3577 - total_train_reward: -843.6510\n",
      "Epoch 7991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0016 - total_train_reward: -1321.4578\n",
      "Epoch 7992/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9270 - total_train_reward: -1183.5405\n",
      "Epoch 7993/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2936 - total_train_reward: -1284.4326\n",
      "Epoch 7994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9112 - total_train_reward: -1168.4201\n",
      "Epoch 7995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3711 - total_train_reward: -1400.4241\n",
      "Epoch 7996/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5363 - total_train_reward: -1344.9071\n",
      "Epoch 7997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7046 - total_train_reward: -1508.2769\n",
      "Epoch 7998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3908 - total_train_reward: -1487.8001\n",
      "Epoch 7999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3194 - total_train_reward: -1815.3790\n",
      "Epoch 8000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4517 - total_train_reward: -1421.1353\n",
      "Epoch 8001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.6582 - total_train_reward: -1706.3151\n",
      "Epoch 8002/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3202 - total_train_reward: -1671.3185\n",
      "Epoch 8003/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8358 - total_train_reward: -1303.1782\n",
      "Epoch 8004/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1113 - total_train_reward: -1801.0345\n",
      "Epoch 8005/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8382 - total_train_reward: -1754.8322\n",
      "Epoch 8006/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2519 - total_train_reward: -977.8985\n",
      "Epoch 8007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5371 - total_train_reward: -1220.6757\n",
      "Epoch 8008/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6094 - total_train_reward: -844.5778\n",
      "Epoch 8009/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2906 - total_train_reward: -1059.8100\n",
      "Epoch 8010/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6575 - total_train_reward: -1045.5703\n",
      "Epoch 8011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.0841 - total_train_reward: -1217.8718\n",
      "Epoch 8012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0669 - total_train_reward: -1058.9152\n",
      "Epoch 8013/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5173 - total_train_reward: -1268.4636\n",
      "Epoch 8014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1690 - total_train_reward: -1224.9433\n",
      "Epoch 8015/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5818 - total_train_reward: -1277.4223\n",
      "Epoch 8016/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4596 - total_train_reward: -1473.8732\n",
      "Epoch 8017/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3463 - total_train_reward: -1337.4580\n",
      "Epoch 8018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5978 - total_train_reward: -1162.1402\n",
      "Epoch 8019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4234 - total_train_reward: -1051.5941\n",
      "Epoch 8020/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8827 - total_train_reward: -725.1275\n",
      "Epoch 8021/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6641 - total_train_reward: -1553.5402\n",
      "Epoch 8022/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6119 - total_train_reward: -1276.2275\n",
      "Epoch 8023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9894 - total_train_reward: -1256.8904\n",
      "Epoch 8024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2354 - total_train_reward: -1814.3457\n",
      "Epoch 8025/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7723 - total_train_reward: -1099.2045\n",
      "Epoch 8026/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7140 - total_train_reward: -1398.8409\n",
      "Epoch 8027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.4976 - total_train_reward: -1215.9856\n",
      "Epoch 8028/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2740 - total_train_reward: -880.2243\n",
      "Epoch 8029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9097 - total_train_reward: -1248.1390\n",
      "Epoch 8030/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5665 - total_train_reward: -1091.9141\n",
      "Epoch 8031/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3006 - total_train_reward: -1168.3915\n",
      "Epoch 8032/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.8874 - total_train_reward: -1639.5328\n",
      "Epoch 8033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3561 - total_train_reward: -1489.8685\n",
      "Epoch 8034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7595 - total_train_reward: -1063.9423\n",
      "Epoch 8035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3272 - total_train_reward: -1474.2265\n",
      "Epoch 8036/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2638 - total_train_reward: -1007.5519\n",
      "Epoch 8037/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1989 - total_train_reward: -1235.3602\n",
      "Epoch 8038/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8383 - total_train_reward: -1245.2931\n",
      "Epoch 8039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6819 - total_train_reward: -1720.5752\n",
      "Epoch 8040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7435 - total_train_reward: -1218.3677\n",
      "Epoch 8041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7371 - total_train_reward: -957.4855\n",
      "Epoch 8042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1978 - total_train_reward: -1041.7214\n",
      "Epoch 8043/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.9519 - total_train_reward: -1220.2688\n",
      "Epoch 8044/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0978 - total_train_reward: -1252.4231\n",
      "Epoch 8045/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0578 - total_train_reward: -1275.1121\n",
      "Epoch 8046/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7833 - total_train_reward: -966.5763\n",
      "Epoch 8047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8851 - total_train_reward: -1213.9042\n",
      "Epoch 8048/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9941 - total_train_reward: -1806.5590\n",
      "Epoch 8049/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2483 - total_train_reward: -1288.9372\n",
      "Epoch 8050/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7960 - total_train_reward: -1062.9189\n",
      "Epoch 8051/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8263 - total_train_reward: -1186.1589\n",
      "Epoch 8052/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9385 - total_train_reward: -1643.2982\n",
      "Epoch 8053/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5807 - total_train_reward: -1744.6050\n",
      "Epoch 8054/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5303 - total_train_reward: -1508.5898\n",
      "Epoch 8055/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6827 - total_train_reward: -1162.4436\n",
      "Epoch 8056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.0685 - total_train_reward: -1216.0003\n",
      "Epoch 8057/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0533 - total_train_reward: -1256.7604\n",
      "Epoch 8058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2171 - total_train_reward: -1315.5643\n",
      "Epoch 8059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6136 - total_train_reward: -1191.6501\n",
      "Epoch 8060/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2681 - total_train_reward: -1063.0085\n",
      "Epoch 8061/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5264 - total_train_reward: -1158.6697\n",
      "Epoch 8062/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4219 - total_train_reward: -1595.2659\n",
      "Epoch 8063/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4769 - total_train_reward: -1290.1187\n",
      "Epoch 8064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1310 - total_train_reward: -1637.3168\n",
      "Epoch 8065/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0328 - total_train_reward: -1303.3045\n",
      "Epoch 8066/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6688 - total_train_reward: -1716.9811\n",
      "Epoch 8067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3226 - total_train_reward: -1151.7651\n",
      "Epoch 8068/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3684 - total_train_reward: -1275.1048\n",
      "Epoch 8069/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1496 - total_train_reward: -1258.0239\n",
      "Epoch 8070/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1470 - total_train_reward: -705.3367\n",
      "Epoch 8071/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7041 - total_train_reward: -1084.4218\n",
      "Epoch 8072/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6718 - total_train_reward: -1342.7531\n",
      "Epoch 8073/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4958 - total_train_reward: -841.5541\n",
      "Epoch 8074/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6459 - total_train_reward: -1811.9936\n",
      "Epoch 8075/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3424 - total_train_reward: -954.4702\n",
      "Epoch 8076/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7290 - total_train_reward: -954.2269\n",
      "Epoch 8077/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4520 - total_train_reward: -1231.2358\n",
      "Epoch 8078/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2849 - total_train_reward: -1062.6032\n",
      "Epoch 8079/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7758 - total_train_reward: -1057.5707\n",
      "Epoch 8080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2404 - total_train_reward: -1039.8106\n",
      "Epoch 8081/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1365 - total_train_reward: -1295.7104\n",
      "Epoch 8082/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6156 - total_train_reward: -1235.7024\n",
      "Epoch 8083/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7232 - total_train_reward: -1270.4448\n",
      "Epoch 8084/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7741 - total_train_reward: -1066.6947\n",
      "Epoch 8085/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2868 - total_train_reward: -1265.4827\n",
      "Epoch 8086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4522 - total_train_reward: -1706.8628\n",
      "Epoch 8087/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2582 - total_train_reward: -1578.7414\n",
      "Epoch 8088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8571 - total_train_reward: -1650.9509\n",
      "Epoch 8089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1551 - total_train_reward: -1288.1404\n",
      "Epoch 8090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7351 - total_train_reward: -954.7299\n",
      "Epoch 8091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3918 - total_train_reward: -1739.1162\n",
      "Epoch 8092/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0158 - total_train_reward: -949.5893\n",
      "Epoch 8093/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 21.6566 - total_train_reward: -1240.3263\n",
      "Epoch 8094/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7308 - total_train_reward: -1236.0202\n",
      "Epoch 8095/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6572 - total_train_reward: -1150.4200\n",
      "Epoch 8096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3970 - total_train_reward: -1706.5381\n",
      "Epoch 8097/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8947 - total_train_reward: -1461.4562\n",
      "Epoch 8098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9724 - total_train_reward: -1560.9388\n",
      "Epoch 8099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4432 - total_train_reward: -1021.9641\n",
      "Epoch 8100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0293 - total_train_reward: -1770.4222\n",
      "Epoch 8101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6836 - total_train_reward: -1292.8889\n",
      "Epoch 8102/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.6169 - total_train_reward: -1737.0061\n",
      "Epoch 8103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6879 - total_train_reward: -1201.2814\n",
      "Epoch 8104/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 5.9092 - total_train_reward: -1060.8483\n",
      "Epoch 8105/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -0.4063 - total_train_reward: -1522.7912\n",
      "Epoch 8106/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4316 - total_train_reward: -991.8089\n",
      "Epoch 8107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7612 - total_train_reward: -1293.3538\n",
      "Epoch 8108/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3081 - total_train_reward: -1230.9542\n",
      "Epoch 8109/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8457 - total_train_reward: -1217.6196\n",
      "Epoch 8110/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9890 - total_train_reward: -1472.5646\n",
      "Epoch 8111/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1623 - total_train_reward: -1426.9032\n",
      "Epoch 8112/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8104 - total_train_reward: -1743.8630\n",
      "Epoch 8113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9019 - total_train_reward: -1304.8911\n",
      "Epoch 8114/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0790 - total_train_reward: -1262.3915\n",
      "Epoch 8115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2027 - total_train_reward: -1038.3143\n",
      "Epoch 8116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0117 - total_train_reward: -1185.0455\n",
      "Epoch 8117/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0769 - total_train_reward: -1808.6440\n",
      "Epoch 8118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2455 - total_train_reward: -1194.7334\n",
      "Epoch 8119/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1912 - total_train_reward: -1184.9332\n",
      "Epoch 8120/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1818 - total_train_reward: -1378.0138\n",
      "Epoch 8121/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3920 - total_train_reward: -1610.8718\n",
      "Epoch 8122/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2423 - total_train_reward: -1011.4037\n",
      "Epoch 8123/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.5002 - total_train_reward: -1268.5830\n",
      "Epoch 8124/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6865 - total_train_reward: -1290.3819\n",
      "Epoch 8125/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2054 - total_train_reward: -1635.6413\n",
      "Epoch 8126/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5854 - total_train_reward: -1707.8875\n",
      "Epoch 8127/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7334 - total_train_reward: -1049.4275\n",
      "Epoch 8128/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3650 - total_train_reward: -1303.6817\n",
      "Epoch 8129/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8981 - total_train_reward: -1624.8874\n",
      "Epoch 8130/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2575 - total_train_reward: -1176.4517\n",
      "Epoch 8131/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8081 - total_train_reward: -1344.0401\n",
      "Epoch 8132/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8132 - total_train_reward: -1229.9192\n",
      "Epoch 8133/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5161 - total_train_reward: -913.4050\n",
      "Epoch 8134/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9285 - total_train_reward: -1282.8895\n",
      "Epoch 8135/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2361 - total_train_reward: -1445.8434\n",
      "Epoch 8136/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8287 - total_train_reward: -1627.7162\n",
      "Epoch 8137/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0136 - total_train_reward: -1300.0040\n",
      "Epoch 8138/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4834 - total_train_reward: -1128.7234\n",
      "Epoch 8139/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8048 - total_train_reward: -1045.0200\n",
      "Epoch 8140/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0236 - total_train_reward: -1279.2462\n",
      "Epoch 8141/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2222 - total_train_reward: -1233.3635\n",
      "Epoch 8142/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0906 - total_train_reward: -1215.5826\n",
      "Epoch 8143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6553 - total_train_reward: -1775.1874\n",
      "Epoch 8144/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6493 - total_train_reward: -1067.8309\n",
      "Epoch 8145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0361 - total_train_reward: -1505.2209\n",
      "Epoch 8146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0082 - total_train_reward: -1301.7880\n",
      "Epoch 8147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4239 - total_train_reward: -1324.4279\n",
      "Epoch 8148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4858 - total_train_reward: -1166.5590\n",
      "Epoch 8149/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1993 - total_train_reward: -1423.7370\n",
      "Epoch 8150/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1136 - total_train_reward: -1277.3465\n",
      "Epoch 8151/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1775 - total_train_reward: -955.2068\n",
      "Epoch 8152/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6909 - total_train_reward: -1319.4860\n",
      "Epoch 8153/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2424 - total_train_reward: -1097.4021\n",
      "Epoch 8154/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6263 - total_train_reward: -1536.1010\n",
      "Epoch 8155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6722 - total_train_reward: -1655.6415\n",
      "Epoch 8156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9661 - total_train_reward: -1340.6023\n",
      "Epoch 8157/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8320 - total_train_reward: -1225.7262\n",
      "Epoch 8158/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5799 - total_train_reward: -933.2519\n",
      "Epoch 8159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4402 - total_train_reward: -1379.4933\n",
      "Epoch 8160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9759 - total_train_reward: -1301.1883\n",
      "Epoch 8161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5695 - total_train_reward: -1185.1054\n",
      "Epoch 8162/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0886 - total_train_reward: -486.2662\n",
      "Epoch 8163/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4983 - total_train_reward: -844.5230\n",
      "Epoch 8164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 13.7915 - total_train_reward: -1252.4260\n",
      "Epoch 8165/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4122 - total_train_reward: -1171.7839\n",
      "Epoch 8166/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9657 - total_train_reward: -1230.8170\n",
      "Epoch 8167/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.2192 - total_train_reward: -1737.7197\n",
      "Epoch 8168/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7917 - total_train_reward: -1305.8440\n",
      "Epoch 8169/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.3078 - total_train_reward: -1221.4976\n",
      "Epoch 8170/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4293 - total_train_reward: -1183.4033\n",
      "Epoch 8171/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7397 - total_train_reward: -1498.2815\n",
      "Epoch 8172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6215 - total_train_reward: -1060.8401\n",
      "Epoch 8173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1938 - total_train_reward: -1275.0647\n",
      "Epoch 8174/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6216 - total_train_reward: -1762.2759\n",
      "Epoch 8175/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8369 - total_train_reward: -1265.1833\n",
      "Epoch 8176/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.4673 - total_train_reward: -1609.5480\n",
      "Epoch 8177/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8913 - total_train_reward: -1067.0874\n",
      "Epoch 8178/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6640 - total_train_reward: -1285.9002\n",
      "Epoch 8179/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2564 - total_train_reward: -1306.0476\n",
      "Epoch 8180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0672 - total_train_reward: -1577.5877\n",
      "Epoch 8181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9415 - total_train_reward: -1273.0905\n",
      "Epoch 8182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6196 - total_train_reward: -1206.7450\n",
      "Epoch 8183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0038 - total_train_reward: -1242.0969\n",
      "Epoch 8184/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8167 - total_train_reward: -1739.8721\n",
      "Epoch 8185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9627 - total_train_reward: -1480.9642\n",
      "Epoch 8186/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7021 - total_train_reward: -1587.0117\n",
      "Epoch 8187/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0693 - total_train_reward: -1068.7317\n",
      "Epoch 8188/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5089 - total_train_reward: -1685.5386\n",
      "Epoch 8189/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3490 - total_train_reward: -1249.2085\n",
      "Epoch 8190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4915 - total_train_reward: -1707.0693\n",
      "Epoch 8191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9585 - total_train_reward: -1295.6056\n",
      "Epoch 8192/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1838 - total_train_reward: -1786.3536\n",
      "Epoch 8193/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4314 - total_train_reward: -1408.8531\n",
      "Epoch 8194/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0352 - total_train_reward: -1307.4676\n",
      "Epoch 8195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2948 - total_train_reward: -1294.9824\n",
      "Epoch 8196/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1443 - total_train_reward: -1262.4904\n",
      "Epoch 8197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8140 - total_train_reward: -1352.7488\n",
      "Epoch 8198/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6883 - total_train_reward: -1746.5024\n",
      "Epoch 8199/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8357 - total_train_reward: -1422.2935\n",
      "Epoch 8200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6419 - total_train_reward: -1705.8617\n",
      "Epoch 8201/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9086 - total_train_reward: -1595.7164\n",
      "Epoch 8202/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3032 - total_train_reward: -1325.6656\n",
      "Epoch 8203/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1180 - total_train_reward: -1734.0970\n",
      "Epoch 8204/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2290 - total_train_reward: -952.9580\n",
      "Epoch 8205/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3475 - total_train_reward: -724.5470\n",
      "Epoch 8206/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7013 - total_train_reward: -1256.5231\n",
      "Epoch 8207/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6897 - total_train_reward: -1064.1015\n",
      "Epoch 8208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1866 - total_train_reward: -1196.8710\n",
      "Epoch 8209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5813 - total_train_reward: -1613.7144\n",
      "Epoch 8210/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2660 - total_train_reward: -1231.7817\n",
      "Epoch 8211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9409 - total_train_reward: -1065.6402\n",
      "Epoch 8212/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.0562 - total_train_reward: -1398.4097\n",
      "Epoch 8213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4843 - total_train_reward: -1485.2537\n",
      "Epoch 8214/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8881 - total_train_reward: -1427.5927\n",
      "Epoch 8215/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2151 - total_train_reward: -1503.0055\n",
      "Epoch 8216/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9173 - total_train_reward: -1182.1169\n",
      "Epoch 8217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.0812 - total_train_reward: -1219.6117\n",
      "Epoch 8218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5379 - total_train_reward: -1347.4014\n",
      "Epoch 8219/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2695 - total_train_reward: -956.8560\n",
      "Epoch 8220/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1317 - total_train_reward: -1222.1267\n",
      "Epoch 8221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9580 - total_train_reward: -1741.7252\n",
      "Epoch 8222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2156 - total_train_reward: -1685.8568\n",
      "Epoch 8223/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9576 - total_train_reward: -1046.4908\n",
      "Epoch 8224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9194 - total_train_reward: -1263.9614\n",
      "Epoch 8225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6216 - total_train_reward: -1284.1951\n",
      "Epoch 8226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4301 - total_train_reward: -954.1156\n",
      "Epoch 8227/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5124 - total_train_reward: -1245.1848\n",
      "Epoch 8228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6494 - total_train_reward: -725.0949\n",
      "Epoch 8229/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4971 - total_train_reward: -1233.8099\n",
      "Epoch 8230/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7772 - total_train_reward: -1247.1237\n",
      "Epoch 8231/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3096 - total_train_reward: -1232.5110\n",
      "Epoch 8232/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2494 - total_train_reward: -1582.0464\n",
      "Epoch 8233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5543 - total_train_reward: -1057.3602\n",
      "Epoch 8234/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0825 - total_train_reward: -1615.7701\n",
      "Epoch 8235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0722 - total_train_reward: -1063.6800\n",
      "Epoch 8236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3980 - total_train_reward: -1248.1307\n",
      "Epoch 8237/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8915 - total_train_reward: -1814.3434\n",
      "Epoch 8238/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3863 - total_train_reward: -923.2472\n",
      "Epoch 8239/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8118 - total_train_reward: -1259.5741\n",
      "Epoch 8240/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0602 - total_train_reward: -1358.3714\n",
      "Epoch 8241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5523 - total_train_reward: -1327.6438\n",
      "Epoch 8242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0325 - total_train_reward: -956.7863\n",
      "Epoch 8243/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7345 - total_train_reward: -1414.5422\n",
      "Epoch 8244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3273 - total_train_reward: -1190.0284\n",
      "Epoch 8245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9250 - total_train_reward: -1495.5716\n",
      "Epoch 8246/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8496 - total_train_reward: -1648.9471\n",
      "Epoch 8247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0482 - total_train_reward: -1458.1164\n",
      "Epoch 8248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9822 - total_train_reward: -1220.1842\n",
      "Epoch 8249/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5782 - total_train_reward: -1053.1310\n",
      "Epoch 8250/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0064 - total_train_reward: -1705.2760\n",
      "Epoch 8251/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1759 - total_train_reward: -1170.3734\n",
      "Epoch 8252/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1856 - total_train_reward: -1605.7133\n",
      "Epoch 8253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7446 - total_train_reward: -1270.3115\n",
      "Epoch 8254/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2711 - total_train_reward: -1541.5382\n",
      "Epoch 8255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6237 - total_train_reward: -1530.3625\n",
      "Epoch 8256/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1062 - total_train_reward: -1283.4448\n",
      "Epoch 8257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.6286 - total_train_reward: -1226.6198\n",
      "Epoch 8258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0697 - total_train_reward: -1284.9424\n",
      "Epoch 8259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7702 - total_train_reward: -1067.4094\n",
      "Epoch 8260/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9599 - total_train_reward: -1557.2365\n",
      "Epoch 8261/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9129 - total_train_reward: -1058.1015\n",
      "Epoch 8262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4085 - total_train_reward: -1478.3484\n",
      "Epoch 8263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5195 - total_train_reward: -1158.7810\n",
      "Epoch 8264/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6296 - total_train_reward: -1387.8170\n",
      "Epoch 8265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1947 - total_train_reward: -1349.6352\n",
      "Epoch 8266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9451 - total_train_reward: -1476.3367\n",
      "Epoch 8267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5316 - total_train_reward: -1049.8843\n",
      "Epoch 8268/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8194 - total_train_reward: -1672.4235\n",
      "Epoch 8269/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8826 - total_train_reward: -1592.8990\n",
      "Epoch 8270/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7109 - total_train_reward: -1252.7681\n",
      "Epoch 8271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5624 - total_train_reward: -1325.7484\n",
      "Epoch 8272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8159 - total_train_reward: -1320.6097\n",
      "Epoch 8273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6055 - total_train_reward: -1117.1614\n",
      "Epoch 8274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5932 - total_train_reward: -1269.6929\n",
      "Epoch 8275/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2987 - total_train_reward: -1245.3085\n",
      "Epoch 8276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6497 - total_train_reward: -1257.3148\n",
      "Epoch 8277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4067 - total_train_reward: -1698.7234\n",
      "Epoch 8278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2169 - total_train_reward: -1065.5509\n",
      "Epoch 8279/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7122 - total_train_reward: -1261.0957\n",
      "Epoch 8280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7495 - total_train_reward: -1406.7391\n",
      "Epoch 8281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9010 - total_train_reward: -1222.3805\n",
      "Epoch 8282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1899 - total_train_reward: -1450.9521\n",
      "Epoch 8283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2663 - total_train_reward: -1224.7839\n",
      "Epoch 8284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5300 - total_train_reward: -736.3126\n",
      "Epoch 8285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0933 - total_train_reward: -1141.1690\n",
      "Epoch 8286/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4293 - total_train_reward: -1423.2228\n",
      "Epoch 8287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4598 - total_train_reward: -1176.3884\n",
      "Epoch 8288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3558 - total_train_reward: -1268.5314\n",
      "Epoch 8289/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6204 - total_train_reward: -1064.3575\n",
      "Epoch 8290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6714 - total_train_reward: -1461.3802\n",
      "Epoch 8291/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6607 - total_train_reward: -1538.3114\n",
      "Epoch 8292/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0979 - total_train_reward: -957.1099\n",
      "Epoch 8293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1615 - total_train_reward: -1254.4930\n",
      "Epoch 8294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0080 - total_train_reward: -1513.6819\n",
      "Epoch 8295/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9382 - total_train_reward: -1041.6265\n",
      "Epoch 8296/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4396 - total_train_reward: -1592.2880\n",
      "Epoch 8297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4243 - total_train_reward: -1285.9770\n",
      "Epoch 8298/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1224 - total_train_reward: -1669.4776\n",
      "Epoch 8299/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7813 - total_train_reward: -722.1974\n",
      "Epoch 8300/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0720 - total_train_reward: -1169.1925\n",
      "Epoch 8301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6005 - total_train_reward: -1278.1645\n",
      "Epoch 8302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8266 - total_train_reward: -956.3593\n",
      "Epoch 8303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0985 - total_train_reward: -1071.2484\n",
      "Epoch 8304/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0930 - total_train_reward: -1285.2588\n",
      "Epoch 8305/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8670 - total_train_reward: -1054.9453\n",
      "Epoch 8306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1055 - total_train_reward: -1470.1636\n",
      "Epoch 8307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3229 - total_train_reward: -1333.1939\n",
      "Epoch 8308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4225 - total_train_reward: -1258.1694\n",
      "Epoch 8309/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9047 - total_train_reward: -1258.7494\n",
      "Epoch 8310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8092 - total_train_reward: -854.1758\n",
      "Epoch 8311/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9463 - total_train_reward: -1810.8021\n",
      "Epoch 8312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.6371 - total_train_reward: -1807.9320\n",
      "Epoch 8313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7998 - total_train_reward: -1717.8916\n",
      "Epoch 8314/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0531 - total_train_reward: -1268.6417\n",
      "Epoch 8315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8010 - total_train_reward: -1281.3354\n",
      "Epoch 8316/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7298 - total_train_reward: -1179.6351\n",
      "Epoch 8317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0482 - total_train_reward: -1796.1827\n",
      "Epoch 8318/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8897 - total_train_reward: -841.1419\n",
      "Epoch 8319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0910 - total_train_reward: -1071.3298\n",
      "Epoch 8320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6680 - total_train_reward: -1300.6816\n",
      "Epoch 8321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6571 - total_train_reward: -1296.2351\n",
      "Epoch 8322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7368 - total_train_reward: -1437.3531\n",
      "Epoch 8323/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.1832 - total_train_reward: -1216.4292\n",
      "Epoch 8324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8063 - total_train_reward: -1814.7218\n",
      "Epoch 8325/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6097 - total_train_reward: -1477.7989\n",
      "Epoch 8326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5368 - total_train_reward: -1187.1345\n",
      "Epoch 8327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9301 - total_train_reward: -904.8788\n",
      "Epoch 8328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0771 - total_train_reward: -1253.4792\n",
      "Epoch 8329/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8799 - total_train_reward: -1521.7273\n",
      "Epoch 8330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9966 - total_train_reward: -1178.8277\n",
      "Epoch 8331/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0496 - total_train_reward: -1370.1260\n",
      "Epoch 8332/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0556 - total_train_reward: -1212.5824\n",
      "Epoch 8333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2379 - total_train_reward: -1333.9474\n",
      "Epoch 8334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2449 - total_train_reward: -1285.1456\n",
      "Epoch 8335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5204 - total_train_reward: -1431.1197\n",
      "Epoch 8336/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5276 - total_train_reward: -1169.1142\n",
      "Epoch 8337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7655 - total_train_reward: -1513.9277\n",
      "Epoch 8338/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6924 - total_train_reward: -1259.5911\n",
      "Epoch 8339/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5564 - total_train_reward: -1392.1919\n",
      "Epoch 8340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3977 - total_train_reward: -1061.5231\n",
      "Epoch 8341/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6455 - total_train_reward: -1359.9392\n",
      "Epoch 8342/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1335 - total_train_reward: -1531.8457\n",
      "Epoch 8343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3359 - total_train_reward: -1110.3689\n",
      "Epoch 8344/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2055 - total_train_reward: -1702.7962\n",
      "Epoch 8345/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6785 - total_train_reward: -1500.6096\n",
      "Epoch 8346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1455 - total_train_reward: -1237.6035\n",
      "Epoch 8347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0342 - total_train_reward: -958.0486\n",
      "Epoch 8348/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6241 - total_train_reward: -1066.6200\n",
      "Epoch 8349/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1505 - total_train_reward: -956.5717\n",
      "Epoch 8350/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5775 - total_train_reward: -1282.8330\n",
      "Epoch 8351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0579 - total_train_reward: -1290.3374\n",
      "Epoch 8352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6409 - total_train_reward: -1713.2074\n",
      "Epoch 8353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2632 - total_train_reward: -1265.2753\n",
      "Epoch 8354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0993 - total_train_reward: -1286.6024\n",
      "Epoch 8355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.9575 - total_train_reward: -1083.2625\n",
      "Epoch 8356/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7457 - total_train_reward: -1274.6159\n",
      "Epoch 8357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6201 - total_train_reward: -1049.9992\n",
      "Epoch 8358/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1933 - total_train_reward: -1763.8672\n",
      "Epoch 8359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0628 - total_train_reward: -1066.3623\n",
      "Epoch 8360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1652 - total_train_reward: -1591.7275\n",
      "Epoch 8361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8578 - total_train_reward: -1719.9683\n",
      "Epoch 8362/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5468 - total_train_reward: -1307.9257\n",
      "Epoch 8363/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1804 - total_train_reward: -942.7337\n",
      "Epoch 8364/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6396 - total_train_reward: -1192.2906\n",
      "Epoch 8365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6991 - total_train_reward: -1245.4338\n",
      "Epoch 8366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4015 - total_train_reward: -1767.8980\n",
      "Epoch 8367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8805 - total_train_reward: -1063.2799\n",
      "Epoch 8368/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1157 - total_train_reward: -1271.4922\n",
      "Epoch 8369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1537 - total_train_reward: -1053.2163\n",
      "Epoch 8370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1448 - total_train_reward: -1577.7959\n",
      "Epoch 8371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4865 - total_train_reward: -1676.3619\n",
      "Epoch 8372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4151 - total_train_reward: -1258.1397\n",
      "Epoch 8373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3386 - total_train_reward: -1265.9558\n",
      "Epoch 8374/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9614 - total_train_reward: -957.8418\n",
      "Epoch 8375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2715 - total_train_reward: -1719.5785\n",
      "Epoch 8376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7711 - total_train_reward: -1278.4762\n",
      "Epoch 8377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4801 - total_train_reward: -808.5064\n",
      "Epoch 8378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0485 - total_train_reward: -1188.4251\n",
      "Epoch 8379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9018 - total_train_reward: -1682.6070\n",
      "Epoch 8380/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6391 - total_train_reward: -1177.1487\n",
      "Epoch 8381/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1759 - total_train_reward: -1063.6193\n",
      "Epoch 8382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7541 - total_train_reward: -1230.0991\n",
      "Epoch 8383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8567 - total_train_reward: -1408.5980\n",
      "Epoch 8384/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3380 - total_train_reward: -1312.8802\n",
      "Epoch 8385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6531 - total_train_reward: -724.4516\n",
      "Epoch 8386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8280 - total_train_reward: -970.7936\n",
      "Epoch 8387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3534 - total_train_reward: -1297.9822\n",
      "Epoch 8388/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6400 - total_train_reward: -1268.2219\n",
      "Epoch 8389/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7765 - total_train_reward: -1060.7281\n",
      "Epoch 8390/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9056 - total_train_reward: -1770.1364\n",
      "Epoch 8391/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.2985 - total_train_reward: -1741.8425\n",
      "Epoch 8392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6403 - total_train_reward: -1231.5931\n",
      "Epoch 8393/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1845 - total_train_reward: -1581.4749\n",
      "Epoch 8394/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0521 - total_train_reward: -1315.9238\n",
      "Epoch 8395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9396 - total_train_reward: -1065.3648\n",
      "Epoch 8396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5180 - total_train_reward: -1302.9321\n",
      "Epoch 8397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6943 - total_train_reward: -1770.4288\n",
      "Epoch 8398/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1554 - total_train_reward: -1656.4748\n",
      "Epoch 8399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9831 - total_train_reward: -1168.5294\n",
      "Epoch 8400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8903 - total_train_reward: -1067.0699\n",
      "Epoch 8401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6368 - total_train_reward: -1063.9334\n",
      "Epoch 8402/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6505 - total_train_reward: -1305.4972\n",
      "Epoch 8403/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8869 - total_train_reward: -1289.6438\n",
      "Epoch 8404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8308 - total_train_reward: -1801.2614\n",
      "Epoch 8405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7267 - total_train_reward: -1664.8161\n",
      "Epoch 8406/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1034 - total_train_reward: -1583.9129\n",
      "Epoch 8407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7366 - total_train_reward: -1495.4639\n",
      "Epoch 8408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8152 - total_train_reward: -1700.0879\n",
      "Epoch 8409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0291 - total_train_reward: -1066.6336\n",
      "Epoch 8410/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1645 - total_train_reward: -1276.9909\n",
      "Epoch 8411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8251 - total_train_reward: -1811.2910\n",
      "Epoch 8412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1752 - total_train_reward: -1276.2381\n",
      "Epoch 8413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9612 - total_train_reward: -1197.4873\n",
      "Epoch 8414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1639 - total_train_reward: -850.7734\n",
      "Epoch 8415/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5243 - total_train_reward: -1280.2411\n",
      "Epoch 8416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8746 - total_train_reward: -1218.2976\n",
      "Epoch 8417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0754 - total_train_reward: -1220.9301\n",
      "Epoch 8418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0038 - total_train_reward: -1312.0389\n",
      "Epoch 8419/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7023 - total_train_reward: -1218.0688\n",
      "Epoch 8420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0333 - total_train_reward: -1398.4298\n",
      "Epoch 8421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9564 - total_train_reward: -1716.9289\n",
      "Epoch 8422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7176 - total_train_reward: -1591.5850\n",
      "Epoch 8423/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1937 - total_train_reward: -949.4953\n",
      "Epoch 8424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0277 - total_train_reward: -1695.2626\n",
      "Epoch 8425/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4194 - total_train_reward: -953.9354\n",
      "Epoch 8426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.1084 - total_train_reward: -1281.0541\n",
      "Epoch 8427/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1857 - total_train_reward: -1294.2978\n",
      "Epoch 8428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6085 - total_train_reward: -1310.4856\n",
      "Epoch 8429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0349 - total_train_reward: -1623.2945\n",
      "Epoch 8430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3951 - total_train_reward: -1343.7230\n",
      "Epoch 8431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3873 - total_train_reward: -1462.0312\n",
      "Epoch 8432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6859 - total_train_reward: -1487.6284\n",
      "Epoch 8433/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5999 - total_train_reward: -1215.4643\n",
      "Epoch 8434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7429 - total_train_reward: -1351.7677\n",
      "Epoch 8435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2449 - total_train_reward: -1552.4974\n",
      "Epoch 8436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4992 - total_train_reward: -1134.8077\n",
      "Epoch 8437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6990 - total_train_reward: -1165.8799\n",
      "Epoch 8438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8779 - total_train_reward: -1635.2346\n",
      "Epoch 8439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3517 - total_train_reward: -1268.9449\n",
      "Epoch 8440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0377 - total_train_reward: -1705.1070\n",
      "Epoch 8441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5768 - total_train_reward: -1208.9595\n",
      "Epoch 8442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9884 - total_train_reward: -1259.9637\n",
      "Epoch 8443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5970 - total_train_reward: -1519.4740\n",
      "Epoch 8444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5726 - total_train_reward: -1018.4402\n",
      "Epoch 8445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6764 - total_train_reward: -1327.3829\n",
      "Epoch 8446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1024 - total_train_reward: -1576.4016\n",
      "Epoch 8447/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0497 - total_train_reward: -1058.8053\n",
      "Epoch 8448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4545 - total_train_reward: -1570.2164\n",
      "Epoch 8449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6934 - total_train_reward: -1330.9261\n",
      "Epoch 8450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0694 - total_train_reward: -1312.2436\n",
      "Epoch 8451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1621 - total_train_reward: -1139.7778\n",
      "Epoch 8452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4623 - total_train_reward: -1515.7862\n",
      "Epoch 8453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9339 - total_train_reward: -1727.3082\n",
      "Epoch 8454/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5448 - total_train_reward: -1459.8593\n",
      "Epoch 8455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3691 - total_train_reward: -1124.9190\n",
      "Epoch 8456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2126 - total_train_reward: -1265.9233\n",
      "Epoch 8457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1410 - total_train_reward: -1470.7115\n",
      "Epoch 8458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8571 - total_train_reward: -1132.4565\n",
      "Epoch 8459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6618 - total_train_reward: -1309.9399\n",
      "Epoch 8460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3700 - total_train_reward: -1047.9178\n",
      "Epoch 8461/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4536 - total_train_reward: -1336.6612\n",
      "Epoch 8462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9789 - total_train_reward: -1347.8600\n",
      "Epoch 8463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5632 - total_train_reward: -934.3661\n",
      "Epoch 8464/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1034 - total_train_reward: -1147.5197\n",
      "Epoch 8465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8362 - total_train_reward: -1585.4567\n",
      "Epoch 8466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2507 - total_train_reward: -1061.7944\n",
      "Epoch 8467/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2776 - total_train_reward: -1793.6800\n",
      "Epoch 8468/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1089 - total_train_reward: -1306.7120\n",
      "Epoch 8469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5993 - total_train_reward: -1252.2912\n",
      "Epoch 8470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4170 - total_train_reward: -1397.9998\n",
      "Epoch 8471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0254 - total_train_reward: -931.8785\n",
      "Epoch 8472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0299 - total_train_reward: -954.4750\n",
      "Epoch 8473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6413 - total_train_reward: -1083.5528\n",
      "Epoch 8474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5301 - total_train_reward: -1330.6443\n",
      "Epoch 8475/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3953 - total_train_reward: -1635.4877\n",
      "Epoch 8476/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7581 - total_train_reward: -1326.8652\n",
      "Epoch 8477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1775 - total_train_reward: -1239.3782\n",
      "Epoch 8478/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.6819 - total_train_reward: -1232.1662\n",
      "Epoch 8479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2270 - total_train_reward: -1263.7883\n",
      "Epoch 8480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3681 - total_train_reward: -1072.3407\n",
      "Epoch 8481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7956 - total_train_reward: -1516.3268\n",
      "Epoch 8482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4751 - total_train_reward: -1317.2668\n",
      "Epoch 8483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9934 - total_train_reward: -953.9424\n",
      "Epoch 8484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0068 - total_train_reward: -1792.3481\n",
      "Epoch 8485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6256 - total_train_reward: -1330.6848\n",
      "Epoch 8486/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8282 - total_train_reward: -1166.5310\n",
      "Epoch 8487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8729 - total_train_reward: -1336.9591\n",
      "Epoch 8488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5335 - total_train_reward: -1331.6934\n",
      "Epoch 8489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9514 - total_train_reward: -1291.2472\n",
      "Epoch 8490/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0817 - total_train_reward: -1380.0232\n",
      "Epoch 8491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6328 - total_train_reward: -1305.6263\n",
      "Epoch 8492/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4835 - total_train_reward: -1404.5114\n",
      "Epoch 8493/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5258 - total_train_reward: -1304.6681\n",
      "Epoch 8494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5774 - total_train_reward: -1044.3051\n",
      "Epoch 8495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7506 - total_train_reward: -1782.9860\n",
      "Epoch 8496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0821 - total_train_reward: -1337.0696\n",
      "Epoch 8497/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5807 - total_train_reward: -1487.1498\n",
      "Epoch 8498/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -10.2941 - total_train_reward: -1395.4885\n",
      "Epoch 8499/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4064 - total_train_reward: -1782.5466\n",
      "Epoch 8500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8288 - total_train_reward: -1185.9339\n",
      "Epoch 8501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5552 - total_train_reward: -1762.1537\n",
      "Epoch 8502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2604 - total_train_reward: -1713.8068\n",
      "Epoch 8503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3071 - total_train_reward: -1471.9765\n",
      "Epoch 8504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4636 - total_train_reward: -1317.3896\n",
      "Epoch 8505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1434 - total_train_reward: -1786.7164\n",
      "Epoch 8506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1097 - total_train_reward: -1508.4176\n",
      "Epoch 8507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7129 - total_train_reward: -1343.2215\n",
      "Epoch 8508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7580 - total_train_reward: -1080.7216\n",
      "Epoch 8509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8753 - total_train_reward: -1747.5762\n",
      "Epoch 8510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5242 - total_train_reward: -1329.1946\n",
      "Epoch 8511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5308 - total_train_reward: -1348.2709\n",
      "Epoch 8512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8433 - total_train_reward: -1009.3596\n",
      "Epoch 8513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5516 - total_train_reward: -1326.8267\n",
      "Epoch 8514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3697 - total_train_reward: -1351.7454\n",
      "Epoch 8515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4093 - total_train_reward: -1316.4183\n",
      "Epoch 8516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5270 - total_train_reward: -1607.1152\n",
      "Epoch 8517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2475 - total_train_reward: -1354.0343\n",
      "Epoch 8518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6090 - total_train_reward: -953.1910\n",
      "Epoch 8519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4578 - total_train_reward: -1194.2962\n",
      "Epoch 8520/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4269 - total_train_reward: -1041.6360\n",
      "Epoch 8521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5377 - total_train_reward: -1753.2554\n",
      "Epoch 8522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1283 - total_train_reward: -1448.9842\n",
      "Epoch 8523/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7703 - total_train_reward: -1331.3751\n",
      "Epoch 8524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5071 - total_train_reward: -1061.4667\n",
      "Epoch 8525/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7365 - total_train_reward: -1786.0916\n",
      "Epoch 8526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1700 - total_train_reward: -952.9456\n",
      "Epoch 8527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4814 - total_train_reward: -1053.9681\n",
      "Epoch 8528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.5288 - total_train_reward: -950.9172\n",
      "Epoch 8529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7871 - total_train_reward: -1295.9927\n",
      "Epoch 8530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9329 - total_train_reward: -1089.3078\n",
      "Epoch 8531/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8332 - total_train_reward: -1232.1486\n",
      "Epoch 8532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4983 - total_train_reward: -1327.7197\n",
      "Epoch 8533/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9854 - total_train_reward: -1348.5263\n",
      "Epoch 8534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5743 - total_train_reward: -1765.2051\n",
      "Epoch 8535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4603 - total_train_reward: -1583.7018\n",
      "Epoch 8536/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6383 - total_train_reward: -1002.8202\n",
      "Epoch 8537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2635 - total_train_reward: -1307.3937\n",
      "Epoch 8538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9925 - total_train_reward: -1586.5239\n",
      "Epoch 8539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3281 - total_train_reward: -1345.1419\n",
      "Epoch 8540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6300 - total_train_reward: -1304.2155\n",
      "Epoch 8541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9742 - total_train_reward: -1348.3172\n",
      "Epoch 8542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0216 - total_train_reward: -1753.3494\n",
      "Epoch 8543/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9152 - total_train_reward: -1312.6166\n",
      "Epoch 8544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6212 - total_train_reward: -1354.3004\n",
      "Epoch 8545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7593 - total_train_reward: -952.3261\n",
      "Epoch 8546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4845 - total_train_reward: -1144.3850\n",
      "Epoch 8547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0078 - total_train_reward: -1401.5070\n",
      "Epoch 8548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2132 - total_train_reward: -1605.6742\n",
      "Epoch 8549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8805 - total_train_reward: -1449.6819\n",
      "Epoch 8550/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6767 - total_train_reward: -1261.6597\n",
      "Epoch 8551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0911 - total_train_reward: -940.1741\n",
      "Epoch 8552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3023 - total_train_reward: -1558.8139\n",
      "Epoch 8553/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5658 - total_train_reward: -1499.8988\n",
      "Epoch 8554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4414 - total_train_reward: -1057.8283\n",
      "Epoch 8555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3980 - total_train_reward: -1682.4580\n",
      "Epoch 8556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1963 - total_train_reward: -910.5535\n",
      "Epoch 8557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7080 - total_train_reward: -1238.0064\n",
      "Epoch 8558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5538 - total_train_reward: -1268.0553\n",
      "Epoch 8559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2897 - total_train_reward: -1341.5232\n",
      "Epoch 8560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8749 - total_train_reward: -1482.5762\n",
      "Epoch 8561/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1385 - total_train_reward: -1430.6383\n",
      "Epoch 8562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7544 - total_train_reward: -1548.9888\n",
      "Epoch 8563/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1408 - total_train_reward: -1232.0825\n",
      "Epoch 8564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9424 - total_train_reward: -1245.0836\n",
      "Epoch 8565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6735 - total_train_reward: -1167.1357\n",
      "Epoch 8566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5183 - total_train_reward: -957.3578\n",
      "Epoch 8567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7999 - total_train_reward: -1334.5898\n",
      "Epoch 8568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0171 - total_train_reward: -1098.2111\n",
      "Epoch 8569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2426 - total_train_reward: -1334.9831\n",
      "Epoch 8570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9852 - total_train_reward: -1389.6967\n",
      "Epoch 8571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6858 - total_train_reward: -1355.5876\n",
      "Epoch 8572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7415 - total_train_reward: -1188.7300\n",
      "Epoch 8573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9942 - total_train_reward: -1781.8637\n",
      "Epoch 8574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7815 - total_train_reward: -949.4498\n",
      "Epoch 8575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0903 - total_train_reward: -1060.4487\n",
      "Epoch 8576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9639 - total_train_reward: -1354.7417\n",
      "Epoch 8577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3733 - total_train_reward: -1775.1710\n",
      "Epoch 8578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1377 - total_train_reward: -1508.8750\n",
      "Epoch 8579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4157 - total_train_reward: -767.8985\n",
      "Epoch 8580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3607 - total_train_reward: -1775.3859\n",
      "Epoch 8581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2419 - total_train_reward: -1217.9325\n",
      "Epoch 8582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3825 - total_train_reward: -1061.6346\n",
      "Epoch 8583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.2663 - total_train_reward: -1228.2876\n",
      "Epoch 8584/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5734 - total_train_reward: -1781.9571\n",
      "Epoch 8585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3542 - total_train_reward: -1536.3490\n",
      "Epoch 8586/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5696 - total_train_reward: -1770.2684\n",
      "Epoch 8587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2692 - total_train_reward: -1165.7347\n",
      "Epoch 8588/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3768 - total_train_reward: -945.0492\n",
      "Epoch 8589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5606 - total_train_reward: -1359.7750\n",
      "Epoch 8590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3687 - total_train_reward: -1255.1299\n",
      "Epoch 8591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3056 - total_train_reward: -1029.8456\n",
      "Epoch 8592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9370 - total_train_reward: -1387.9373\n",
      "Epoch 8593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5218 - total_train_reward: -1345.4131\n",
      "Epoch 8594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0162 - total_train_reward: -1307.6550\n",
      "Epoch 8595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0718 - total_train_reward: -952.9175\n",
      "Epoch 8596/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4349 - total_train_reward: -1605.9199\n",
      "Epoch 8597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0941 - total_train_reward: -1308.2480\n",
      "Epoch 8598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0293 - total_train_reward: -1765.9648\n",
      "Epoch 8599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7734 - total_train_reward: -1062.0683\n",
      "Epoch 8600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4576 - total_train_reward: -1470.5175\n",
      "Epoch 8601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7427 - total_train_reward: -1353.0602\n",
      "Epoch 8602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2539 - total_train_reward: -1205.3911\n",
      "Epoch 8603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8547 - total_train_reward: -1066.1205\n",
      "Epoch 8604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0331 - total_train_reward: -1003.0747\n",
      "Epoch 8605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8881 - total_train_reward: -1285.0006\n",
      "Epoch 8606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0406 - total_train_reward: -1159.6172\n",
      "Epoch 8607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6798 - total_train_reward: -1784.2581\n",
      "Epoch 8608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6530 - total_train_reward: -1556.2572\n",
      "Epoch 8609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7174 - total_train_reward: -1581.5457\n",
      "Epoch 8610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2404 - total_train_reward: -1054.9166\n",
      "Epoch 8611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1772 - total_train_reward: -1607.5238\n",
      "Epoch 8612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.8746 - total_train_reward: -1231.8436\n",
      "Epoch 8613/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2573 - total_train_reward: -1466.6813\n",
      "Epoch 8614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0354 - total_train_reward: -1194.9027\n",
      "Epoch 8615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5923 - total_train_reward: -1158.4885\n",
      "Epoch 8616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3732 - total_train_reward: -1778.9026\n",
      "Epoch 8617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1893 - total_train_reward: -1675.7528\n",
      "Epoch 8618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7682 - total_train_reward: -1342.2755\n",
      "Epoch 8619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7870 - total_train_reward: -1311.6298\n",
      "Epoch 8620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8918 - total_train_reward: -1322.3994\n",
      "Epoch 8621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6479 - total_train_reward: -1254.7816\n",
      "Epoch 8622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5429 - total_train_reward: -1336.6106\n",
      "Epoch 8623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8529 - total_train_reward: -992.8850\n",
      "Epoch 8624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6722 - total_train_reward: -1369.0779\n",
      "Epoch 8625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4092 - total_train_reward: -1049.6599\n",
      "Epoch 8626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.6276 - total_train_reward: -1233.2709\n",
      "Epoch 8627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4226 - total_train_reward: -1341.7263\n",
      "Epoch 8628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1595 - total_train_reward: -1282.5305\n",
      "Epoch 8629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1490 - total_train_reward: -1281.2111\n",
      "Epoch 8630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2257 - total_train_reward: -1349.2634\n",
      "Epoch 8631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2423 - total_train_reward: -1541.5220\n",
      "Epoch 8632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2598 - total_train_reward: -1673.9670\n",
      "Epoch 8633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1188 - total_train_reward: -1148.0989\n",
      "Epoch 8634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8647 - total_train_reward: -1108.0301\n",
      "Epoch 8635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1579 - total_train_reward: -721.4795\n",
      "Epoch 8636/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5964 - total_train_reward: -955.0109\n",
      "Epoch 8637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4732 - total_train_reward: -1657.5411\n",
      "Epoch 8638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0905 - total_train_reward: -1384.4314\n",
      "Epoch 8639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2315 - total_train_reward: -1340.8487\n",
      "Epoch 8640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.5960 - total_train_reward: -1553.8814\n",
      "Epoch 8641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4164 - total_train_reward: -1370.5041\n",
      "Epoch 8642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4602 - total_train_reward: -1373.7682\n",
      "Epoch 8643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4188 - total_train_reward: -1674.8632\n",
      "Epoch 8644/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3103 - total_train_reward: -1262.0562\n",
      "Epoch 8645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7063 - total_train_reward: -1063.7703\n",
      "Epoch 8646/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6955 - total_train_reward: -1251.3562\n",
      "Epoch 8647/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1870 - total_train_reward: -1600.3547\n",
      "Epoch 8648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1647 - total_train_reward: -1092.3053\n",
      "Epoch 8649/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3948 - total_train_reward: -1449.7445\n",
      "Epoch 8650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.1894 - total_train_reward: -1215.7683\n",
      "Epoch 8651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5505 - total_train_reward: -1354.2431\n",
      "Epoch 8652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4198 - total_train_reward: -953.6023\n",
      "Epoch 8653/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.8901 - total_train_reward: -1229.7424\n",
      "Epoch 8654/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7741 - total_train_reward: -1352.5081\n",
      "Epoch 8655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9429 - total_train_reward: -1329.6768\n",
      "Epoch 8656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3543 - total_train_reward: -1214.4503\n",
      "Epoch 8657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0780 - total_train_reward: -1601.6630\n",
      "Epoch 8658/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7350 - total_train_reward: -840.8767\n",
      "Epoch 8659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1163 - total_train_reward: -1347.2912\n",
      "Epoch 8660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7349 - total_train_reward: -1700.6318\n",
      "Epoch 8661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4575 - total_train_reward: -1397.9625\n",
      "Epoch 8662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8953 - total_train_reward: -1773.1984\n",
      "Epoch 8663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1792 - total_train_reward: -1672.3551\n",
      "Epoch 8664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9191 - total_train_reward: -1205.2054\n",
      "Epoch 8665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2009 - total_train_reward: -1356.6157\n",
      "Epoch 8666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6489 - total_train_reward: -1333.3958\n",
      "Epoch 8667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8362 - total_train_reward: -1352.5194\n",
      "Epoch 8668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4699 - total_train_reward: -843.4534\n",
      "Epoch 8669/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8334 - total_train_reward: -1223.6831\n",
      "Epoch 8670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7620 - total_train_reward: -933.8519\n",
      "Epoch 8671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1274 - total_train_reward: -837.4738\n",
      "Epoch 8672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.1046 - total_train_reward: -1097.3816\n",
      "Epoch 8673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4177 - total_train_reward: -1154.2568\n",
      "Epoch 8674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1945 - total_train_reward: -1185.4846\n",
      "Epoch 8675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7069 - total_train_reward: -1315.8948\n",
      "Epoch 8676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3412 - total_train_reward: -1416.0388\n",
      "Epoch 8677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9529 - total_train_reward: -956.3638\n",
      "Epoch 8678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7154 - total_train_reward: -1540.8094\n",
      "Epoch 8679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1094 - total_train_reward: -1624.4715\n",
      "Epoch 8680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4712 - total_train_reward: -721.7262\n",
      "Epoch 8681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3768 - total_train_reward: -1167.4305\n",
      "Epoch 8682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1328 - total_train_reward: -1512.1944\n",
      "Epoch 8683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3005 - total_train_reward: -1655.2572\n",
      "Epoch 8684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5951 - total_train_reward: -1162.0125\n",
      "Epoch 8685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6097 - total_train_reward: -1499.0135\n",
      "Epoch 8686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5045 - total_train_reward: -1620.1477\n",
      "Epoch 8687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5928 - total_train_reward: -1138.6836\n",
      "Epoch 8688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0952 - total_train_reward: -1390.4890\n",
      "Epoch 8689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.9625 - total_train_reward: -1311.5840\n",
      "Epoch 8690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7356 - total_train_reward: -952.2410\n",
      "Epoch 8691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8260 - total_train_reward: -952.8048\n",
      "Epoch 8692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4553 - total_train_reward: -1529.1190\n",
      "Epoch 8693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3526 - total_train_reward: -1639.4013\n",
      "Epoch 8694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3222 - total_train_reward: -1187.0920\n",
      "Epoch 8695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8483 - total_train_reward: -1534.3576\n",
      "Epoch 8696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3304 - total_train_reward: -1630.5153\n",
      "Epoch 8697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0165 - total_train_reward: -1455.3319\n",
      "Epoch 8698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9471 - total_train_reward: -1024.0696\n",
      "Epoch 8699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0036 - total_train_reward: -1263.8560\n",
      "Epoch 8700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8216 - total_train_reward: -1530.8590\n",
      "Epoch 8701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2246 - total_train_reward: -1561.0549\n",
      "Epoch 8702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1944 - total_train_reward: -1287.6596\n",
      "Epoch 8703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2028 - total_train_reward: -1477.2423\n",
      "Epoch 8704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2827 - total_train_reward: -843.4812\n",
      "Epoch 8705/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0556 - total_train_reward: -1109.6525\n",
      "Epoch 8706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7273 - total_train_reward: -1426.3800\n",
      "Epoch 8707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3459 - total_train_reward: -1512.8372\n",
      "Epoch 8708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0616 - total_train_reward: -1193.3932\n",
      "Epoch 8709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.8901 - total_train_reward: -1234.1349\n",
      "Epoch 8710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2083 - total_train_reward: -951.5608\n",
      "Epoch 8711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7653 - total_train_reward: -1166.2753\n",
      "Epoch 8712/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4588 - total_train_reward: -1301.3010\n",
      "Epoch 8713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9920 - total_train_reward: -1356.1701\n",
      "Epoch 8714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4251 - total_train_reward: -1064.6398\n",
      "Epoch 8715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2378 - total_train_reward: -1673.6894\n",
      "Epoch 8716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6929 - total_train_reward: -1070.0988\n",
      "Epoch 8717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8385 - total_train_reward: -1363.2524\n",
      "Epoch 8718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3441 - total_train_reward: -1375.9294\n",
      "Epoch 8719/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1711 - total_train_reward: -1760.7686\n",
      "Epoch 8720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3989 - total_train_reward: -1672.0873\n",
      "Epoch 8721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 17.1573 - total_train_reward: -1357.5225\n",
      "Epoch 8722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5967 - total_train_reward: -1360.7330\n",
      "Epoch 8723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0578 - total_train_reward: -1269.1683\n",
      "Epoch 8724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7083 - total_train_reward: -1239.6937\n",
      "Epoch 8725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4342 - total_train_reward: -1070.4329\n",
      "Epoch 8726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8390 - total_train_reward: -1324.7518\n",
      "Epoch 8727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6126 - total_train_reward: -1126.8554\n",
      "Epoch 8728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9205 - total_train_reward: -1165.7017\n",
      "Epoch 8729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9593 - total_train_reward: -1287.5711\n",
      "Epoch 8730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7060 - total_train_reward: -1753.0252\n",
      "Epoch 8731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0742 - total_train_reward: -1641.4320\n",
      "Epoch 8732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6503 - total_train_reward: -1273.9608\n",
      "Epoch 8733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3009 - total_train_reward: -1361.9115\n",
      "Epoch 8734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8667 - total_train_reward: -1112.6921\n",
      "Epoch 8735/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0824 - total_train_reward: -952.2787\n",
      "Epoch 8736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8931 - total_train_reward: -1561.4464\n",
      "Epoch 8737/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1388 - total_train_reward: -1056.0526\n",
      "Epoch 8738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7934 - total_train_reward: -838.7815\n",
      "Epoch 8739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8324 - total_train_reward: -1388.0651\n",
      "Epoch 8740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9572 - total_train_reward: -1771.6047\n",
      "Epoch 8741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3798 - total_train_reward: -1345.4486\n",
      "Epoch 8742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7315 - total_train_reward: -1358.5319\n",
      "Epoch 8743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1828 - total_train_reward: -1177.4526\n",
      "Epoch 8744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2555 - total_train_reward: -1473.1324\n",
      "Epoch 8745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8274 - total_train_reward: -1524.7963\n",
      "Epoch 8746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9903 - total_train_reward: -1353.8355\n",
      "Epoch 8747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3714 - total_train_reward: -1314.4148\n",
      "Epoch 8748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4447 - total_train_reward: -1358.6315\n",
      "Epoch 8749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4547 - total_train_reward: -1109.0683\n",
      "Epoch 8750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8496 - total_train_reward: -758.8535\n",
      "Epoch 8751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4755 - total_train_reward: -1366.4313\n",
      "Epoch 8752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6827 - total_train_reward: -1578.9490\n",
      "Epoch 8753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3234 - total_train_reward: -1343.7012\n",
      "Epoch 8754/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5580 - total_train_reward: -1723.6908\n",
      "Epoch 8755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2070 - total_train_reward: -1590.5116\n",
      "Epoch 8756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2499 - total_train_reward: -1349.7255\n",
      "Epoch 8757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0831 - total_train_reward: -1347.2625\n",
      "Epoch 8758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9616 - total_train_reward: -1354.7773\n",
      "Epoch 8759/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8174 - total_train_reward: -1054.0042\n",
      "Epoch 8760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8293 - total_train_reward: -1311.0404\n",
      "Epoch 8761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7507 - total_train_reward: -1652.7854\n",
      "Epoch 8762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0691 - total_train_reward: -1354.5954\n",
      "Epoch 8763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3137 - total_train_reward: -1259.9752\n",
      "Epoch 8764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2035 - total_train_reward: -1699.3246\n",
      "Epoch 8765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.4333 - total_train_reward: -1260.2809\n",
      "Epoch 8766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8714 - total_train_reward: -1364.0625\n",
      "Epoch 8767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5258 - total_train_reward: -1668.8212\n",
      "Epoch 8768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5040 - total_train_reward: -1174.6263\n",
      "Epoch 8769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5805 - total_train_reward: -1378.1898\n",
      "Epoch 8770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9701 - total_train_reward: -1090.8577\n",
      "Epoch 8771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4380 - total_train_reward: -1170.8707\n",
      "Epoch 8772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1626 - total_train_reward: -1279.4078\n",
      "Epoch 8773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1339 - total_train_reward: -1703.4454\n",
      "Epoch 8774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7925 - total_train_reward: -1694.2893\n",
      "Epoch 8775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1352 - total_train_reward: -1353.8882\n",
      "Epoch 8776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3239 - total_train_reward: -1777.6472\n",
      "Epoch 8777/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7250 - total_train_reward: -1343.3656\n",
      "Epoch 8778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.8971 - total_train_reward: -1532.7760\n",
      "Epoch 8779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0924 - total_train_reward: -951.2652\n",
      "Epoch 8780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5396 - total_train_reward: -1360.0646\n",
      "Epoch 8781/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2699 - total_train_reward: -1758.7698\n",
      "Epoch 8782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5745 - total_train_reward: -1328.0986\n",
      "Epoch 8783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4596 - total_train_reward: -1365.1018\n",
      "Epoch 8784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6266 - total_train_reward: -1160.4113\n",
      "Epoch 8785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6717 - total_train_reward: -1204.5966\n",
      "Epoch 8786/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1820 - total_train_reward: -1383.0828\n",
      "Epoch 8787/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8964 - total_train_reward: -1501.7728\n",
      "Epoch 8788/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3848 - total_train_reward: -1370.4659\n",
      "Epoch 8789/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7586 - total_train_reward: -1359.8435\n",
      "Epoch 8790/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0809 - total_train_reward: -1035.0158\n",
      "Epoch 8791/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0544 - total_train_reward: -1306.0078\n",
      "Epoch 8792/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8099 - total_train_reward: -1726.4934\n",
      "Epoch 8793/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0435 - total_train_reward: -1273.3017\n",
      "Epoch 8794/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6790 - total_train_reward: -850.3089\n",
      "Epoch 8795/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6075 - total_train_reward: -1364.4000\n",
      "Epoch 8796/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9787 - total_train_reward: -1360.3455\n",
      "Epoch 8797/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3667 - total_train_reward: -1442.0202\n",
      "Epoch 8798/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4496 - total_train_reward: -1677.0651\n",
      "Epoch 8799/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3178 - total_train_reward: -1229.1448\n",
      "Epoch 8800/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7791 - total_train_reward: -1413.5970\n",
      "Epoch 8801/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.4379 - total_train_reward: -1321.1583\n",
      "Epoch 8802/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5327 - total_train_reward: -1755.6342\n",
      "Epoch 8803/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9775 - total_train_reward: -1515.0359\n",
      "Epoch 8804/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9988 - total_train_reward: -1300.7961\n",
      "Epoch 8805/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5646 - total_train_reward: -1513.4615\n",
      "Epoch 8806/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.5511 - total_train_reward: -1346.9129\n",
      "Epoch 8807/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6030 - total_train_reward: -1099.4587\n",
      "Epoch 8808/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0003 - total_train_reward: -1365.0927\n",
      "Epoch 8809/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4858 - total_train_reward: -1361.3073\n",
      "Epoch 8810/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5748 - total_train_reward: -1002.1321\n",
      "Epoch 8811/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2673 - total_train_reward: -1368.6745\n",
      "Epoch 8812/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8361 - total_train_reward: -1172.3673\n",
      "Epoch 8813/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7375 - total_train_reward: -1719.4944\n",
      "Epoch 8814/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9336 - total_train_reward: -1365.2763\n",
      "Epoch 8815/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7791 - total_train_reward: -1344.1326\n",
      "Epoch 8816/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0854 - total_train_reward: -1746.0563\n",
      "Epoch 8817/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3032 - total_train_reward: -1360.5568\n",
      "Epoch 8818/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7766 - total_train_reward: -1338.1626\n",
      "Epoch 8819/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4347 - total_train_reward: -1314.3412\n",
      "Epoch 8820/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4245 - total_train_reward: -1322.2511\n",
      "Epoch 8821/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6249 - total_train_reward: -1180.8172\n",
      "Epoch 8822/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1517 - total_train_reward: -1761.0153\n",
      "Epoch 8823/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5072 - total_train_reward: -1717.4900\n",
      "Epoch 8824/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1090 - total_train_reward: -1368.3525\n",
      "Epoch 8825/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8936 - total_train_reward: -1737.0367\n",
      "Epoch 8826/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4529 - total_train_reward: -1245.4461\n",
      "Epoch 8827/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.2066 - total_train_reward: -1517.8770\n",
      "Epoch 8828/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0661 - total_train_reward: -1284.3489\n",
      "Epoch 8829/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9922 - total_train_reward: -1364.1030\n",
      "Epoch 8830/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5334 - total_train_reward: -1667.4434\n",
      "Epoch 8831/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6697 - total_train_reward: -1101.1186\n",
      "Epoch 8832/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5857 - total_train_reward: -1304.2220\n",
      "Epoch 8833/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0255 - total_train_reward: -1365.8894\n",
      "Epoch 8834/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0472 - total_train_reward: -1318.1390\n",
      "Epoch 8835/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4473 - total_train_reward: -1347.9402\n",
      "Epoch 8836/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0437 - total_train_reward: -1409.7861\n",
      "Epoch 8837/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1351 - total_train_reward: -1344.7136\n",
      "Epoch 8838/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1773 - total_train_reward: -1286.2267\n",
      "Epoch 8839/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1471 - total_train_reward: -1276.1455\n",
      "Epoch 8840/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7713 - total_train_reward: -1348.5779\n",
      "Epoch 8841/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1121 - total_train_reward: -1407.3351\n",
      "Epoch 8842/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5852 - total_train_reward: -1347.9352\n",
      "Epoch 8843/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5905 - total_train_reward: -1180.5308\n",
      "Epoch 8844/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4464 - total_train_reward: -1354.1518\n",
      "Epoch 8845/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0159 - total_train_reward: -1192.0827\n",
      "Epoch 8846/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1889 - total_train_reward: -1123.7716\n",
      "Epoch 8847/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2490 - total_train_reward: -1257.3029\n",
      "Epoch 8848/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6722 - total_train_reward: -1367.7385\n",
      "Epoch 8849/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9747 - total_train_reward: -1235.7341\n",
      "Epoch 8850/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6519 - total_train_reward: -1356.0598\n",
      "Epoch 8851/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3348 - total_train_reward: -1368.0081\n",
      "Epoch 8852/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0551 - total_train_reward: -1344.9692\n",
      "Epoch 8853/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6161 - total_train_reward: -1595.1888\n",
      "Epoch 8854/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1801 - total_train_reward: -1060.1598\n",
      "Epoch 8855/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5713 - total_train_reward: -1364.9261\n",
      "Epoch 8856/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0849 - total_train_reward: -1300.5067\n",
      "Epoch 8857/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2550 - total_train_reward: -1066.1436\n",
      "Epoch 8858/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2048 - total_train_reward: -1211.0920\n",
      "Epoch 8859/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5995 - total_train_reward: -883.0430\n",
      "Epoch 8860/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3805 - total_train_reward: -1351.0227\n",
      "Epoch 8861/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8273 - total_train_reward: -1172.4953\n",
      "Epoch 8862/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9099 - total_train_reward: -1186.6757\n",
      "Epoch 8863/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.4993 - total_train_reward: -1339.3910\n",
      "Epoch 8864/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9280 - total_train_reward: -1022.8879\n",
      "Epoch 8865/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3472 - total_train_reward: -1175.0854\n",
      "Epoch 8866/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8326 - total_train_reward: -1355.3705\n",
      "Epoch 8867/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6512 - total_train_reward: -1761.7290\n",
      "Epoch 8868/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5125 - total_train_reward: -1363.6455\n",
      "Epoch 8869/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5277 - total_train_reward: -1206.3435\n",
      "Epoch 8870/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8159 - total_train_reward: -1363.3337\n",
      "Epoch 8871/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5534 - total_train_reward: -1242.0722\n",
      "Epoch 8872/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8552 - total_train_reward: -1158.9246\n",
      "Epoch 8873/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7055 - total_train_reward: -1546.8214\n",
      "Epoch 8874/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4354 - total_train_reward: -1490.6711\n",
      "Epoch 8875/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8447 - total_train_reward: -1349.1152\n",
      "Epoch 8876/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2867 - total_train_reward: -719.3100\n",
      "Epoch 8877/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4159 - total_train_reward: -1736.0070\n",
      "Epoch 8878/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2761 - total_train_reward: -1771.2495\n",
      "Epoch 8879/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9068 - total_train_reward: -1074.6253\n",
      "Epoch 8880/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4990 - total_train_reward: -1320.6177\n",
      "Epoch 8881/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4291 - total_train_reward: -1363.3839\n",
      "Epoch 8882/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0577 - total_train_reward: -1386.5986\n",
      "Epoch 8883/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0336 - total_train_reward: -1749.1503\n",
      "Epoch 8884/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1341 - total_train_reward: -1166.3798\n",
      "Epoch 8885/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4441 - total_train_reward: -1643.6832\n",
      "Epoch 8886/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1370 - total_train_reward: -1765.9910\n",
      "Epoch 8887/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1302 - total_train_reward: -1537.1408\n",
      "Epoch 8888/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0623 - total_train_reward: -1352.5072\n",
      "Epoch 8889/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4071 - total_train_reward: -1357.7712\n",
      "Epoch 8890/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8266 - total_train_reward: -1539.0233\n",
      "Epoch 8891/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9871 - total_train_reward: -1553.4113\n",
      "Epoch 8892/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1961 - total_train_reward: -1335.1200\n",
      "Epoch 8893/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4388 - total_train_reward: -1074.1478\n",
      "Epoch 8894/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6166 - total_train_reward: -1270.3810\n",
      "Epoch 8895/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9415 - total_train_reward: -1159.2647\n",
      "Epoch 8896/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8909 - total_train_reward: -1648.5932\n",
      "Epoch 8897/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3056 - total_train_reward: -1653.3483\n",
      "Epoch 8898/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8308 - total_train_reward: -1299.6969\n",
      "Epoch 8899/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8293 - total_train_reward: -1361.4041\n",
      "Epoch 8900/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.8150 - total_train_reward: -1243.8380\n",
      "Epoch 8901/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.0986 - total_train_reward: -1287.6240\n",
      "Epoch 8902/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6836 - total_train_reward: -1092.9797\n",
      "Epoch 8903/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0383 - total_train_reward: -1408.5275\n",
      "Epoch 8904/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5534 - total_train_reward: -930.8605\n",
      "Epoch 8905/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1239 - total_train_reward: -1151.2980\n",
      "Epoch 8906/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0387 - total_train_reward: -1226.4463\n",
      "Epoch 8907/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3564 - total_train_reward: -1291.4591\n",
      "Epoch 8908/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6019 - total_train_reward: -961.0376\n",
      "Epoch 8909/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9166 - total_train_reward: -1347.5873\n",
      "Epoch 8910/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1404 - total_train_reward: -1588.5507\n",
      "Epoch 8911/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2191 - total_train_reward: -1275.2706\n",
      "Epoch 8912/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0996 - total_train_reward: -1201.7085\n",
      "Epoch 8913/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2570 - total_train_reward: -856.9791\n",
      "Epoch 8914/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3138 - total_train_reward: -1060.1864\n",
      "Epoch 8915/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0066 - total_train_reward: -1346.6305\n",
      "Epoch 8916/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4155 - total_train_reward: -958.6091\n",
      "Epoch 8917/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7898 - total_train_reward: -1609.6409\n",
      "Epoch 8918/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1957 - total_train_reward: -1352.3590\n",
      "Epoch 8919/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9360 - total_train_reward: -1303.1339\n",
      "Epoch 8920/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1419 - total_train_reward: -750.8400\n",
      "Epoch 8921/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9949 - total_train_reward: -1266.3672\n",
      "Epoch 8922/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6330 - total_train_reward: -1174.5289\n",
      "Epoch 8923/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2262 - total_train_reward: -834.3995\n",
      "Epoch 8924/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.5096 - total_train_reward: -1653.6902\n",
      "Epoch 8925/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3353 - total_train_reward: -1286.8777\n",
      "Epoch 8926/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7087 - total_train_reward: -1476.6300\n",
      "Epoch 8927/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9180 - total_train_reward: -1345.6528\n",
      "Epoch 8928/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0438 - total_train_reward: -1715.2834\n",
      "Epoch 8929/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.8114 - total_train_reward: -1280.1829\n",
      "Epoch 8930/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1486 - total_train_reward: -845.0323\n",
      "Epoch 8931/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6442 - total_train_reward: -1322.5129\n",
      "Epoch 8932/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.1365 - total_train_reward: -1486.9140\n",
      "Epoch 8933/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8882 - total_train_reward: -1272.7453\n",
      "Epoch 8934/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9751 - total_train_reward: -953.5384\n",
      "Epoch 8935/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7553 - total_train_reward: -1739.6682\n",
      "Epoch 8936/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5502 - total_train_reward: -1767.7689\n",
      "Epoch 8937/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2587 - total_train_reward: -1347.9634\n",
      "Epoch 8938/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.9336 - total_train_reward: -1229.4772\n",
      "Epoch 8939/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2789 - total_train_reward: -1061.3469\n",
      "Epoch 8940/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8300 - total_train_reward: -1354.4422\n",
      "Epoch 8941/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1076 - total_train_reward: -964.6120\n",
      "Epoch 8942/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6561 - total_train_reward: -1353.2773\n",
      "Epoch 8943/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6050 - total_train_reward: -1353.7107\n",
      "Epoch 8944/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2066 - total_train_reward: -1433.5728\n",
      "Epoch 8945/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5078 - total_train_reward: -1312.2724\n",
      "Epoch 8946/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1823 - total_train_reward: -1634.4628\n",
      "Epoch 8947/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6276 - total_train_reward: -1521.9601\n",
      "Epoch 8948/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2990 - total_train_reward: -1205.5152\n",
      "Epoch 8949/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3922 - total_train_reward: -1460.9323\n",
      "Epoch 8950/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1174 - total_train_reward: -1422.3285\n",
      "Epoch 8951/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0895 - total_train_reward: -1642.7544\n",
      "Epoch 8952/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8198 - total_train_reward: -1340.0220\n",
      "Epoch 8953/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4931 - total_train_reward: -1596.6073\n",
      "Epoch 8954/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4330 - total_train_reward: -1338.8661\n",
      "Epoch 8955/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1960 - total_train_reward: -1774.9685\n",
      "Epoch 8956/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9962 - total_train_reward: -1269.7111\n",
      "Epoch 8957/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9649 - total_train_reward: -1257.0488\n",
      "Epoch 8958/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0862 - total_train_reward: -1300.4219\n",
      "Epoch 8959/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5766 - total_train_reward: -1441.1047\n",
      "Epoch 8960/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7210 - total_train_reward: -1211.4684\n",
      "Epoch 8961/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2774 - total_train_reward: -952.7765\n",
      "Epoch 8962/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6531 - total_train_reward: -958.6007\n",
      "Epoch 8963/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2998 - total_train_reward: -1059.9757\n",
      "Epoch 8964/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3869 - total_train_reward: -1163.4170\n",
      "Epoch 8965/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9653 - total_train_reward: -1109.7724\n",
      "Epoch 8966/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4770 - total_train_reward: -1341.0487\n",
      "Epoch 8967/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3046 - total_train_reward: -1313.8248\n",
      "Epoch 8968/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2433 - total_train_reward: -1038.7801\n",
      "Epoch 8969/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5624 - total_train_reward: -1331.1067\n",
      "Epoch 8970/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2064 - total_train_reward: -1348.6827\n",
      "Epoch 8971/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8637 - total_train_reward: -836.0244\n",
      "Epoch 8972/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0083 - total_train_reward: -1702.4807\n",
      "Epoch 8973/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5944 - total_train_reward: -1372.9823\n",
      "Epoch 8974/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5636 - total_train_reward: -953.5817\n",
      "Epoch 8975/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6082 - total_train_reward: -1319.0137\n",
      "Epoch 8976/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6918 - total_train_reward: -1576.9863\n",
      "Epoch 8977/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7062 - total_train_reward: -1306.9023\n",
      "Epoch 8978/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9052 - total_train_reward: -1355.7305\n",
      "Epoch 8979/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6924 - total_train_reward: -1355.5521\n",
      "Epoch 8980/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3618 - total_train_reward: -1519.1072\n",
      "Epoch 8981/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1567 - total_train_reward: -848.2461\n",
      "Epoch 8982/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7368 - total_train_reward: -1323.1308\n",
      "Epoch 8983/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0091 - total_train_reward: -1127.7955\n",
      "Epoch 8984/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9872 - total_train_reward: -1326.7728\n",
      "Epoch 8985/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9008 - total_train_reward: -1157.7670\n",
      "Epoch 8986/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2129 - total_train_reward: -1137.9614\n",
      "Epoch 8987/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2039 - total_train_reward: -1162.7389\n",
      "Epoch 8988/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6352 - total_train_reward: -1228.8880\n",
      "Epoch 8989/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3346 - total_train_reward: -1097.7636\n",
      "Epoch 8990/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2094 - total_train_reward: -1325.6162\n",
      "Epoch 8991/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8446 - total_train_reward: -1348.5891\n",
      "Epoch 8992/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0805 - total_train_reward: -952.9937\n",
      "Epoch 8993/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3972 - total_train_reward: -1283.2147\n",
      "Epoch 8994/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4076 - total_train_reward: -1320.0290\n",
      "Epoch 8995/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5885 - total_train_reward: -1332.6354\n",
      "Epoch 8996/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8262 - total_train_reward: -1562.7844\n",
      "Epoch 8997/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8389 - total_train_reward: -1183.7817\n",
      "Epoch 8998/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6608 - total_train_reward: -1273.7029\n",
      "Epoch 8999/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2633 - total_train_reward: -1339.6932\n",
      "Epoch 9000/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7970 - total_train_reward: -1325.4059\n",
      "Epoch 9001/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8954 - total_train_reward: -1342.5095\n",
      "Epoch 9002/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6657 - total_train_reward: -1377.2148\n",
      "Epoch 9003/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9075 - total_train_reward: -1786.4139\n",
      "Epoch 9004/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5237 - total_train_reward: -1340.9939\n",
      "Epoch 9005/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1800 - total_train_reward: -953.5116\n",
      "Epoch 9006/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9526 - total_train_reward: -1478.9096\n",
      "Epoch 9007/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0378 - total_train_reward: -1348.7023\n",
      "Epoch 9008/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.4072 - total_train_reward: -888.3194\n",
      "Epoch 9009/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.8888 - total_train_reward: -1207.4280\n",
      "Epoch 9010/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1986 - total_train_reward: -1481.3305\n",
      "Epoch 9011/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 17.8637 - total_train_reward: -1200.1356\n",
      "Epoch 9012/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -15.7937 - total_train_reward: -1431.3008\n",
      "Epoch 9013/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6250 - total_train_reward: -1168.5481\n",
      "Epoch 9014/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8951 - total_train_reward: -1784.6698\n",
      "Epoch 9015/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0198 - total_train_reward: -1710.9535\n",
      "Epoch 9016/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8861 - total_train_reward: -949.6432\n",
      "Epoch 9017/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6562 - total_train_reward: -998.1208\n",
      "Epoch 9018/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0356 - total_train_reward: -1464.3384\n",
      "Epoch 9019/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6368 - total_train_reward: -1022.0830\n",
      "Epoch 9020/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.1858 - total_train_reward: -1329.8505\n",
      "Epoch 9021/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9081 - total_train_reward: -1200.5946\n",
      "Epoch 9022/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7744 - total_train_reward: -1749.5893\n",
      "Epoch 9023/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2836 - total_train_reward: -984.3577\n",
      "Epoch 9024/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6884 - total_train_reward: -1050.2662\n",
      "Epoch 9025/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6094 - total_train_reward: -1697.3844\n",
      "Epoch 9026/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5372 - total_train_reward: -1429.3719\n",
      "Epoch 9027/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7616 - total_train_reward: -1332.3194\n",
      "Epoch 9028/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4435 - total_train_reward: -1316.9279\n",
      "Epoch 9029/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4186 - total_train_reward: -1250.1076\n",
      "Epoch 9030/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2501 - total_train_reward: -1060.2833\n",
      "Epoch 9031/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.3420 - total_train_reward: -1336.4637\n",
      "Epoch 9032/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0376 - total_train_reward: -1528.2200\n",
      "Epoch 9033/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1281 - total_train_reward: -1087.2036\n",
      "Epoch 9034/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4773 - total_train_reward: -1023.7093\n",
      "Epoch 9035/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5102 - total_train_reward: -1358.3587\n",
      "Epoch 9036/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2208 - total_train_reward: -1255.7699\n",
      "Epoch 9037/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.5087 - total_train_reward: -1167.5323\n",
      "Epoch 9038/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6870 - total_train_reward: -1341.1611\n",
      "Epoch 9039/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5360 - total_train_reward: -1634.2175\n",
      "Epoch 9040/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6899 - total_train_reward: -1380.9765\n",
      "Epoch 9041/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2311 - total_train_reward: -1215.1465\n",
      "Epoch 9042/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0093 - total_train_reward: -1679.8920\n",
      "Epoch 9043/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5400 - total_train_reward: -1328.9124\n",
      "Epoch 9044/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3658 - total_train_reward: -956.3530\n",
      "Epoch 9045/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6019 - total_train_reward: -1236.3986\n",
      "Epoch 9046/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3317 - total_train_reward: -1064.6099\n",
      "Epoch 9047/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8621 - total_train_reward: -1600.1185\n",
      "Epoch 9048/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9875 - total_train_reward: -1324.0417\n",
      "Epoch 9049/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.9179 - total_train_reward: -1285.2232\n",
      "Epoch 9050/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7619 - total_train_reward: -1320.6981\n",
      "Epoch 9051/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6583 - total_train_reward: -1325.4515\n",
      "Epoch 9052/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7037 - total_train_reward: -989.4718\n",
      "Epoch 9053/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3134 - total_train_reward: -1347.8259\n",
      "Epoch 9054/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9537 - total_train_reward: -1338.1477\n",
      "Epoch 9055/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7716 - total_train_reward: -1386.2957\n",
      "Epoch 9056/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7132 - total_train_reward: -1752.0225\n",
      "Epoch 9057/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6666 - total_train_reward: -1351.7061\n",
      "Epoch 9058/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7094 - total_train_reward: -1428.2392\n",
      "Epoch 9059/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2269 - total_train_reward: -1346.6044\n",
      "Epoch 9060/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4438 - total_train_reward: -1393.1261\n",
      "Epoch 9061/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0478 - total_train_reward: -1173.0606\n",
      "Epoch 9062/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1106 - total_train_reward: -753.3211\n",
      "Epoch 9063/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5000 - total_train_reward: -1553.3675\n",
      "Epoch 9064/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3510 - total_train_reward: -1357.0217\n",
      "Epoch 9065/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4625 - total_train_reward: -1682.5709\n",
      "Epoch 9066/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7765 - total_train_reward: -981.7613\n",
      "Epoch 9067/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7881 - total_train_reward: -1341.4501\n",
      "Epoch 9068/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.0594 - total_train_reward: -972.1730\n",
      "Epoch 9069/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2665 - total_train_reward: -743.0846\n",
      "Epoch 9070/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6194 - total_train_reward: -1328.2337\n",
      "Epoch 9071/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0496 - total_train_reward: -844.4676\n",
      "Epoch 9072/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.1598 - total_train_reward: -1250.4083\n",
      "Epoch 9073/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4352 - total_train_reward: -1508.2398\n",
      "Epoch 9074/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8869 - total_train_reward: -1330.8933\n",
      "Epoch 9075/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5087 - total_train_reward: -1267.8832\n",
      "Epoch 9076/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6977 - total_train_reward: -1576.4100\n",
      "Epoch 9077/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5297 - total_train_reward: -1271.1325\n",
      "Epoch 9078/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1957 - total_train_reward: -1757.4593\n",
      "Epoch 9079/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7396 - total_train_reward: -813.0498\n",
      "Epoch 9080/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1735 - total_train_reward: -1686.5337\n",
      "Epoch 9081/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.5039 - total_train_reward: -1132.0953\n",
      "Epoch 9082/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1100 - total_train_reward: -953.5626\n",
      "Epoch 9083/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9277 - total_train_reward: -1303.4810\n",
      "Epoch 9084/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0366 - total_train_reward: -1777.9151\n",
      "Epoch 9085/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4623 - total_train_reward: -1304.6695\n",
      "Epoch 9086/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.2198 - total_train_reward: -1230.0709\n",
      "Epoch 9087/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8275 - total_train_reward: -1035.9844\n",
      "Epoch 9088/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2976 - total_train_reward: -1275.8135\n",
      "Epoch 9089/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9053 - total_train_reward: -1344.6158\n",
      "Epoch 9090/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3439 - total_train_reward: -1095.8995\n",
      "Epoch 9091/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6633 - total_train_reward: -1229.0923\n",
      "Epoch 9092/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9982 - total_train_reward: -1060.4769\n",
      "Epoch 9093/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2782 - total_train_reward: -1399.5547\n",
      "Epoch 9094/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.8128 - total_train_reward: -1337.5115\n",
      "Epoch 9095/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3368 - total_train_reward: -1154.8827\n",
      "Epoch 9096/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0794 - total_train_reward: -955.9290\n",
      "Epoch 9097/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9901 - total_train_reward: -1192.8011\n",
      "Epoch 9098/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4928 - total_train_reward: -1786.2370\n",
      "Epoch 9099/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3823 - total_train_reward: -1333.9992\n",
      "Epoch 9100/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9259 - total_train_reward: -1349.7788\n",
      "Epoch 9101/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6622 - total_train_reward: -1290.5202\n",
      "Epoch 9102/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1291 - total_train_reward: -1361.0131\n",
      "Epoch 9103/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5310 - total_train_reward: -1275.0137\n",
      "Epoch 9104/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5480 - total_train_reward: -1316.5883\n",
      "Epoch 9105/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5200 - total_train_reward: -1224.0175\n",
      "Epoch 9106/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.0245 - total_train_reward: -1603.0557\n",
      "Epoch 9107/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2046 - total_train_reward: -1335.5856\n",
      "Epoch 9108/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1801 - total_train_reward: -1329.4714\n",
      "Epoch 9109/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0541 - total_train_reward: -1187.2478\n",
      "Epoch 9110/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0206 - total_train_reward: -979.3108\n",
      "Epoch 9111/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2304 - total_train_reward: -1335.1246\n",
      "Epoch 9112/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6909 - total_train_reward: -1305.6565\n",
      "Epoch 9113/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6459 - total_train_reward: -1568.0354\n",
      "Epoch 9114/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2773 - total_train_reward: -1526.0465\n",
      "Epoch 9115/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4587 - total_train_reward: -1275.2144\n",
      "Epoch 9116/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7538 - total_train_reward: -1207.5849\n",
      "Epoch 9117/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9584 - total_train_reward: -1425.0137\n",
      "Epoch 9118/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6429 - total_train_reward: -1338.8528\n",
      "Epoch 9119/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6153 - total_train_reward: -1666.7916\n",
      "Epoch 9120/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1759 - total_train_reward: -722.1432\n",
      "Epoch 9121/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0736 - total_train_reward: -1309.0669\n",
      "Epoch 9122/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -8.9595 - total_train_reward: -1297.6349\n",
      "Epoch 9123/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -8.5281 - total_train_reward: -1389.8815\n",
      "Epoch 9124/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -3.1165 - total_train_reward: -1496.3200\n",
      "Epoch 9125/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 8.2547 - total_train_reward: -1245.4895\n",
      "Epoch 9126/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 1.3882 - total_train_reward: -1324.1401\n",
      "Epoch 9127/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -0.4712 - total_train_reward: -1343.6817\n",
      "Epoch 9128/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9215 - total_train_reward: -1497.6942\n",
      "Epoch 9129/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1865 - total_train_reward: -1489.7157\n",
      "Epoch 9130/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -3.4250 - total_train_reward: -1060.7954\n",
      "Epoch 9131/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -4.8390 - total_train_reward: -1175.5276\n",
      "Epoch 9132/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2904 - total_train_reward: -1397.0014\n",
      "Epoch 9133/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 4.3478 - total_train_reward: -1110.9185\n",
      "Epoch 9134/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 1.8122 - total_train_reward: -1758.0304\n",
      "Epoch 9135/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 0.8487 - total_train_reward: -1524.0651\n",
      "Epoch 9136/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 5.9285 - total_train_reward: -956.1030\n",
      "Epoch 9137/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 1.7055 - total_train_reward: -1794.1897\n",
      "Epoch 9138/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 6.0945 - total_train_reward: -1274.3207\n",
      "Epoch 9139/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.1368 - total_train_reward: -1648.9496\n",
      "Epoch 9140/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7906 - total_train_reward: -1292.1050\n",
      "Epoch 9141/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -6.0542 - total_train_reward: -1792.5198\n",
      "Epoch 9142/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -3.9159 - total_train_reward: -1349.1676\n",
      "Epoch 9143/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3400 - total_train_reward: -1244.7417\n",
      "Epoch 9144/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -10.4045 - total_train_reward: -1754.3628\n",
      "Epoch 9145/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2519 - total_train_reward: -1437.4524\n",
      "Epoch 9146/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6414 - total_train_reward: -1797.3563\n",
      "Epoch 9147/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2820 - total_train_reward: -1412.6610\n",
      "Epoch 9148/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5379 - total_train_reward: -1287.5203\n",
      "Epoch 9149/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -12.9903 - total_train_reward: -1637.9670\n",
      "Epoch 9150/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 6.7602 - total_train_reward: -1213.1811\n",
      "Epoch 9151/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.7303 - total_train_reward: -1300.1413\n",
      "Epoch 9152/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 10.6326 - total_train_reward: -1224.5112\n",
      "Epoch 9153/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 2.1034 - total_train_reward: -1388.0681\n",
      "Epoch 9154/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 2.8553 - total_train_reward: -1274.5161\n",
      "Epoch 9155/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3277 - total_train_reward: -1636.9162\n",
      "Epoch 9156/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0526 - total_train_reward: -1290.3166\n",
      "Epoch 9157/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -2.0722 - total_train_reward: -1100.5852\n",
      "Epoch 9158/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 19.5430 - total_train_reward: -915.0671\n",
      "Epoch 9159/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 16.9029 - total_train_reward: -1250.1747\n",
      "Epoch 9160/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8043 - total_train_reward: -1783.6011\n",
      "Epoch 9161/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8580 - total_train_reward: -1521.9216\n",
      "Epoch 9162/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2616 - total_train_reward: -1802.5259\n",
      "Epoch 9163/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.7856 - total_train_reward: -1050.6334\n",
      "Epoch 9164/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5182 - total_train_reward: -1125.8661\n",
      "Epoch 9165/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 7.0765 - total_train_reward: -974.3899\n",
      "Epoch 9166/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8573 - total_train_reward: -1155.9681\n",
      "Epoch 9167/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.9293 - total_train_reward: -952.9594\n",
      "Epoch 9168/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6906 - total_train_reward: -1120.9520\n",
      "Epoch 9169/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3909 - total_train_reward: -1339.6247\n",
      "Epoch 9170/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6919 - total_train_reward: -1512.1982\n",
      "Epoch 9171/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 1.6682 - total_train_reward: -1462.6063\n",
      "Epoch 9172/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.7901 - total_train_reward: -1212.0012\n",
      "Epoch 9173/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8233 - total_train_reward: -1207.4168\n",
      "Epoch 9174/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5032 - total_train_reward: -1304.7773\n",
      "Epoch 9175/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -3.6606 - total_train_reward: -1165.2181\n",
      "Epoch 9176/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1665 - total_train_reward: -1091.6254\n",
      "Epoch 9177/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 2.2537 - total_train_reward: -1810.6292\n",
      "Epoch 9178/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -12.6088 - total_train_reward: -1706.5051\n",
      "Epoch 9179/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0248 - total_train_reward: -1452.8616\n",
      "Epoch 9180/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5184 - total_train_reward: -1166.1732\n",
      "Epoch 9181/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4109 - total_train_reward: -1259.4634\n",
      "Epoch 9182/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7245 - total_train_reward: -1245.7345\n",
      "Epoch 9183/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5284 - total_train_reward: -1268.0775\n",
      "Epoch 9184/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5584 - total_train_reward: -1248.3703\n",
      "Epoch 9185/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5977 - total_train_reward: -1221.0565\n",
      "Epoch 9186/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0835 - total_train_reward: -1272.2307\n",
      "Epoch 9187/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6774 - total_train_reward: -1057.3277\n",
      "Epoch 9188/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.4145 - total_train_reward: -1221.4257\n",
      "Epoch 9189/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6019 - total_train_reward: -1324.2528\n",
      "Epoch 9190/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4875 - total_train_reward: -1381.7488\n",
      "Epoch 9191/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0319 - total_train_reward: -1300.6306\n",
      "Epoch 9192/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.9124 - total_train_reward: -1065.0544\n",
      "Epoch 9193/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -10.0517 - total_train_reward: -1359.6768\n",
      "Epoch 9194/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7770 - total_train_reward: -1238.2490\n",
      "Epoch 9195/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0086 - total_train_reward: -1646.5481\n",
      "Epoch 9196/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8882 - total_train_reward: -1267.6536\n",
      "Epoch 9197/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0706 - total_train_reward: -1240.2550\n",
      "Epoch 9198/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5191 - total_train_reward: -1360.9845\n",
      "Epoch 9199/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -3.6542 - total_train_reward: -1246.3991\n",
      "Epoch 9200/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0907 - total_train_reward: -1219.7412\n",
      "Epoch 9201/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8938 - total_train_reward: -972.5768\n",
      "Epoch 9202/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -5.4439 - total_train_reward: -1330.5971\n",
      "Epoch 9203/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3429 - total_train_reward: -1645.6021\n",
      "Epoch 9204/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8824 - total_train_reward: -1090.9736\n",
      "Epoch 9205/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 2.6733 - total_train_reward: -1803.3754\n",
      "Epoch 9206/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -9.0929 - total_train_reward: -1133.3333\n",
      "Epoch 9207/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2902 - total_train_reward: -1168.4842\n",
      "Epoch 9208/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4016 - total_train_reward: -1284.1104\n",
      "Epoch 9209/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3477 - total_train_reward: -1211.7249\n",
      "Epoch 9210/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 5.4544 - total_train_reward: -1291.0805\n",
      "Epoch 9211/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8159 - total_train_reward: -1232.6471\n",
      "Epoch 9212/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4062 - total_train_reward: -1490.9685\n",
      "Epoch 9213/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5792 - total_train_reward: -956.1261\n",
      "Epoch 9214/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 2.8662 - total_train_reward: -1279.4528\n",
      "Epoch 9215/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8177 - total_train_reward: -1051.7536\n",
      "Epoch 9216/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2375 - total_train_reward: -847.7469\n",
      "Epoch 9217/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.4249 - total_train_reward: -1254.3815\n",
      "Epoch 9218/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.5961 - total_train_reward: -1023.5945\n",
      "Epoch 9219/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -10.9929 - total_train_reward: -1775.2027\n",
      "Epoch 9220/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.0611 - total_train_reward: -1239.9613\n",
      "Epoch 9221/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0031 - total_train_reward: -1161.0990\n",
      "Epoch 9222/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.2710 - total_train_reward: -1255.7132\n",
      "Epoch 9223/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1276 - total_train_reward: -1146.4620\n",
      "Epoch 9224/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3697 - total_train_reward: -1536.5565\n",
      "Epoch 9225/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3870 - total_train_reward: -1184.6943\n",
      "Epoch 9226/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4226 - total_train_reward: -1768.2380\n",
      "Epoch 9227/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 1.7230 - total_train_reward: -1587.7594\n",
      "Epoch 9228/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7917 - total_train_reward: -1758.6044\n",
      "Epoch 9229/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -8.3638 - total_train_reward: -1333.2555\n",
      "Epoch 9230/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 0.4210 - total_train_reward: -1090.5863\n",
      "Epoch 9231/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6366 - total_train_reward: -1227.7043\n",
      "Epoch 9232/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1098 - total_train_reward: -1296.9501\n",
      "Epoch 9233/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7150 - total_train_reward: -1745.6210\n",
      "Epoch 9234/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3798 - total_train_reward: -1585.1399\n",
      "Epoch 9235/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6041 - total_train_reward: -1253.4831\n",
      "Epoch 9236/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8115 - total_train_reward: -1255.2515\n",
      "Epoch 9237/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.5255 - total_train_reward: -1798.7347\n",
      "Epoch 9238/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 0.7701 - total_train_reward: -1718.4643\n",
      "Epoch 9239/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0515 - total_train_reward: -1569.8258\n",
      "Epoch 9240/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9785 - total_train_reward: -1290.6675\n",
      "Epoch 9241/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4758 - total_train_reward: -1635.4453\n",
      "Epoch 9242/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0196 - total_train_reward: -1064.5174\n",
      "Epoch 9243/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4929 - total_train_reward: -1739.4607\n",
      "Epoch 9244/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7174 - total_train_reward: -1659.0258\n",
      "Epoch 9245/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6456 - total_train_reward: -957.3517\n",
      "Epoch 9246/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6612 - total_train_reward: -1343.4786\n",
      "Epoch 9247/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5556 - total_train_reward: -956.5550\n",
      "Epoch 9248/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0425 - total_train_reward: -1346.1788\n",
      "Epoch 9249/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9223 - total_train_reward: -1443.2237\n",
      "Epoch 9250/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 5.7631 - total_train_reward: -954.9978\n",
      "Epoch 9251/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5967 - total_train_reward: -957.1638\n",
      "Epoch 9252/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2744 - total_train_reward: -1810.7772\n",
      "Epoch 9253/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3431 - total_train_reward: -1173.2085\n",
      "Epoch 9254/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3603 - total_train_reward: -1349.5937\n",
      "Epoch 9255/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7246 - total_train_reward: -1059.6813\n",
      "Epoch 9256/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9752 - total_train_reward: -1785.4600\n",
      "Epoch 9257/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.9872 - total_train_reward: -1813.1938\n",
      "Epoch 9258/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3448 - total_train_reward: -1056.5216\n",
      "Epoch 9259/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2302 - total_train_reward: -1221.0418\n",
      "Epoch 9260/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1170 - total_train_reward: -1217.6904\n",
      "Epoch 9261/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8624 - total_train_reward: -1443.2901\n",
      "Epoch 9262/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1994 - total_train_reward: -1586.4060\n",
      "Epoch 9263/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6847 - total_train_reward: -1779.9725\n",
      "Epoch 9264/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.6740 - total_train_reward: -1172.3140\n",
      "Epoch 9265/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5130 - total_train_reward: -1260.8453\n",
      "Epoch 9266/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2330 - total_train_reward: -1393.1739\n",
      "Epoch 9267/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2960 - total_train_reward: -1541.9585\n",
      "Epoch 9268/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5335 - total_train_reward: -1464.0038\n",
      "Epoch 9269/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8672 - total_train_reward: -1081.5164\n",
      "Epoch 9270/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9726 - total_train_reward: -1203.9193\n",
      "Epoch 9271/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3054 - total_train_reward: -963.9487\n",
      "Epoch 9272/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6382 - total_train_reward: -1829.5617\n",
      "Epoch 9273/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9159 - total_train_reward: -1241.5588\n",
      "Epoch 9274/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0319 - total_train_reward: -1481.6262\n",
      "Epoch 9275/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5083 - total_train_reward: -1735.7999\n",
      "Epoch 9276/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.1692 - total_train_reward: -1533.8236\n",
      "Epoch 9277/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2637 - total_train_reward: -1337.5242\n",
      "Epoch 9278/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5210 - total_train_reward: -1173.6311\n",
      "Epoch 9279/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 4.4352 - total_train_reward: -1100.0856\n",
      "Epoch 9280/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4256 - total_train_reward: -1787.2326\n",
      "Epoch 9281/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7531 - total_train_reward: -1467.8634\n",
      "Epoch 9282/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7119 - total_train_reward: -978.1068\n",
      "Epoch 9283/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.7847 - total_train_reward: -1828.5119\n",
      "Epoch 9284/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1606 - total_train_reward: -844.8183\n",
      "Epoch 9285/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2542 - total_train_reward: -1269.8509\n",
      "Epoch 9286/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8062 - total_train_reward: -1164.8131\n",
      "Epoch 9287/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.7450 - total_train_reward: -1105.6376\n",
      "Epoch 9288/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0904 - total_train_reward: -1141.9312\n",
      "Epoch 9289/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0515 - total_train_reward: -1166.5093\n",
      "Epoch 9290/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0248 - total_train_reward: -1173.2308\n",
      "Epoch 9291/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2986 - total_train_reward: -1235.7981\n",
      "Epoch 9292/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8143 - total_train_reward: -957.9303\n",
      "Epoch 9293/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6711 - total_train_reward: -793.8227\n",
      "Epoch 9294/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7620 - total_train_reward: -1029.9070\n",
      "Epoch 9295/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4817 - total_train_reward: -1563.8161\n",
      "Epoch 9296/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2848 - total_train_reward: -773.6781\n",
      "Epoch 9297/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.9799 - total_train_reward: -1221.9006\n",
      "Epoch 9298/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0999 - total_train_reward: -1735.3809\n",
      "Epoch 9299/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.6650 - total_train_reward: -1754.3755\n",
      "Epoch 9300/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9949 - total_train_reward: -1204.0315\n",
      "Epoch 9301/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7235 - total_train_reward: -1455.6656\n",
      "Epoch 9302/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3805 - total_train_reward: -1357.5544\n",
      "Epoch 9303/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1090 - total_train_reward: -1205.0260\n",
      "Epoch 9304/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9172 - total_train_reward: -1687.1594\n",
      "Epoch 9305/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.4725 - total_train_reward: -1222.8729\n",
      "Epoch 9306/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5081 - total_train_reward: -1061.0467\n",
      "Epoch 9307/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2122 - total_train_reward: -1238.6336\n",
      "Epoch 9308/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.5857 - total_train_reward: -1213.9888\n",
      "Epoch 9309/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3190 - total_train_reward: -1111.1610\n",
      "Epoch 9310/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6370 - total_train_reward: -1289.1595\n",
      "Epoch 9311/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3748 - total_train_reward: -1479.7655\n",
      "Epoch 9312/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4361 - total_train_reward: -1571.6029\n",
      "Epoch 9313/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.5243 - total_train_reward: -1058.5396\n",
      "Epoch 9314/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4678 - total_train_reward: -1550.6628\n",
      "Epoch 9315/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7778 - total_train_reward: -944.9895\n",
      "Epoch 9316/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.8921 - total_train_reward: -1217.0033\n",
      "Epoch 9317/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3206 - total_train_reward: -1411.7762\n",
      "Epoch 9318/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4313 - total_train_reward: -1508.5924\n",
      "Epoch 9319/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3589 - total_train_reward: -726.0064\n",
      "Epoch 9320/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7928 - total_train_reward: -1789.9975\n",
      "Epoch 9321/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4012 - total_train_reward: -958.1548\n",
      "Epoch 9322/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.7487 - total_train_reward: -1588.1536\n",
      "Epoch 9323/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1420 - total_train_reward: -1419.2369\n",
      "Epoch 9324/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3240 - total_train_reward: -1243.9148\n",
      "Epoch 9325/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -7.4719 - total_train_reward: -1252.1922\n",
      "Epoch 9326/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8536 - total_train_reward: -840.4533\n",
      "Epoch 9327/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9565 - total_train_reward: -1697.0233\n",
      "Epoch 9328/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.7486 - total_train_reward: -1497.1872\n",
      "Epoch 9329/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2451 - total_train_reward: -1220.1907\n",
      "Epoch 9330/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7028 - total_train_reward: -1537.5100\n",
      "Epoch 9331/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6904 - total_train_reward: -1216.1462\n",
      "Epoch 9332/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.9679 - total_train_reward: -1005.1294\n",
      "Epoch 9333/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5996 - total_train_reward: -1497.2758\n",
      "Epoch 9334/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8439 - total_train_reward: -1191.2709\n",
      "Epoch 9335/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1642 - total_train_reward: -1802.6752\n",
      "Epoch 9336/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.1406 - total_train_reward: -1315.2560\n",
      "Epoch 9337/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8768 - total_train_reward: -1088.9842\n",
      "Epoch 9338/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -5.1317 - total_train_reward: -1522.0952\n",
      "Epoch 9339/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1588 - total_train_reward: -1607.7306\n",
      "Epoch 9340/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0810 - total_train_reward: -1284.9199\n",
      "Epoch 9341/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0544 - total_train_reward: -1601.8496\n",
      "Epoch 9342/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: 6.3846 - total_train_reward: -1209.0109\n",
      "Epoch 9343/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3456 - total_train_reward: -1212.1122\n",
      "Epoch 9344/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1359 - total_train_reward: -1215.6311\n",
      "Epoch 9345/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0498 - total_train_reward: -1197.0703\n",
      "Epoch 9346/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7762 - total_train_reward: -1259.7608\n",
      "Epoch 9347/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.2432 - total_train_reward: -1207.7142\n",
      "Epoch 9348/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4740 - total_train_reward: -1210.3298\n",
      "Epoch 9349/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6023 - total_train_reward: -978.4310\n",
      "Epoch 9350/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3995 - total_train_reward: -1561.2540\n",
      "Epoch 9351/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9800 - total_train_reward: -1190.7112\n",
      "Epoch 9352/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.4408 - total_train_reward: -1433.6123\n",
      "Epoch 9353/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5864 - total_train_reward: -1164.3355\n",
      "Epoch 9354/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3108 - total_train_reward: -1034.8749\n",
      "Epoch 9355/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9706 - total_train_reward: -1455.7799\n",
      "Epoch 9356/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8994 - total_train_reward: -1681.9864\n",
      "Epoch 9357/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1395 - total_train_reward: -1536.0959\n",
      "Epoch 9358/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4222 - total_train_reward: -1303.5731\n",
      "Epoch 9359/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3924 - total_train_reward: -1698.1759\n",
      "Epoch 9360/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.6533 - total_train_reward: -1218.2826\n",
      "Epoch 9361/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8083 - total_train_reward: -1413.8761\n",
      "Epoch 9362/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -11.8938 - total_train_reward: -1722.9695\n",
      "Epoch 9363/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3477 - total_train_reward: -1205.8245\n",
      "Epoch 9364/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9483 - total_train_reward: -1207.3393\n",
      "Epoch 9365/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.0823 - total_train_reward: -1063.8682\n",
      "Epoch 9366/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1698 - total_train_reward: -1189.1001\n",
      "Epoch 9367/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8000 - total_train_reward: -843.5878\n",
      "Epoch 9368/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.2423 - total_train_reward: -1452.8982\n",
      "Epoch 9369/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6568 - total_train_reward: -1218.5893\n",
      "Epoch 9370/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.1018 - total_train_reward: -1063.0276\n",
      "Epoch 9371/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7664 - total_train_reward: -1417.8943\n",
      "Epoch 9372/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1602 - total_train_reward: -1078.5618\n",
      "Epoch 9373/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0582 - total_train_reward: -966.0042\n",
      "Epoch 9374/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -13.1265 - total_train_reward: -1394.6161\n",
      "Epoch 9375/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6770 - total_train_reward: -1160.9379\n",
      "Epoch 9376/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4361 - total_train_reward: -1556.1050\n",
      "Epoch 9377/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9642 - total_train_reward: -1258.0825\n",
      "Epoch 9378/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6533 - total_train_reward: -957.0671\n",
      "Epoch 9379/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1583 - total_train_reward: -1399.6079\n",
      "Epoch 9380/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2381 - total_train_reward: -1226.4473\n",
      "Epoch 9381/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -6.6333 - total_train_reward: -1479.7913\n",
      "Epoch 9382/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3817 - total_train_reward: -1110.3082\n",
      "Epoch 9383/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4237 - total_train_reward: -1170.4119\n",
      "Epoch 9384/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6665 - total_train_reward: -1207.8379\n",
      "Epoch 9385/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.1157 - total_train_reward: -1056.6772\n",
      "Epoch 9386/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9098 - total_train_reward: -1827.9832\n",
      "Epoch 9387/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0904 - total_train_reward: -1199.1171\n",
      "Epoch 9388/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1361 - total_train_reward: -1064.4397\n",
      "Epoch 9389/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0964 - total_train_reward: -1063.0860\n",
      "Epoch 9390/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.7554 - total_train_reward: -1830.2705\n",
      "Epoch 9391/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7051 - total_train_reward: -1827.9647\n",
      "Epoch 9392/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.2335 - total_train_reward: -1523.3242\n",
      "Epoch 9393/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.2226 - total_train_reward: -1208.7775\n",
      "Epoch 9394/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.4607 - total_train_reward: -1220.3562\n",
      "Epoch 9395/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8649 - total_train_reward: -1625.5697\n",
      "Epoch 9396/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3252 - total_train_reward: -1216.6141\n",
      "Epoch 9397/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4928 - total_train_reward: -845.5861\n",
      "Epoch 9398/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4995 - total_train_reward: -1367.2755\n",
      "Epoch 9399/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3787 - total_train_reward: -1649.0488\n",
      "Epoch 9400/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9497 - total_train_reward: -1477.9834\n",
      "Epoch 9401/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0261 - total_train_reward: -1181.0648\n",
      "Epoch 9402/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7541 - total_train_reward: -1187.7443\n",
      "Epoch 9403/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3209 - total_train_reward: -1215.3288\n",
      "Epoch 9404/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.4431 - total_train_reward: -1217.8242\n",
      "Epoch 9405/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.1363 - total_train_reward: -1698.1502\n",
      "Epoch 9406/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2806 - total_train_reward: -931.0624\n",
      "Epoch 9407/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4524 - total_train_reward: -1563.1808\n",
      "Epoch 9408/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0404 - total_train_reward: -1623.2122\n",
      "Epoch 9409/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3883 - total_train_reward: -1090.2118\n",
      "Epoch 9410/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9634 - total_train_reward: -1603.9362\n",
      "Epoch 9411/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.0624 - total_train_reward: -1233.1590\n",
      "Epoch 9412/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8461 - total_train_reward: -1273.0758\n",
      "Epoch 9413/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.0143 - total_train_reward: -1224.5894\n",
      "Epoch 9414/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1367 - total_train_reward: -1165.6677\n",
      "Epoch 9415/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2893 - total_train_reward: -1061.7289\n",
      "Epoch 9416/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.3286 - total_train_reward: -1526.1460\n",
      "Epoch 9417/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0199 - total_train_reward: -1114.8776\n",
      "Epoch 9418/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8250 - total_train_reward: -1211.9275\n",
      "Epoch 9419/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8064 - total_train_reward: -1218.0558\n",
      "Epoch 9420/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.7762 - total_train_reward: -728.3384\n",
      "Epoch 9421/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7973 - total_train_reward: -1572.4032\n",
      "Epoch 9422/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.7325 - total_train_reward: -1745.1014\n",
      "Epoch 9423/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7736 - total_train_reward: -1834.7903\n",
      "Epoch 9424/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2053 - total_train_reward: -1529.9598\n",
      "Epoch 9425/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1672 - total_train_reward: -1300.4590\n",
      "Epoch 9426/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2198 - total_train_reward: -1170.3733\n",
      "Epoch 9427/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4116 - total_train_reward: -1838.4827\n",
      "Epoch 9428/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.3071 - total_train_reward: -1154.8401\n",
      "Epoch 9429/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9083 - total_train_reward: -727.0540\n",
      "Epoch 9430/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9904 - total_train_reward: -1757.8755\n",
      "Epoch 9431/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5969 - total_train_reward: -1294.3500\n",
      "Epoch 9432/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 15.9129 - total_train_reward: -1209.2541\n",
      "Epoch 9433/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7607 - total_train_reward: -1215.0719\n",
      "Epoch 9434/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9699 - total_train_reward: -1591.0262\n",
      "Epoch 9435/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.6669 - total_train_reward: -1298.1303\n",
      "Epoch 9436/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2244 - total_train_reward: -1484.3701\n",
      "Epoch 9437/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2550 - total_train_reward: -959.1267\n",
      "Epoch 9438/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.6666 - total_train_reward: -1089.0834\n",
      "Epoch 9439/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0455 - total_train_reward: -1067.2153\n",
      "Epoch 9440/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4293 - total_train_reward: -958.6241\n",
      "Epoch 9441/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.8519 - total_train_reward: -1027.0323\n",
      "Epoch 9442/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.9638 - total_train_reward: -1204.4478\n",
      "Epoch 9443/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6718 - total_train_reward: -1162.5954\n",
      "Epoch 9444/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.6166 - total_train_reward: -1833.2360\n",
      "Epoch 9445/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0516 - total_train_reward: -1592.1581\n",
      "Epoch 9446/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.6253 - total_train_reward: -1525.8343\n",
      "Epoch 9447/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.7483 - total_train_reward: -1783.2776\n",
      "Epoch 9448/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5962 - total_train_reward: -1217.7875\n",
      "Epoch 9449/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3483 - total_train_reward: -1469.8896\n",
      "Epoch 9450/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2218 - total_train_reward: -1039.2041\n",
      "Epoch 9451/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2101 - total_train_reward: -1172.9935\n",
      "Epoch 9452/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0152 - total_train_reward: -1518.8312\n",
      "Epoch 9453/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6484 - total_train_reward: -1723.3035\n",
      "Epoch 9454/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9232 - total_train_reward: -1170.3237\n",
      "Epoch 9455/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0018 - total_train_reward: -959.9699\n",
      "Epoch 9456/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.1007 - total_train_reward: -1151.3616\n",
      "Epoch 9457/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.3240 - total_train_reward: -1184.6516\n",
      "Epoch 9458/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7107 - total_train_reward: -1070.2168\n",
      "Epoch 9459/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5471 - total_train_reward: -1459.0447\n",
      "Epoch 9460/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7232 - total_train_reward: -1044.6140\n",
      "Epoch 9461/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8618 - total_train_reward: -1206.0514\n",
      "Epoch 9462/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.4738 - total_train_reward: -1065.1430\n",
      "Epoch 9463/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3246 - total_train_reward: -1296.4428\n",
      "Epoch 9464/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2668 - total_train_reward: -1827.8301\n",
      "Epoch 9465/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8844 - total_train_reward: -966.2410\n",
      "Epoch 9466/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.0953 - total_train_reward: -1192.0767\n",
      "Epoch 9467/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3598 - total_train_reward: -1059.4616\n",
      "Epoch 9468/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9003 - total_train_reward: -726.4430\n",
      "Epoch 9469/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.2090 - total_train_reward: -1212.2632\n",
      "Epoch 9470/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8053 - total_train_reward: -1211.8523\n",
      "Epoch 9471/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2288 - total_train_reward: -1520.8334\n",
      "Epoch 9472/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8592 - total_train_reward: -1714.3074\n",
      "Epoch 9473/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4086 - total_train_reward: -1218.7580\n",
      "Epoch 9474/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6683 - total_train_reward: -1213.5268\n",
      "Epoch 9475/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8857 - total_train_reward: -1361.1121\n",
      "Epoch 9476/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.7332 - total_train_reward: -1695.3870\n",
      "Epoch 9477/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7966 - total_train_reward: -1797.3049\n",
      "Epoch 9478/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2688 - total_train_reward: -1018.9984\n",
      "Epoch 9479/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7649 - total_train_reward: -1834.2103\n",
      "Epoch 9480/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.6576 - total_train_reward: -1222.6847\n",
      "Epoch 9481/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.4413 - total_train_reward: -1365.0822\n",
      "Epoch 9482/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9510 - total_train_reward: -1223.8627\n",
      "Epoch 9483/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6145 - total_train_reward: -1177.1000\n",
      "Epoch 9484/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9122 - total_train_reward: -1054.5923\n",
      "Epoch 9485/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.6444 - total_train_reward: -1707.5672\n",
      "Epoch 9486/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8539 - total_train_reward: -1669.9972\n",
      "Epoch 9487/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2179 - total_train_reward: -1449.7966\n",
      "Epoch 9488/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.1962 - total_train_reward: -1218.3843\n",
      "Epoch 9489/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.4517 - total_train_reward: -1080.6695\n",
      "Epoch 9490/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8419 - total_train_reward: -1280.9033\n",
      "Epoch 9491/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6917 - total_train_reward: -1319.4949\n",
      "Epoch 9492/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1462 - total_train_reward: -1470.4509\n",
      "Epoch 9493/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8673 - total_train_reward: -830.3368\n",
      "Epoch 9494/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1685 - total_train_reward: -844.7901\n",
      "Epoch 9495/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9353 - total_train_reward: -1560.7523\n",
      "Epoch 9496/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.2874 - total_train_reward: -870.0278\n",
      "Epoch 9497/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.4866 - total_train_reward: -1224.5237\n",
      "Epoch 9498/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3308 - total_train_reward: -1189.0560\n",
      "Epoch 9499/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6813 - total_train_reward: -728.8926\n",
      "Epoch 9500/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.3735 - total_train_reward: -1064.0758\n",
      "Epoch 9501/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.8052 - total_train_reward: -1404.2745\n",
      "Epoch 9502/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 14.4050 - total_train_reward: -1212.7077\n",
      "Epoch 9503/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9395 - total_train_reward: -1531.6313\n",
      "Epoch 9504/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5094 - total_train_reward: -1301.8394\n",
      "Epoch 9505/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1556 - total_train_reward: -1794.5847\n",
      "Epoch 9506/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6015 - total_train_reward: -1188.2955\n",
      "Epoch 9507/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5113 - total_train_reward: -1364.5435\n",
      "Epoch 9508/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.3209 - total_train_reward: -1180.8975\n",
      "Epoch 9509/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.5945 - total_train_reward: -1327.2932\n",
      "Epoch 9510/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7225 - total_train_reward: -1187.5027\n",
      "Epoch 9511/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.3144 - total_train_reward: -958.1197\n",
      "Epoch 9512/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8407 - total_train_reward: -1304.8002\n",
      "Epoch 9513/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1585 - total_train_reward: -1394.5965\n",
      "Epoch 9514/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6072 - total_train_reward: -1237.3742\n",
      "Epoch 9515/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7704 - total_train_reward: -1274.4968\n",
      "Epoch 9516/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9419 - total_train_reward: -958.5323\n",
      "Epoch 9517/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7743 - total_train_reward: -1738.7241\n",
      "Epoch 9518/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9504 - total_train_reward: -1217.8699\n",
      "Epoch 9519/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5926 - total_train_reward: -976.6620\n",
      "Epoch 9520/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7406 - total_train_reward: -1125.2083\n",
      "Epoch 9521/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4513 - total_train_reward: -1068.5818\n",
      "Epoch 9522/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7446 - total_train_reward: -1170.6123\n",
      "Epoch 9523/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.7950 - total_train_reward: -1200.7829\n",
      "Epoch 9524/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4601 - total_train_reward: -1590.5409\n",
      "Epoch 9525/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3749 - total_train_reward: -1224.9015\n",
      "Epoch 9526/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3192 - total_train_reward: -1208.9632\n",
      "Epoch 9527/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8776 - total_train_reward: -1223.5004\n",
      "Epoch 9528/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6374 - total_train_reward: -698.8606\n",
      "Epoch 9529/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 41.4044 - total_train_reward: -1214.4837\n",
      "Epoch 9530/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9995 - total_train_reward: -1220.2593\n",
      "Epoch 9531/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4064 - total_train_reward: -1199.6784\n",
      "Epoch 9532/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5145 - total_train_reward: -1834.5778\n",
      "Epoch 9533/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9184 - total_train_reward: -1769.5156\n",
      "Epoch 9534/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7134 - total_train_reward: -1209.8782\n",
      "Epoch 9535/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9342 - total_train_reward: -1426.0006\n",
      "Epoch 9536/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.9913 - total_train_reward: -1815.7605\n",
      "Epoch 9537/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0440 - total_train_reward: -1022.3618\n",
      "Epoch 9538/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3912 - total_train_reward: -1486.7525\n",
      "Epoch 9539/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.6690 - total_train_reward: -1366.2878\n",
      "Epoch 9540/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2847 - total_train_reward: -1164.8653\n",
      "Epoch 9541/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8271 - total_train_reward: -1215.7831\n",
      "Epoch 9542/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2755 - total_train_reward: -1133.7149\n",
      "Epoch 9543/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.7089 - total_train_reward: -1688.2165\n",
      "Epoch 9544/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3576 - total_train_reward: -1274.2051\n",
      "Epoch 9545/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2469 - total_train_reward: -1230.0535\n",
      "Epoch 9546/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0466 - total_train_reward: -1288.0866\n",
      "Epoch 9547/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.4069 - total_train_reward: -1212.9344\n",
      "Epoch 9548/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.3216 - total_train_reward: -1622.1351\n",
      "Epoch 9549/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5239 - total_train_reward: -1164.8881\n",
      "Epoch 9550/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9378 - total_train_reward: -1407.0500\n",
      "Epoch 9551/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5700 - total_train_reward: -859.3682\n",
      "Epoch 9552/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3622 - total_train_reward: -1316.0528\n",
      "Epoch 9553/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4377 - total_train_reward: -1676.1945\n",
      "Epoch 9554/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3930 - total_train_reward: -1489.9902\n",
      "Epoch 9555/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.7139 - total_train_reward: -1656.4226\n",
      "Epoch 9556/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9411 - total_train_reward: -1310.5320\n",
      "Epoch 9557/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2350 - total_train_reward: -1423.9717\n",
      "Epoch 9558/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7960 - total_train_reward: -1498.9331\n",
      "Epoch 9559/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.9070 - total_train_reward: -1199.5290\n",
      "Epoch 9560/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8478 - total_train_reward: -1755.6682\n",
      "Epoch 9561/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.9867 - total_train_reward: -1009.0717\n",
      "Epoch 9562/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.6476 - total_train_reward: -1201.0672\n",
      "Epoch 9563/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5511 - total_train_reward: -1337.6423\n",
      "Epoch 9564/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8525 - total_train_reward: -1294.8832\n",
      "Epoch 9565/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.5196 - total_train_reward: -1211.2130\n",
      "Epoch 9566/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7385 - total_train_reward: -962.7150\n",
      "Epoch 9567/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.3473 - total_train_reward: -1825.1692\n",
      "Epoch 9568/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4656 - total_train_reward: -1517.4915\n",
      "Epoch 9569/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9127 - total_train_reward: -1502.3309\n",
      "Epoch 9570/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8817 - total_train_reward: -843.0430\n",
      "Epoch 9571/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 11.9242 - total_train_reward: -1196.2527\n",
      "Epoch 9572/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1542 - total_train_reward: -1061.5473\n",
      "Epoch 9573/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4866 - total_train_reward: -1708.0625\n",
      "Epoch 9574/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5772 - total_train_reward: -1276.3769\n",
      "Epoch 9575/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.2926 - total_train_reward: -1493.0247\n",
      "Epoch 9576/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6656 - total_train_reward: -1170.9739\n",
      "Epoch 9577/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8586 - total_train_reward: -1088.0069\n",
      "Epoch 9578/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.1497 - total_train_reward: -1299.3028\n",
      "Epoch 9579/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7055 - total_train_reward: -1067.1302\n",
      "Epoch 9580/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9314 - total_train_reward: -1306.8967\n",
      "Epoch 9581/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5316 - total_train_reward: -1647.7244\n",
      "Epoch 9582/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.3067 - total_train_reward: -1214.0830\n",
      "Epoch 9583/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3218 - total_train_reward: -1269.2992\n",
      "Epoch 9584/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.3320 - total_train_reward: -1182.1479\n",
      "Epoch 9585/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2003 - total_train_reward: -1714.7790\n",
      "Epoch 9586/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7846 - total_train_reward: -1149.0609\n",
      "Epoch 9587/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5302 - total_train_reward: -957.7312\n",
      "Epoch 9588/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4680 - total_train_reward: -1173.3658\n",
      "Epoch 9589/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.6917 - total_train_reward: -1329.9087\n",
      "Epoch 9590/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2975 - total_train_reward: -1796.0252\n",
      "Epoch 9591/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.3049 - total_train_reward: -1284.8034\n",
      "Epoch 9592/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.8455 - total_train_reward: -1681.9472\n",
      "Epoch 9593/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.0105 - total_train_reward: -1214.0052\n",
      "Epoch 9594/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1829 - total_train_reward: -1178.7264\n",
      "Epoch 9595/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.7305 - total_train_reward: -1220.5113\n",
      "Epoch 9596/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0304 - total_train_reward: -1273.4626\n",
      "Epoch 9597/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 8.4186 - total_train_reward: -1034.6749\n",
      "Epoch 9598/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.8766 - total_train_reward: -1115.2091\n",
      "Epoch 9599/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 12.2621 - total_train_reward: -919.9235\n",
      "Epoch 9600/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.8279 - total_train_reward: -827.0421\n",
      "Epoch 9601/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.9090 - total_train_reward: -1735.7174\n",
      "Epoch 9602/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0938 - total_train_reward: -1061.9826\n",
      "Epoch 9603/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.0595 - total_train_reward: -1176.1553\n",
      "Epoch 9604/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4523 - total_train_reward: -1684.4060\n",
      "Epoch 9605/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.8029 - total_train_reward: -1657.7891\n",
      "Epoch 9606/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4487 - total_train_reward: -1292.2382\n",
      "Epoch 9607/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0126 - total_train_reward: -1515.7887\n",
      "Epoch 9608/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4331 - total_train_reward: -1467.0089\n",
      "Epoch 9609/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5729 - total_train_reward: -1242.8560\n",
      "Epoch 9610/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 9.3791 - total_train_reward: -1217.5572\n",
      "Epoch 9611/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6730 - total_train_reward: -1301.3277\n",
      "Epoch 9612/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6278 - total_train_reward: -727.9000\n",
      "Epoch 9613/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8282 - total_train_reward: -1470.1986\n",
      "Epoch 9614/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.1212 - total_train_reward: -1421.0872\n",
      "Epoch 9615/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.4138 - total_train_reward: -1067.8750\n",
      "Epoch 9616/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.1393 - total_train_reward: -1208.0504\n",
      "Epoch 9617/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.8688 - total_train_reward: -1246.3199\n",
      "Epoch 9618/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.6071 - total_train_reward: -1505.8687\n",
      "Epoch 9619/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0701 - total_train_reward: -1496.4715\n",
      "Epoch 9620/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -14.6681 - total_train_reward: -1463.2598\n",
      "Epoch 9621/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4891 - total_train_reward: -1170.2393\n",
      "Epoch 9622/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5348 - total_train_reward: -1302.2702\n",
      "Epoch 9623/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.0538 - total_train_reward: -1309.3563\n",
      "Epoch 9624/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.7456 - total_train_reward: -850.7204\n",
      "Epoch 9625/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1770 - total_train_reward: -962.5911\n",
      "Epoch 9626/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9286 - total_train_reward: -1544.5239\n",
      "Epoch 9627/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.8982 - total_train_reward: -1224.3908\n",
      "Epoch 9628/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.2321 - total_train_reward: -1210.8409\n",
      "Epoch 9629/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0778 - total_train_reward: -1468.6821\n",
      "Epoch 9630/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1422 - total_train_reward: -1124.6010\n",
      "Epoch 9631/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4425 - total_train_reward: -1253.8776\n",
      "Epoch 9632/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5435 - total_train_reward: -1491.6070\n",
      "Epoch 9633/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4400 - total_train_reward: -1273.5127\n",
      "Epoch 9634/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.4050 - total_train_reward: -1696.6460\n",
      "Epoch 9635/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.9115 - total_train_reward: -1532.2918\n",
      "Epoch 9636/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2987 - total_train_reward: -1584.6920\n",
      "Epoch 9637/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.9308 - total_train_reward: -1242.5841\n",
      "Epoch 9638/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0147 - total_train_reward: -1072.4401\n",
      "Epoch 9639/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0292 - total_train_reward: -1400.5709\n",
      "Epoch 9640/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9936 - total_train_reward: -1038.0039\n",
      "Epoch 9641/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.7313 - total_train_reward: -1300.5844\n",
      "Epoch 9642/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6981 - total_train_reward: -1225.7509\n",
      "Epoch 9643/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.5230 - total_train_reward: -1136.4060\n",
      "Epoch 9644/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4069 - total_train_reward: -1399.3046\n",
      "Epoch 9645/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.2197 - total_train_reward: -1250.3920\n",
      "Epoch 9646/10000\n",
      "1/1 [==============================] - 4s 4s/step - actor_loss: -6.6548 - total_train_reward: -959.8996\n",
      "Epoch 9647/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -9.3686 - total_train_reward: -1470.4984\n",
      "Epoch 9648/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.8110 - total_train_reward: -842.7036\n",
      "Epoch 9649/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.0947 - total_train_reward: -1650.8144\n",
      "Epoch 9650/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5667 - total_train_reward: -1501.3172\n",
      "Epoch 9651/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.8027 - total_train_reward: -1434.7040\n",
      "Epoch 9652/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1314 - total_train_reward: -1217.9207\n",
      "Epoch 9653/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.0621 - total_train_reward: -1222.8554\n",
      "Epoch 9654/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9307 - total_train_reward: -1648.6215\n",
      "Epoch 9655/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.2199 - total_train_reward: -1306.0305\n",
      "Epoch 9656/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.8759 - total_train_reward: -1362.0817\n",
      "Epoch 9657/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2537 - total_train_reward: -1778.4278\n",
      "Epoch 9658/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -10.7577 - total_train_reward: -1343.5415\n",
      "Epoch 9659/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1462 - total_train_reward: -1297.0161\n",
      "Epoch 9660/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3701 - total_train_reward: -1215.3348\n",
      "Epoch 9661/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2397 - total_train_reward: -1808.0774\n",
      "Epoch 9662/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9561 - total_train_reward: -1343.9768\n",
      "Epoch 9663/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5277 - total_train_reward: -1215.5899\n",
      "Epoch 9664/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.3528 - total_train_reward: -1354.9859\n",
      "Epoch 9665/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0990 - total_train_reward: -1196.5429\n",
      "Epoch 9666/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.7275 - total_train_reward: -1322.7874\n",
      "Epoch 9667/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.9149 - total_train_reward: -956.6238\n",
      "Epoch 9668/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.6695 - total_train_reward: -1224.3642\n",
      "Epoch 9669/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.9184 - total_train_reward: -1489.3862\n",
      "Epoch 9670/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.3497 - total_train_reward: -1232.5312\n",
      "Epoch 9671/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4849 - total_train_reward: -1063.2900\n",
      "Epoch 9672/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.0315 - total_train_reward: -1311.2475\n",
      "Epoch 9673/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.0805 - total_train_reward: -1214.1779\n",
      "Epoch 9674/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.6227 - total_train_reward: -1397.6228\n",
      "Epoch 9675/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.1449 - total_train_reward: -1742.8370\n",
      "Epoch 9676/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0613 - total_train_reward: -1174.8039\n",
      "Epoch 9677/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.0800 - total_train_reward: -1287.1356\n",
      "Epoch 9678/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6531 - total_train_reward: -1457.0194\n",
      "Epoch 9679/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.9502 - total_train_reward: -1793.5129\n",
      "Epoch 9680/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1149 - total_train_reward: -1067.2198\n",
      "Epoch 9681/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0647 - total_train_reward: -1454.1099\n",
      "Epoch 9682/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7027 - total_train_reward: -1077.4655\n",
      "Epoch 9683/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 6.3095 - total_train_reward: -1081.1889\n",
      "Epoch 9684/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7018 - total_train_reward: -1367.9246\n",
      "Epoch 9685/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.3148 - total_train_reward: -1121.5462\n",
      "Epoch 9686/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.5603 - total_train_reward: -844.5065\n",
      "Epoch 9687/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5424 - total_train_reward: -1363.8704\n",
      "Epoch 9688/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.0829 - total_train_reward: -1273.7703\n",
      "Epoch 9689/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.3480 - total_train_reward: -1501.1045\n",
      "Epoch 9690/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.9701 - total_train_reward: -1510.0061\n",
      "Epoch 9691/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.8935 - total_train_reward: -1075.4808\n",
      "Epoch 9692/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.0569 - total_train_reward: -726.4961\n",
      "Epoch 9693/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.2804 - total_train_reward: -1165.1299\n",
      "Epoch 9694/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.8238 - total_train_reward: -1110.6542\n",
      "Epoch 9695/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.7032 - total_train_reward: -963.9829\n",
      "Epoch 9696/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2211 - total_train_reward: -1201.3723\n",
      "Epoch 9697/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.5061 - total_train_reward: -1235.2407\n",
      "Epoch 9698/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.9882 - total_train_reward: -1206.8771\n",
      "Epoch 9699/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.0049 - total_train_reward: -725.7625\n",
      "Epoch 9700/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5166 - total_train_reward: -1829.6500\n",
      "Epoch 9701/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.7657 - total_train_reward: -1063.9447\n",
      "Epoch 9702/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.1974 - total_train_reward: -1223.0151\n",
      "Epoch 9703/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3497 - total_train_reward: -1611.0333\n",
      "Epoch 9704/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.1543 - total_train_reward: -1064.6769\n",
      "Epoch 9705/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.3151 - total_train_reward: -958.2055\n",
      "Epoch 9706/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.6609 - total_train_reward: -1190.4561\n",
      "Epoch 9707/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.4442 - total_train_reward: -1063.4882\n",
      "Epoch 9708/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.8460 - total_train_reward: -1064.7093\n",
      "Epoch 9709/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1445 - total_train_reward: -1057.3859\n",
      "Epoch 9710/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2176 - total_train_reward: -1073.2659\n",
      "Epoch 9711/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4425 - total_train_reward: -1742.6396\n",
      "Epoch 9712/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.0091 - total_train_reward: -727.2624\n",
      "Epoch 9713/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8263 - total_train_reward: -1325.4573\n",
      "Epoch 9714/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.4236 - total_train_reward: -1257.3603\n",
      "Epoch 9715/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -1.4923 - total_train_reward: -1044.7699\n",
      "Epoch 9716/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8832 - total_train_reward: -1613.3343\n",
      "Epoch 9717/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4152 - total_train_reward: -938.7814\n",
      "Epoch 9718/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5816 - total_train_reward: -1066.1806\n",
      "Epoch 9719/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -12.9013 - total_train_reward: -1738.4320\n",
      "Epoch 9720/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.0052 - total_train_reward: -1390.4241\n",
      "Epoch 9721/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.5765 - total_train_reward: -1780.3205\n",
      "Epoch 9722/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.4145 - total_train_reward: -1592.2092\n",
      "Epoch 9723/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 4.1699 - total_train_reward: -1302.7621\n",
      "Epoch 9724/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.5887 - total_train_reward: -1180.6820\n",
      "Epoch 9725/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 10.3977 - total_train_reward: -1215.5156\n",
      "Epoch 9726/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.0945 - total_train_reward: -1070.2129\n",
      "Epoch 9727/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.5898 - total_train_reward: -1277.9739\n",
      "Epoch 9728/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.5983 - total_train_reward: -1331.3494\n",
      "Epoch 9729/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.9046 - total_train_reward: -1579.7845\n",
      "Epoch 9730/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.5826 - total_train_reward: -978.0502\n",
      "Epoch 9731/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 7.9922 - total_train_reward: -1246.3107\n",
      "Epoch 9732/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5062 - total_train_reward: -1215.4440\n",
      "Epoch 9733/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.2496 - total_train_reward: -1055.6523\n",
      "Epoch 9734/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.7107 - total_train_reward: -1216.9080\n",
      "Epoch 9735/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.9432 - total_train_reward: -728.0761\n",
      "Epoch 9736/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.6387 - total_train_reward: -1067.6950\n",
      "Epoch 9737/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -3.2302 - total_train_reward: -1200.8740\n",
      "Epoch 9738/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.5800 - total_train_reward: -1052.6542\n",
      "Epoch 9739/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.2945 - total_train_reward: -1337.0651\n",
      "Epoch 9740/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.3800 - total_train_reward: -1579.5229\n",
      "Epoch 9741/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5429 - total_train_reward: -1270.7460\n",
      "Epoch 9742/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.2874 - total_train_reward: -1609.9811\n",
      "Epoch 9743/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8870 - total_train_reward: -1234.3817\n",
      "Epoch 9744/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.6003 - total_train_reward: -1565.0927\n",
      "Epoch 9745/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.6924 - total_train_reward: -1275.6654\n",
      "Epoch 9746/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.9405 - total_train_reward: -962.3621\n",
      "Epoch 9747/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.8959 - total_train_reward: -922.2593\n",
      "Epoch 9748/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -13.0791 - total_train_reward: -1500.2163\n",
      "Epoch 9749/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.2875 - total_train_reward: -1280.1965\n",
      "Epoch 9750/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.5476 - total_train_reward: -1595.4541\n",
      "Epoch 9751/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.8041 - total_train_reward: -1079.8290\n",
      "Epoch 9752/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.8971 - total_train_reward: -1325.2405\n",
      "Epoch 9753/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -4.5990 - total_train_reward: -947.4099\n",
      "Epoch 9754/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.2794 - total_train_reward: -1216.7128\n",
      "Epoch 9755/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.7969 - total_train_reward: -1216.2654\n",
      "Epoch 9756/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4802 - total_train_reward: -1552.7203\n",
      "Epoch 9757/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -7.1409 - total_train_reward: -1178.6136\n",
      "Epoch 9758/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.3107 - total_train_reward: -1525.1163\n",
      "Epoch 9759/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.1518 - total_train_reward: -1345.1481\n",
      "Epoch 9760/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.2545 - total_train_reward: -1548.6158\n",
      "Epoch 9761/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.9311 - total_train_reward: -1726.6493\n",
      "Epoch 9762/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.6479 - total_train_reward: -1787.0743\n",
      "Epoch 9763/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.1120 - total_train_reward: -957.0978\n",
      "Epoch 9764/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1909 - total_train_reward: -1244.3788\n",
      "Epoch 9765/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.1445 - total_train_reward: -1202.2181\n",
      "Epoch 9766/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.2093 - total_train_reward: -1557.5966\n",
      "Epoch 9767/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.8753 - total_train_reward: -1132.7251\n",
      "Epoch 9768/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.0492 - total_train_reward: -1757.1324\n",
      "Epoch 9769/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -11.4633 - total_train_reward: -1319.8765\n",
      "Epoch 9770/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 2.7723 - total_train_reward: -970.4512\n",
      "Epoch 9771/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 1.4340 - total_train_reward: -1496.1315\n",
      "Epoch 9772/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.5547 - total_train_reward: -1581.7436\n",
      "Epoch 9773/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 3.1410 - total_train_reward: -1283.7895\n",
      "Epoch 9774/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 5.6117 - total_train_reward: -837.9391\n",
      "Epoch 9775/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -8.3401 - total_train_reward: -1413.1665\n",
      "Epoch 9776/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -2.3710 - total_train_reward: -955.2556\n",
      "Epoch 9777/10000\n",
      "1/1 [==============================] - 3s 3s/step - actor_loss: -10.0136 - total_train_reward: -1659.0806\n",
      "Epoch 9778/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: 0.6345 - total_train_reward: -809.0044\n",
      "Epoch 9779/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -9.4385 - total_train_reward: -1517.6807\n",
      "Epoch 9780/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -10.2978 - total_train_reward: -1463.2126\n",
      "Epoch 9781/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.1606 - total_train_reward: -1201.2872\n",
      "Epoch 9782/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -5.2269 - total_train_reward: -1307.3434\n",
      "Epoch 9783/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -6.4825 - total_train_reward: -1295.4824\n",
      "Epoch 9784/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -3.7876 - total_train_reward: -1233.9949\n",
      "Epoch 9785/10000\n",
      "1/1 [==============================] - 2s 2s/step - actor_loss: -0.0013 - total_train_reward: -1213.6737\n",
      "Epoch 9786/10000\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        model_builder,\n",
    "        objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "        max_trials = 50,\n",
    "        #distribution_strategy= strategy,\n",
    "        directory=dir,\n",
    "        project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1],  epochs= 10000)\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "        writer= \"Training/fit_ppo_tf/\"\n",
    "\n",
    "        training_steps = 1000000\n",
    "        entropy_factor = 0.05\n",
    "        discount = 0.99\n",
    "        dense_units_actor = [128]#64, 32]\n",
    "        num_layers_actor = 1\n",
    "        dense_units_critic = [128]#64,32]\n",
    "        num_layers_crit =1\n",
    "\n",
    "        model = get_model(discount = discount, dense_units_act=dense_units_actor,  dense_units_crit=dense_units_critic, num_layer_a=num_layers_actor, num_layer_c = num_layers_crit,\n",
    "                          writer = writer, environment_name = ENV,reward_scaler = 1,  lr_actor= 0.00001, lr_critic= 0.00001, \n",
    "                      gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 50)\n",
    "                 \n",
    "        model.compile()\n",
    "        model.fit([-1], [-1], epochs= 1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=50):\n",
    "    if int(trials.trial_id) == 49:\n",
    "        print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = gym.make(ENV, render_mode = \"rgb_array\")\n",
    "dir = r\"Hyperparam_kt_ppo\"\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=1):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "\n",
    "    training_steps = 1000000\n",
    "    entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "    discount = trials.hyperparameters.values[\"discount\"]\n",
    "    dense_units_actor = [128]#64, 32]\n",
    "    num_layers_actor = 1\n",
    "    dense_units_critic = [128]#64,32]\n",
    "    num_layers_crit =1\n",
    "\n",
    "    model = run_training(training_steps,  discount, dense_units_actor,  dense_units_critic, num_layers_actor, num_layers_crit, writer, \n",
    "                    environment_name = ENV,reward_scaler = 1, return_agent = True, lr_actor= 0.00001, lr_critic= 0.00001, \n",
    "                    gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 50)\n",
    "\n",
    "    \n",
    "env_model = tuner.get_best_models()[0]\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\")\n",
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
