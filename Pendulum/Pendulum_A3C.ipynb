{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 10:56:40.192715: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from A3C_Agent import *\n",
    "from ENV_DETAILS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Artificial_Intelligence\\Portfolio\\RL_updated\\Pendulum\\Pendulum_A3C.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Artificial_Intelligence/Portfolio/RL_updated/Pendulum/Pendulum_A3C.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnt\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Artificial_Intelligence/Portfolio/RL_updated/Pendulum/Pendulum_A3C.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     main_hyper_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mArtificial_Intelligence\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mPortfolio\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mRL_updated\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mPendulum\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m# Windows\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Artificial_Intelligence/Portfolio/RL_updated/Pendulum/Pendulum_A3C.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     conda_python_exec \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39myanie\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39manaconda3\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39menvs\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mai_dev\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mpython.exe \u001b[39m\u001b[39m'\u001b[39m\u001b[39m# Windows\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\Pendulum\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/Pendulum/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_a3c/' # Linux\n",
    "\n",
    "ENV = \"Pendulum-v1\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81201607,  0.7711965 , -1.8408298 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1. -1. -8.], [1. 1. 8.], (3,), float32), (3,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1373922, -0.9905167,  0.9398161], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-2.0, 2.0, (1,), float32),\n",
       " Box(-2.0, 2.0, (1,), float32),\n",
       " (array([-0.7096961 , -0.7045079 , -0.95374924], dtype=float32),\n",
       "  -5.355570393613848,\n",
       "  False,\n",
       "  False,\n",
       "  {}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()[0]\n",
    "env.action_space, env.action_space, env.step(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Asynch Actor-Critic \n",
    "\n",
    "This time we will implement the A3C not considering Actor and Critic as part of the same network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs_general/hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_A3C/\"\n",
    "\n",
    "if TUNING_TYPE == \"MANUAL\":\n",
    "    params = {}\n",
    "    params[\"n_enviroment\"] =[20]\n",
    "    params[\"discount\"] =[0.96, 0.97]\n",
    "    params[\"end_of_episode\"] = [400]\n",
    "    params[\"learning_rate\"] = [0.001]\n",
    "    params[\"entropy_factor\"] = [ 0.01, 0.05]\n",
    "    params[\"epsilon\"] = [1]\n",
    "    params[\"boltzman_factor\"] = [1]\n",
    "    params[\"exploration_technique\"] = ['soft', 'epsilon']\n",
    "    params[\"training_steps\"] = [3000000]\n",
    "    params[\"dense_units\"] = [32, 128]\n",
    "    params[\"time_to_update\"] = [400]\n",
    "    params[\"use_LSTM\"] =[False]\n",
    "\n",
    "\n",
    "    hyperparam_combination = list(itertools.product(*list(params.values())))\n",
    "    \n",
    "\n",
    "    try:\n",
    "\n",
    "        files = [name for name in os.listdir(logs_dir) if os.path.isfile(os.path.join(logs_dir, name)) and name != \"logfile.txt\" and name != \"merged_results.json\"]\n",
    "        if len(files) >= 1: merge_JsonFiles(main_hyper_dir, logs_dir, files)\n",
    "\n",
    "        res_file = logs_dir+\"merged_results.json\"\n",
    "        def without_keys(d, keys):\n",
    "            return {x: d[x] for x in d if x not in keys}\n",
    "\n",
    "\n",
    "        if os.path.isfile(res_file):\n",
    "            with open(res_file, 'r') as f:\n",
    "                complete_file = json.load(f)\n",
    "\n",
    "            newlist = sorted(complete_file, key=lambda d: d['mean_rewards'], reverse=True) \n",
    "            params = []\n",
    "            for i, f in enumerate(newlist):\n",
    "                label = \"disc : \"+str(f['discount'])+\" | \"+\"lr : \"+str(f['learning_rate'])+\" | \"+\"entropy : \"+str(f['entropy_factor'])+\" | \"+\"update : \"+str(f['time_to_update'])\n",
    "                plt.figure(figsize=[35,4])\n",
    "                plt.plot(f['rewards'], label = label)\n",
    "                plt.legend()\n",
    "                max_mean_reward = f['mean_rewards']\n",
    "                params.append(f)\n",
    "                print(without_keys(f,\"rewards\"))\n",
    "                if i == 10:\n",
    "                    break\n",
    "            plt.title(\"Evaluation rewards\"); plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"continuous_actor_critic_agent\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " v (Dense)                   multiple                  33        \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  0 (unused)\n",
      "                                                                 \n",
      " actor_mu (Dense)            multiple                  33        \n",
      "                                                                 \n",
      " actor_sigma (Dense)         multiple                  33        \n",
      "                                                                 \n",
      " inp_0 (Dense)               multiple                  128       \n",
      "                                                                 \n",
      " inp_1 (Dense)               multiple                  1056      \n",
      "                                                                 \n",
      " inp_2 (Dense)               multiple                  1056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,339\n",
      "Trainable params: 2,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_layers = 3\n",
    "dense = [32 for _ in range(num_layers)]\n",
    "actorcritic_agent = ContinuousActorCriticAgent(\n",
    "                    env = env, \n",
    "                    d = dense,\n",
    "                    lstm_unit = 32,\n",
    "                    use_LSTM = False,\n",
    "                    sigma_noise = 0.0001,\n",
    "                    num_layers = num_layers\n",
    "                    )\n",
    "obs = env.reset()[0]\n",
    "obs = obs.reshape((1,env.observation_space.shape[0]))\n",
    "actorcritic_agent(obs, inference = True)\n",
    "actorcritic_agent.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_kt_A3C/keras_tunning_soft/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_A3C\"\n",
    "    project_name = \"keras_tunning_soft\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel(hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/A3C_soft/\", \n",
    "                          end_of_episode = EPISODES, n_enviroment = 10, \n",
    "                  evaluation_epoch = 2000, training_steps = 500000,\n",
    "                  sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                  discount_min = 0.90, discount_max = 0.99,\n",
    "                  entropy_min = 0.00001, entropy_max = 0.01,\n",
    "                  lr_min = 0.000001, lr_max = 0.005,\n",
    "                  dense_min = 32, dense_max = 300,\n",
    "                  lstm_min = 32, lstm_max = 128,\n",
    "                  time_to_update_min = 100, time_to_update_max=600,\n",
    "                  environment_name=ENV,\n",
    "                  continuous_actions_space= True,\n",
    "                  reward_scaler = 16.2736044,\n",
    "                  n_dense_layers = 4\n",
    "                  ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 30,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "   \n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "        writer= \"Training/fit_A3C/\"\n",
    "\n",
    "\n",
    "        exploration_tech = \"soft\"\n",
    "        discount = 0.99\n",
    "        learning_rate = 0.00001\n",
    "        entropy_factor = 0.001\n",
    "        dense_units = [256]\n",
    "        sigma_noise =  1e-5\n",
    "\n",
    "        n_enviroment = 30\n",
    "        end_ep = 1000\n",
    "        ep = 1\n",
    "        bolt_fact = 1\n",
    "        training_steps = 4000000\n",
    "\n",
    "        time_to_update= 100\n",
    "        lstm_units= 32\n",
    "        end_of_episode = EPISODES\n",
    "        save_factor=50000\n",
    "        sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH\n",
    "        sucess_criteria_value = SUCESS_CRITERIA_VALUE\n",
    "        environment_name = ENV\n",
    "        reward_scaler = 16.2736044\n",
    "        continuous_space_actions = True\n",
    "        return_agent = True\n",
    "        use_LSTM = False\n",
    "\n",
    "        env_model = run_training(training_steps, learning_rate, entropy_factor, exploration_tech, discount, time_to_update, dense_units, lstm_units, \n",
    "                        n_enviroment, writer, use_LSTM, end_of_episode, save_factor, sucess_criteria_epochs , sucess_criteria_value , \n",
    "                        environment_name, reward_scaler, continuous_space_actions, sigma_noise, return_agent)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial id :28 | Score :-10.882266520567075 --> {'discount': 0.91, 'entropy_factor': 0.004869523196580806, 'learning_rate': 6.325876829563758e-05, 'n_dense_layers': 1, 'dense_units_0': 164, 'dense_units_1': 177, 'dense_units_2': 86, 'dense_units_3': 134}\n",
      "Trial id :11 | Score :-45.21249650747056 --> {'discount': 0.92, 'entropy_factor': 0.0025030534967846957, 'learning_rate': 0.00136942884893527, 'n_dense_layers': 2, 'dense_units_0': 227, 'dense_units_1': 129, 'dense_units_2': 101}\n",
      "Trial id :09 | Score :-68.6664493532472 --> {'discount': 0.93, 'entropy_factor': 0.00046711278429575583, 'learning_rate': 0.0001548827581762531, 'n_dense_layers': 2, 'dense_units_0': 151, 'dense_units_1': 69, 'dense_units_2': 147}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=3):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "\n",
    "    n_env = 30\n",
    "    end_ep = 1000\n",
    "    ep = 1\n",
    "    bolt_fact = 1\n",
    "    train_steps = 5000000\n",
    "    disc = trials.hyperparameters.values[\"discount\"]\n",
    "    lr = trials.hyperparameters.values[\"learning_rate\"]\n",
    "    entropy_fact = trials.hyperparameters.values[\"entropy_factor\"]\n",
    "    n_hidden_layer_hp = trials.hyperparameters.values[\"n_dense_layers\"]\n",
    "\n",
    "    d = []\n",
    "    for i in range(n_hidden_layer_hp):\n",
    "        d.append(trials.hyperparameters.values['dense_units_'+str(i)])\n",
    "\n",
    "    sigma_noise = 1e-5\n",
    "    ulstm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_kt_A3C/keras_tunning_soft/tuner0.json\n",
      "Trial id :28 | Score :-10.882266520567075 --> {'discount': 0.91, 'entropy_factor': 0.004869523196580806, 'learning_rate': 6.325876829563758e-05, 'n_dense_layers': 1, 'dense_units_0': 164, 'dense_units_1': 177, 'dense_units_2': 86, 'dense_units_3': 134}\n",
      "reading...\n",
      "Trial number :  30\n",
      "Trial id :11 | Score :-45.21249650747056 --> {'discount': 0.92, 'entropy_factor': 0.0025030534967846957, 'learning_rate': 0.00136942884893527, 'n_dense_layers': 2, 'dense_units_0': 227, 'dense_units_1': 129, 'dense_units_2': 101}\n",
      "Trial id :09 | Score :-68.6664493532472 --> {'discount': 0.93, 'entropy_factor': 0.00046711278429575583, 'learning_rate': 0.0001548827581762531, 'n_dense_layers': 2, 'dense_units_0': 151, 'dense_units_1': 69, 'dense_units_2': 147}\n",
      "Moviepy - Building video ./A3C_soft_video.mp4.\n",
      "Moviepy - Writing video ./A3C_soft_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./A3C_soft_video.mp4\n",
      "Final mean reward ' soft ': -11.025725769174128\n"
     ]
    }
   ],
   "source": [
    "val_env = gym.make(ENV, render_mode = \"rgb_array\")\n",
    "dir = r\"Hyperparam_kt_A3C\"\n",
    "\n",
    "exploration_tech =\"soft\"\n",
    "project_name = \"keras_tunning_\"+exploration_tech\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "        MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/A3C_\"+exploration_tech+\"/\", exploration_tech =exploration_tech, \n",
    "                        end_of_episode = EPISODES, n_enviroment = 10, \n",
    "                  evaluation_epoch = 2000, training_steps = 500000,\n",
    "                  sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                  discount_min = 0.90, discount_max = 0.99,\n",
    "                  entropy_min = 0.00001, entropy_max = 0.01,\n",
    "                  lr_min = 0.000001, lr_max = 0.005,\n",
    "                  dense_min = 32, dense_max = 300,\n",
    "                  lstm_min = 32, lstm_max = 128,\n",
    "                  time_to_update_min = 100, time_to_update_max=600,\n",
    "                  environment_name=ENV,\n",
    "                  continuous_actions_space= True,\n",
    "                  reward_scaler = 16.2736044\n",
    "                  ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 30,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "    )\n",
    "\n",
    "for i, trials in enumerate(tuner.oracle.get_best_trials(num_trials=3)):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "    \n",
    "    if i ==0:\n",
    "        print(\"reading...\")\n",
    "        env_model = tuner.get_best_models()[0]\n",
    "        discount = trials.hyperparameters.values[\"discount\"]\n",
    "        learning_rate = trials.hyperparameters.values[\"learning_rate\"]\n",
    "        entropy_factor = trials.hyperparameters.values[\"entropy_factor\"]\n",
    "        n_hidden_layer_hp = trials.hyperparameters.values[\"n_dense_layers\"]\n",
    "\n",
    "        dense_units = []\n",
    "        for i in range(n_hidden_layer_hp):\n",
    "            dense_units.append(trials.hyperparameters.values['dense_units_'+str(i)])\n",
    "        sigma_noise = 1e-5\n",
    "\n",
    "\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech, sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, reward_scaler = 16.2736044 ,   video_name = \"./A3C_\"+exploration_tech+\"_video.mp4\", continuous_action_space= True)\n",
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
