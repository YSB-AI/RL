{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 16:39:24.241611: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_with_intrinsic_reward import * #PPO_Agent_v2 PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\Pendulum\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/Pendulum/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_a3c/' # Linux\n",
    "\n",
    "ENV = \"Pendulum-v1\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PendulumEnv<Pendulum-v1>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08234285, -0.23212363,  4.406051  ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1. -1. -8.], [1. 1. 8.], (3,), float32), 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96322674"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-2.0, 2.0, (1,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-2.0, 2.0, (1,), float32),\n",
       " Box(-2.0, 2.0, (1,), float32),\n",
       " (array([-0.46611664,  0.88472325,  0.5077581 ], dtype=float32),\n",
       "  -4.123292908600494,\n",
       "  False,\n",
       "  {}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space, env.action_space, env.step(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs_general/hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_PPO/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial number :  0\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.97              |0.97              |discount\n",
      "0.94              |0.94              |gae_lambda\n",
      "0.00084088        |0.00084088        |lr_actor\n",
      "0.0008108         |0.0008108         |lr_critic\n",
      "0.00099861        |0.00099861        |lr_model\n",
      "0.1               |0.1               |policy_clip\n",
      "0.31064           |0.31064           |scaling_factor_reward\n",
      "0.012824          |0.012824          |entropy_coeff\n",
      "2                 |2                 |n_dense_layers_actor\n",
      "252               |252               |dense_units_act_0\n",
      "2                 |2                 |n_dense_layers_critic\n",
      "187               |187               |dense_units_crit_0\n",
      "1                 |1                 |n_dense_layers_model\n",
      "64                |64                |n_dense_layers_model0\n",
      "\n",
      "Trial number :  1\n",
      "WARNING:tensorflow:From /home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "Epoch: 200 : Reward eval/Train: -1260.7020386278234/-1284.7505883529548 \n",
      "Epoch: 400 : Reward eval/Train: -1293.267634738415/-1248.3795010529298 \n",
      "Epoch: 600 : Reward eval/Train: -1478.5142073843936/-1451.380210392222 \n",
      "Epoch: 800 : Reward eval/Train: -1455.6564498251664/-1395.1096322848803 \n",
      "Epoch: 1000 : Reward eval/Train: -1489.6381533246908/-1394.0195485889317 \n",
      "Epoch: 1200 : Reward eval/Train: -1448.3969558119336/-1345.0595804150178 \n",
      "Epoch: 1400 : Reward eval/Train: -1416.1190513155157/-1310.9086391967817 \n",
      "Epoch: 1600 : Reward eval/Train: -1389.8811626143342/-1327.5538301711929 \n",
      "Epoch: 1800 : Reward eval/Train: -1427.1367093134272/-1306.714845884351 \n",
      "Epoch: 2000 : Reward eval/Train: -1468.0862750721496/-1357.507980242954 \n",
      "Epoch: 2200 : Reward eval/Train: -1452.1069778586657/-1356.2126053224627 \n",
      "Epoch: 2400 : Reward eval/Train: -1476.793900064692/-1327.7519021768233 \n",
      "Epoch: 2600 : Reward eval/Train: -1447.2311594638315/-1308.9797974626372 \n",
      "Epoch: 2800 : Reward eval/Train: -1469.0771322371033/-1326.5465854137735 \n",
      "Epoch: 3000 : Reward eval/Train: -1489.972195082179/-1341.5703484792373 \n",
      "Epoch: 3200 : Reward eval/Train: -1498.1659886548343/-1371.871924917824 \n",
      "Epoch: 3400 : Reward eval/Train: -1505.8410743792883/-1379.820769214834 \n",
      "Epoch: 3600 : Reward eval/Train: -1518.9976460279868/-1390.2473273475027 \n",
      "Epoch: 3800 : Reward eval/Train: -1530.8501582846372/-1394.3302685425815 \n",
      "Epoch: 4000 : Reward eval/Train: -1535.077656903743/-1415.0448201115023 \n",
      "Epoch: 4200 : Reward eval/Train: -1544.0693147079141/-1431.2273810077593 \n",
      "Epoch: 4400 : Reward eval/Train: -1557.0477223256582/-1447.664791401404 \n",
      "Epoch: 4600 : Reward eval/Train: -1551.2410044093888/-1463.4706564468015 \n",
      "Epoch: 4800 : Reward eval/Train: -1559.9337634760566/-1470.9607932124138 \n",
      "Epoch: 5000 : Reward eval/Train: -1547.6366636761263/-1481.4729466604656 \n",
      "Epoch: 5200 : Reward eval/Train: -1526.4741891700073/-1474.1012450280386 \n",
      "Epoch: 5400 : Reward eval/Train: -1535.4980266994655/-1473.8771663424848 \n",
      "Epoch: 5600 : Reward eval/Train: -1546.5588001305237/-1450.0916964269538 \n",
      "Epoch: 5800 : Reward eval/Train: -1541.1490428418506/-1462.4537101097444 \n",
      "Epoch: 6000 : Reward eval/Train: -1537.6834645536321/-1471.626095363296 \n",
      "Epoch: 6200 : Reward eval/Train: -1539.6281821392324/-1474.6842518205528 \n",
      "Epoch: 6400 : Reward eval/Train: -1549.8161053262156/-1454.9695854428987 \n",
      "Epoch: 6600 : Reward eval/Train: -1546.1997058959073/-1466.3288618374406 \n",
      "Epoch: 6800 : Reward eval/Train: -1532.1158412730372/-1474.2043617980935 \n",
      "Epoch: 7000 : Reward eval/Train: -1513.8362862599706/-1462.1769690650585 \n",
      "Epoch: 7200 : Reward eval/Train: -1521.666635067803/-1464.452714038895 \n",
      "Epoch: 7400 : Reward eval/Train: -1511.4875315596692/-1472.1541135155128 \n",
      "Epoch: 7600 : Reward eval/Train: -1504.6934668951703/-1483.9825755795098 \n",
      "Epoch: 7800 : Reward eval/Train: -1499.5429669384068/-1470.4989663017452 \n",
      "Epoch: 8000 : Reward eval/Train: -1501.4617258758283/-1474.6863975766264 \n",
      "Epoch: 8200 : Reward eval/Train: -1510.1410663867578/-1474.13839904168 \n",
      "Epoch: 8400 : Reward eval/Train: -1514.0529649077848/-1482.6022531675878 \n",
      "Epoch: 8600 : Reward eval/Train: -1514.4540673549075/-1480.6783504555406 \n",
      "Epoch: 8800 : Reward eval/Train: -1518.0485980749693/-1474.4838632360663 \n",
      "Epoch: 9000 : Reward eval/Train: -1522.0664644669528/-1484.2939579244085 \n",
      "Epoch: 9200 : Reward eval/Train: -1522.9858899638261/-1480.5611325625318 \n",
      "Epoch: 9400 : Reward eval/Train: -1511.7956805619067/-1485.0869491275644 \n",
      "Epoch: 9600 : Reward eval/Train: -1508.357744084752/-1487.2375607333097 \n",
      "Epoch: 9800 : Reward eval/Train: -1502.4311642870114/-1494.1455375283745 \n",
      "Epoch: 10000 : Reward eval/Train: -1506.4065481808204/-1493.964023488035 \n",
      "Epoch: 10200 : Reward eval/Train: -1511.600664548097/-1485.290227234665 \n",
      "Epoch: 10400 : Reward eval/Train: -1507.6887186985853/-1491.8231248656323 \n",
      "Epoch: 10600 : Reward eval/Train: -1500.6489249163706/-1489.2169666498692 \n",
      "Epoch: 10800 : Reward eval/Train: -1506.1351440582398/-1494.3675665390303 \n",
      "Epoch: 11000 : Reward eval/Train: -1508.5848105724695/-1485.0616688313933 \n",
      "Epoch: 11200 : Reward eval/Train: -1510.1502262547608/-1489.282943633429 \n",
      "Epoch: 11400 : Reward eval/Train: -1502.9716810002612/-1494.0644855661196 \n",
      "Epoch: 11600 : Reward eval/Train: -1500.750177964362/-1488.0097486892134 \n",
      "Epoch: 11800 : Reward eval/Train: -1505.2665468070143/-1493.5784208117495 \n",
      "Epoch: 12000 : Reward eval/Train: -1510.0798696910826/-1486.796155182303 \n",
      "Epoch: 12200 : Reward eval/Train: -1504.4723692595917/-1490.3819942835742 \n",
      "Epoch: 12400 : Reward eval/Train: -1506.6588724936307/-1496.1761402774448 \n",
      "Epoch: 12600 : Reward eval/Train: -1502.0194125204828/-1494.199897961424 \n",
      "Epoch: 12800 : Reward eval/Train: -1498.436508681496/-1498.2814422595097 \n",
      "Epoch: 13000 : Reward eval/Train: -1503.6987264688007/-1503.4226129952087 \n",
      "Epoch: 13200 : Reward eval/Train: -1503.036206493709/-1493.8311028736405 \n",
      "Epoch: 13400 : Reward eval/Train: -1496.7281297995594/-1491.3250380947059 \n",
      "Epoch: 13600 : Reward eval/Train: -1492.5031107576594/-1495.956145834135 \n",
      "Epoch: 13800 : Reward eval/Train: -1487.7161629570728/-1494.540786320593 \n",
      "Epoch: 14000 : Reward eval/Train: -1485.65846246014/-1494.950569456683 \n",
      "Epoch: 14200 : Reward eval/Train: -1478.6399756606731/-1488.071159202382 \n",
      "Epoch: 14400 : Reward eval/Train: -1478.6369180255106/-1481.7175101323407 \n",
      "Epoch: 14600 : Reward eval/Train: -1478.0469023432352/-1480.2962368206847 \n",
      "Epoch: 14800 : Reward eval/Train: -1482.3899791131673/-1477.5737272828621 \n",
      "Epoch: 15000 : Reward eval/Train: -1486.1752465012873/-1469.6395147417609 \n",
      "Epoch: 15200 : Reward eval/Train: -1482.1084495270254/-1462.0346327628588 \n",
      "Epoch: 15400 : Reward eval/Train: -1476.626740303816/-1466.9867560327377 \n",
      "Epoch: 15600 : Reward eval/Train: -1469.1475186845098/-1462.2367718233495 \n",
      "Epoch: 15800 : Reward eval/Train: -1473.741681048462/-1455.2330277428953 \n",
      "Epoch: 16000 : Reward eval/Train: -1468.22110244133/-1451.1023744074828 \n",
      "Epoch: 16200 : Reward eval/Train: -1468.0503680836457/-1450.7037378530474 \n",
      "Epoch: 16400 : Reward eval/Train: -1462.2798410988269/-1455.6596131799527 \n",
      "Epoch: 16600 : Reward eval/Train: -1457.7832767440257/-1460.2384725529173 \n",
      "Epoch: 16800 : Reward eval/Train: -1453.254860673225/-1455.8974528392932 \n",
      "Epoch: 17000 : Reward eval/Train: -1457.6229348732168/-1450.6560947317596 \n",
      "Epoch: 17200 : Reward eval/Train: -1454.9015238522668/-1445.1510713783325 \n",
      "Epoch: 17400 : Reward eval/Train: -1456.1184628668657/-1439.8334343775766 \n",
      "Epoch: 17600 : Reward eval/Train: -1460.8936161867225/-1437.8868823720304 \n",
      "Epoch: 17800 : Reward eval/Train: -1457.5031121369368/-1434.4314289451784 \n",
      "Epoch: 18000 : Reward eval/Train: -1453.5184623893585/-1431.6637499956628 \n",
      "Epoch: 18200 : Reward eval/Train: -1449.0043721232623/-1428.5233503848376 \n",
      "Epoch: 18400 : Reward eval/Train: -1453.873101529468/-1424.746793512881 \n",
      "Epoch: 18600 : Reward eval/Train: -1452.357466677052/-1420.9015605971233 \n",
      "Epoch: 18800 : Reward eval/Train: -1449.0727017708139/-1417.7567771691224 \n",
      "Epoch: 19000 : Reward eval/Train: -1453.2482404146294/-1415.473679340702 \n",
      "Epoch: 19200 : Reward eval/Train: -1453.4834140192995/-1412.4964048292202 \n",
      "Epoch: 19400 : Reward eval/Train: -1450.1718563070046/-1410.0310150846108 \n",
      "Epoch: 19600 : Reward eval/Train: -1447.4179301095949/-1407.4878317266962 \n",
      "Epoch: 19800 : Reward eval/Train: -1443.6495108271008/-1404.9309170649879 \n",
      "Epoch: 20000 : Reward eval/Train: -1446.6712806997436/-1403.030116002599 \n",
      "Epoch: 20200 : Reward eval/Train: -1442.9679897043184/-1401.5278899478014 \n",
      "Epoch: 20400 : Reward eval/Train: -1440.040538551049/-1400.3702839917794 \n",
      "Epoch: 20600 : Reward eval/Train: -1435.6713904800713/-1396.723196360996 \n",
      "Epoch: 20800 : Reward eval/Train: -1431.4073094857335/-1393.9495315157462 \n",
      "Epoch: 21000 : Reward eval/Train: -1428.3499522156062/-1389.1744392241446 \n",
      "Epoch: 21200 : Reward eval/Train: -1430.7017484622043/-1384.5276408327989 \n",
      "Epoch: 21400 : Reward eval/Train: -1433.7853054113211/-1380.0420056919183 \n",
      "Epoch: 21600 : Reward eval/Train: -1431.6207532755889/-1377.5224156581908 \n",
      "Epoch: 21800 : Reward eval/Train: -1428.0375966524862/-1375.5087655422287 \n",
      "Epoch: 22000 : Reward eval/Train: -1426.886253978956/-1373.842370667686 \n",
      "Epoch: 22200 : Reward eval/Train: -1428.7253293628596/-1371.6656284588623 \n",
      "Epoch: 22400 : Reward eval/Train: -1427.9136824847758/-1368.9619668066148 \n",
      "Epoch: 22600 : Reward eval/Train: -1425.87603506565/-1367.8706659007123 \n",
      "Epoch: 22800 : Reward eval/Train: -1423.9444385547513/-1370.3293020413591 \n",
      "Epoch: 23000 : Reward eval/Train: -1423.9142187262275/-1368.6111885828634 \n",
      "Epoch: 23200 : Reward eval/Train: -1423.033206972734/-1368.808785410015 \n",
      "Epoch: 23400 : Reward eval/Train: -1423.0014181727886/-1366.9662017257494 \n",
      "Epoch: 23600 : Reward eval/Train: -1422.4456767286254/-1366.130500240564 \n",
      "Epoch: 23800 : Reward eval/Train: -1421.2345372408886/-1365.8612439230978 \n",
      "Epoch: 24000 : Reward eval/Train: -1421.1887199275486/-1365.0472600331932 \n",
      "Epoch: 24200 : Reward eval/Train: -1420.7578044824345/-1364.3040773589803 \n",
      "Epoch: 24400 : Reward eval/Train: -1420.5816522411437/-1364.9238700606234 \n",
      "Epoch: 24600 : Reward eval/Train: -1419.0021780618995/-1365.0209387270056 \n",
      "Epoch: 24800 : Reward eval/Train: -1416.086315810429/-1365.3031036167397 \n",
      "Epoch: 25000 : Reward eval/Train: -1415.3444784371204/-1363.0384931871106 \n",
      "Epoch: 25200 : Reward eval/Train: -1414.9742869986528/-1362.4182924526908 \n",
      "Epoch: 25400 : Reward eval/Train: -1411.2439800514105/-1362.7245892983972 \n",
      "Epoch: 25600 : Reward eval/Train: -1411.0185209637061/-1362.4878327367915 \n",
      "Epoch: 25800 : Reward eval/Train: -1411.095590097891/-1362.1728136472957 \n",
      "Epoch: 26000 : Reward eval/Train: -1410.3219875904838/-1360.894923298226 \n",
      "Epoch: 26200 : Reward eval/Train: -1410.3050890718732/-1360.5332243414089 \n",
      "Epoch: 26400 : Reward eval/Train: -1409.4593041664184/-1362.5023408938343 \n",
      "Epoch: 26600 : Reward eval/Train: -1410.530035946098/-1362.6354595466516 \n",
      "Epoch: 26800 : Reward eval/Train: -1409.9818147291285/-1362.831660215401 \n",
      "Epoch: 27000 : Reward eval/Train: -1409.3344911263648/-1362.0422628634415 \n",
      "Epoch: 27200 : Reward eval/Train: -1409.101289349121/-1361.8138112320698 \n",
      "Epoch: 27400 : Reward eval/Train: -1410.4905450107362/-1360.5280257073916 \n",
      "Epoch: 27600 : Reward eval/Train: -1408.9869898510253/-1359.0302052059708 \n",
      "Epoch: 27800 : Reward eval/Train: -1409.32292649129/-1357.6932257532578 \n",
      "Epoch: 28000 : Reward eval/Train: -1409.9039499435612/-1358.8432692439417 \n",
      "Epoch: 28200 : Reward eval/Train: -1411.510534690434/-1361.5767957438406 \n",
      "Epoch: 28400 : Reward eval/Train: -1409.849593995693/-1361.333993581911 \n",
      "Epoch: 28600 : Reward eval/Train: -1407.977019424978/-1358.3824400582023 \n",
      "Epoch: 28800 : Reward eval/Train: -1410.0608529779738/-1356.319554583616 \n",
      "Epoch: 29000 : Reward eval/Train: -1409.746349995833/-1355.952749076354 \n",
      "Epoch: 29200 : Reward eval/Train: -1411.5005031576322/-1356.447580174398 \n",
      "Epoch: 29400 : Reward eval/Train: -1412.6066830814657/-1355.4565267113792 \n",
      "Epoch: 29600 : Reward eval/Train: -1413.802440429372/-1355.3348511434904 \n",
      "Epoch: 29800 : Reward eval/Train: -1414.1630683847673/-1356.3642373332427 \n",
      "Epoch: 30000 : Reward eval/Train: -1414.0577401741016/-1355.3151471068036 \n",
      "Epoch: 30200 : Reward eval/Train: -1412.1266734119492/-1354.315873408874 \n",
      "Epoch: 30400 : Reward eval/Train: -1414.1668191932308/-1354.6994102913723 \n",
      "Epoch: 30600 : Reward eval/Train: -1413.5727717008044/-1355.985611401209 \n",
      "Epoch: 30800 : Reward eval/Train: -1415.6577342590679/-1355.528193583262 \n",
      "Epoch: 31000 : Reward eval/Train: -1417.2685429078508/-1356.11315617601 \n",
      "Epoch: 31200 : Reward eval/Train: -1418.061132446066/-1356.5221745742856 \n",
      "Epoch: 31400 : Reward eval/Train: -1418.9455654199717/-1355.5281507258587 \n",
      "Epoch: 31600 : Reward eval/Train: -1416.2610068729591/-1356.6100631712745 \n",
      "Epoch: 31800 : Reward eval/Train: -1417.1822903242805/-1355.527926929151 \n",
      "Epoch: 32000 : Reward eval/Train: -1414.436363648736/-1355.911564498072 \n",
      "Epoch: 32200 : Reward eval/Train: -1412.871937427078/-1354.8126748622662 \n",
      "Epoch: 32400 : Reward eval/Train: -1414.2629418994673/-1354.5464250913094 \n",
      "Epoch: 32600 : Reward eval/Train: -1414.956546507789/-1355.2771010807833 \n",
      "Epoch: 32800 : Reward eval/Train: -1413.0960905966854/-1356.8542896552106 \n",
      "Epoch: 33000 : Reward eval/Train: -1412.630501482896/-1356.3412576976978 \n",
      "Epoch: 33200 : Reward eval/Train: -1411.3359361129785/-1358.1627554134293 \n",
      "Epoch: 33400 : Reward eval/Train: -1409.8804869784403/-1357.2127550610567 \n",
      "Epoch: 33600 : Reward eval/Train: -1408.6004387616622/-1355.9606279177883 \n",
      "Epoch: 33800 : Reward eval/Train: -1406.80077570082/-1357.9521563054868 \n",
      "Epoch: 34000 : Reward eval/Train: -1405.0295236214608/-1356.8283311071607 \n",
      "Epoch: 34200 : Reward eval/Train: -1405.2574162123685/-1358.0395555050845 \n",
      "Epoch: 34400 : Reward eval/Train: -1403.3402091973542/-1356.894394332461 \n",
      "Epoch: 34600 : Reward eval/Train: -1403.674059536701/-1355.984870852244 \n",
      "Epoch: 34800 : Reward eval/Train: -1402.4284784640492/-1356.9109181809417 \n",
      "Epoch: 35000 : Reward eval/Train: -1401.4462058978738/-1357.6859328213636 \n",
      "Epoch: 35200 : Reward eval/Train: -1402.0313900829055/-1356.7649258173956 \n",
      "Epoch: 35400 : Reward eval/Train: -1402.9700838013666/-1355.5343465964047 \n",
      "Epoch: 35600 : Reward eval/Train: -1403.5545459136583/-1354.0581515448096 \n",
      "Epoch: 35800 : Reward eval/Train: -1402.478442192059/-1353.229049858542 \n",
      "Epoch: 36000 : Reward eval/Train: -1402.343833875816/-1355.0767949855408 \n",
      "Epoch: 36200 : Reward eval/Train: -1400.7462104290082/-1355.1728797305902 \n",
      "Epoch: 36400 : Reward eval/Train: -1399.9961117429089/-1354.8461290420207 \n",
      "Epoch: 36600 : Reward eval/Train: -1401.379876648588/-1354.1617142816401 \n",
      "Epoch: 36800 : Reward eval/Train: -1400.6473223444398/-1353.3487233835697 \n",
      "Epoch: 37000 : Reward eval/Train: -1400.5415027212491/-1352.5279152167166 \n",
      "Epoch: 37200 : Reward eval/Train: -1400.6655758859097/-1352.6854053941693 \n",
      "Epoch: 37400 : Reward eval/Train: -1399.74443230227/-1352.2770744902593 \n",
      "Epoch: 37600 : Reward eval/Train: -1398.2876205679256/-1352.4769829981701 \n",
      "Epoch: 37800 : Reward eval/Train: -1397.4275297607737/-1354.501434811378 \n",
      "Epoch: 38000 : Reward eval/Train: -1396.4848427275215/-1355.3046648994405 \n",
      "Epoch: 38200 : Reward eval/Train: -1395.1334907323874/-1355.0190693658922 \n",
      "Epoch: 38400 : Reward eval/Train: -1394.829386369139/-1353.8317101994062 \n",
      "Epoch: 38600 : Reward eval/Train: -1394.8319153215277/-1353.3019652085463 \n",
      "Epoch: 38800 : Reward eval/Train: -1393.5750189898902/-1353.8147583445118 \n",
      "Epoch: 39000 : Reward eval/Train: -1393.4949001122247/-1354.8491112368754 \n",
      "Epoch: 39200 : Reward eval/Train: -1392.8511706850359/-1354.3437797116928 \n",
      "Epoch: 39400 : Reward eval/Train: -1392.297480457867/-1354.4677192008548 \n",
      "Epoch: 39600 : Reward eval/Train: -1392.4264305805377/-1355.9407548623287 \n",
      "Epoch: 39800 : Reward eval/Train: -1391.5662199731034/-1355.8081085072683 \n",
      "Epoch: 40000 : Reward eval/Train: -1390.9934175723681/-1355.6801710078528 \n",
      "Epoch: 40200 : Reward eval/Train: -1390.2895757577119/-1355.0517064603532 \n",
      "Epoch: 40400 : Reward eval/Train: -1388.9791351319407/-1354.7956797079 \n",
      "Epoch: 40600 : Reward eval/Train: -1388.3190623376636/-1355.7230604762785 \n",
      "Epoch: 40800 : Reward eval/Train: -1387.8595588704054/-1355.1518472972678 \n",
      "Epoch: 41000 : Reward eval/Train: -1388.6244693788365/-1354.8265621365342 \n",
      "Epoch: 41200 : Reward eval/Train: -1389.5355873635176/-1355.5082046678087 \n",
      "Epoch: 41400 : Reward eval/Train: -1389.3532427221105/-1356.0590425028008 \n",
      "Epoch: 41600 : Reward eval/Train: -1389.9909674454543/-1356.2030893519366 \n",
      "Epoch: 41800 : Reward eval/Train: -1390.170116846762/-1355.4509442601525 \n",
      "Epoch: 42000 : Reward eval/Train: -1389.3288317470112/-1354.656682990879 \n",
      "Epoch: 42200 : Reward eval/Train: -1389.8798715139612/-1354.7862692660767 \n",
      "Epoch: 42400 : Reward eval/Train: -1390.6283921819502/-1355.5356419613063 \n",
      "Epoch: 42600 : Reward eval/Train: -1390.664985355064/-1355.6610271434981 \n",
      "Epoch: 42800 : Reward eval/Train: -1390.7126966894111/-1355.2387447277367 \n",
      "Epoch: 43000 : Reward eval/Train: -1389.3076361934425/-1354.6749654201226 \n",
      "Epoch: 43200 : Reward eval/Train: -1389.3359814735265/-1355.1397763323448 \n",
      "Epoch: 43400 : Reward eval/Train: -1388.6506148497442/-1355.3008458593515 \n",
      "Epoch: 43600 : Reward eval/Train: -1390.1314365740236/-1356.2555098144285 \n",
      "Epoch: 43800 : Reward eval/Train: -1390.3969430271413/-1356.4948457692392 \n",
      "Epoch: 44000 : Reward eval/Train: -1389.837314876624/-1356.9976369286046 \n",
      "Epoch: 44200 : Reward eval/Train: -1389.9135654612883/-1357.6786868099136 \n",
      "Epoch: 44400 : Reward eval/Train: -1390.0789896170256/-1356.6910395597938 \n",
      "Epoch: 44600 : Reward eval/Train: -1390.0934011436516/-1357.8694183174644 \n",
      "Epoch: 44800 : Reward eval/Train: -1390.062350253755/-1358.3471622865052 \n",
      "Epoch: 45000 : Reward eval/Train: -1390.6188199479082/-1357.6805772831583 \n",
      "Epoch: 45200 : Reward eval/Train: -1390.444708042547/-1357.2801524132976 \n",
      "Epoch: 45400 : Reward eval/Train: -1390.7940731636188/-1356.2552633800033 \n",
      "Epoch: 45600 : Reward eval/Train: -1390.6003168993798/-1356.3219017641034 \n",
      "Epoch: 45800 : Reward eval/Train: -1389.9138410951866/-1356.0647231774071 \n",
      "Epoch: 46000 : Reward eval/Train: -1390.4770265173493/-1355.34395448258 \n",
      "Epoch: 46200 : Reward eval/Train: -1391.2804479685826/-1356.7604704928988 \n",
      "Epoch: 46400 : Reward eval/Train: -1391.2710548194368/-1356.6002328703419 \n",
      "Epoch: 46600 : Reward eval/Train: -1392.1161873259125/-1355.7371906383528 \n",
      "Epoch: 46800 : Reward eval/Train: -1392.4005628986345/-1354.846979821984 \n",
      "Epoch: 47000 : Reward eval/Train: -1392.8340465358083/-1354.7602558266435 \n",
      "Epoch: 47200 : Reward eval/Train: -1392.124374534726/-1355.3226776723914 \n",
      "Epoch: 47400 : Reward eval/Train: -1392.0878192131427/-1355.5161004654208 \n",
      "Epoch: 47600 : Reward eval/Train: -1392.3863722903352/-1355.1927591863064 \n",
      "Epoch: 47800 : Reward eval/Train: -1392.9629005162342/-1355.5799155736802 \n",
      "Epoch: 48000 : Reward eval/Train: -1392.4630444995007/-1354.6192857037945 \n",
      "Epoch: 48200 : Reward eval/Train: -1392.7857874391564/-1356.4784906990603 \n",
      "Epoch: 48400 : Reward eval/Train: -1392.7825172014911/-1357.1421357092945 \n",
      "Epoch: 48600 : Reward eval/Train: -1393.106994458476/-1357.645174880201 \n",
      "Epoch: 48800 : Reward eval/Train: -1393.4917665333194/-1358.792883067504 \n",
      "Epoch: 49000 : Reward eval/Train: -1393.9887727969635/-1359.0583797489203 \n",
      "Epoch: 49200 : Reward eval/Train: -1394.5538337865287/-1358.139450915743 \n",
      "Epoch: 49400 : Reward eval/Train: -1394.9931872491788/-1359.4395077860434 \n",
      "Epoch: 49600 : Reward eval/Train: -1395.6717273433228/-1359.4620544334405 \n",
      "Epoch: 49800 : Reward eval/Train: -1395.0235448182973/-1359.5304445423158 \n",
      "Epoch: 50000 : Reward eval/Train: -1394.9816852662475/-1360.2832115700478 \n",
      "Epoch: 50200 : Reward eval/Train: -1393.996317816897/-1359.5776409841849 \n",
      "Epoch: 50400 : Reward eval/Train: -1392.8772832090017/-1359.65710090999 \n",
      "Epoch: 50600 : Reward eval/Train: -1393.5304856305925/-1360.418752212021 \n",
      "Epoch: 50800 : Reward eval/Train: -1393.9693926996013/-1360.9382943459436 \n",
      "Epoch: 51000 : Reward eval/Train: -1393.2528223829142/-1360.6228914215162 \n",
      "Epoch: 51200 : Reward eval/Train: -1392.280327187831/-1360.1932583079545 \n",
      "Epoch: 51400 : Reward eval/Train: -1392.3241862092984/-1359.9061556118527 \n",
      "Epoch: 51600 : Reward eval/Train: -1393.0782260569392/-1360.2383433561333 \n",
      "Epoch: 51800 : Reward eval/Train: -1393.5983373185613/-1359.1452530228535 \n",
      "Epoch: 52000 : Reward eval/Train: -1394.2146538348948/-1359.43848121282 \n",
      "Epoch: 52200 : Reward eval/Train: -1393.8005643963118/-1360.2412176337966 \n",
      "Epoch: 52400 : Reward eval/Train: -1393.3488463635392/-1358.8570799325041 \n",
      "Epoch: 52600 : Reward eval/Train: -1393.2259134008084/-1359.697524183346 \n",
      "Epoch: 52800 : Reward eval/Train: -1393.4776988444744/-1361.1196567220877 \n",
      "Epoch: 53000 : Reward eval/Train: -1393.830883151938/-1361.1659194963345 \n",
      "Epoch: 53200 : Reward eval/Train: -1394.4806307108781/-1362.2757807117468 \n",
      "Epoch: 53400 : Reward eval/Train: -1394.4490847433028/-1361.4037344043452 \n",
      "Epoch: 53600 : Reward eval/Train: -1394.9500195839332/-1362.3213992909352 \n",
      "Epoch: 53800 : Reward eval/Train: -1395.6769644593664/-1361.9842743607585 \n",
      "Epoch: 54000 : Reward eval/Train: -1395.4738508835828/-1361.4573353552448 \n",
      "Epoch: 54200 : Reward eval/Train: -1394.996567880101/-1361.820002803917 \n",
      "Epoch: 54400 : Reward eval/Train: -1394.7638195239128/-1362.6902788563325 \n",
      "Epoch: 54600 : Reward eval/Train: -1395.2559350427862/-1363.2827465572886 \n",
      "Epoch: 54800 : Reward eval/Train: -1395.0493326124774/-1364.1424073755727 \n",
      "Epoch: 55000 : Reward eval/Train: -1395.807488658112/-1363.898735984773 \n",
      "Epoch: 55200 : Reward eval/Train: -1396.5945817798004/-1364.1595964618043 \n",
      "Epoch: 55400 : Reward eval/Train: -1397.2546049149446/-1363.8474080498306 \n",
      "Epoch: 55600 : Reward eval/Train: -1397.6680574167506/-1364.639658446539 \n",
      "Epoch: 55800 : Reward eval/Train: -1398.3215141035514/-1363.889370505169 \n",
      "Epoch: 56000 : Reward eval/Train: -1398.8294791259084/-1364.6281325752923 \n",
      "Epoch: 56200 : Reward eval/Train: -1398.5224622975525/-1365.3025070566416 \n",
      "Epoch: 56400 : Reward eval/Train: -1398.804509450816/-1365.9945901309752 \n",
      "Epoch: 56600 : Reward eval/Train: -1399.0196679895175/-1366.7383957598115 \n",
      "Epoch: 56800 : Reward eval/Train: -1399.1951463964494/-1367.4491734003664 \n",
      "Epoch: 57000 : Reward eval/Train: -1399.6413657150447/-1366.6053848271304 \n",
      "Epoch: 57200 : Reward eval/Train: -1400.1088133642631/-1366.6651577713683 \n",
      "Epoch: 57400 : Reward eval/Train: -1400.065051921318/-1366.2785187283334 \n",
      "Epoch: 57600 : Reward eval/Train: -1399.9999646531444/-1366.2290663867632 \n",
      "Epoch: 57800 : Reward eval/Train: -1399.2164810291154/-1366.5564758889354 \n",
      "Epoch: 58000 : Reward eval/Train: -1399.7526960652242/-1367.0889087273226 \n",
      "Epoch: 58200 : Reward eval/Train: -1400.1334008235626/-1367.412117415288 \n",
      "Epoch: 58400 : Reward eval/Train: -1400.1336441292963/-1367.2770181435906 \n",
      "Epoch: 58600 : Reward eval/Train: -1400.6330048051307/-1367.0470194777174 \n",
      "Epoch: 58800 : Reward eval/Train: -1400.8881665272513/-1367.43159422692 \n",
      "Epoch: 59000 : Reward eval/Train: -1400.6339891702567/-1367.8561153016221 \n",
      "Epoch: 59200 : Reward eval/Train: -1400.6512490514804/-1368.4755683903825 \n",
      "Epoch: 59400 : Reward eval/Train: -1400.7312268992782/-1368.336747018166 \n",
      "Epoch: 59600 : Reward eval/Train: -1400.3451601415534/-1367.9268475857884 \n",
      "Epoch: 59800 : Reward eval/Train: -1399.7401877771376/-1368.5338519879342 \n",
      "Epoch: 60000 : Reward eval/Train: -1399.3307591662572/-1367.803624343181 \n",
      "Epoch: 60200 : Reward eval/Train: -1399.6602726896065/-1368.237189903846 \n",
      "Epoch: 60400 : Reward eval/Train: -1399.822637131966/-1368.292710476731 \n",
      "Epoch: 60600 : Reward eval/Train: -1400.8809351385328/-1368.7779511214349 \n",
      "Epoch: 60800 : Reward eval/Train: -1400.8155666310865/-1368.7938246297238 \n",
      "Epoch: 61000 : Reward eval/Train: -1400.6385761731672/-1368.1346068650655 \n",
      "Epoch: 61200 : Reward eval/Train: -1400.9679593078433/-1367.7285172980269 \n",
      "Epoch: 61400 : Reward eval/Train: -1401.063025825323/-1367.0197118676274 \n",
      "Epoch: 61600 : Reward eval/Train: -1400.039895393828/-1367.1134572651993 \n",
      "Epoch: 61800 : Reward eval/Train: -1399.6702079805505/-1366.6984517328337 \n",
      "Epoch: 62000 : Reward eval/Train: -1398.9951018581119/-1365.444012114108 \n",
      "Epoch: 62200 : Reward eval/Train: -1399.781134083003/-1365.4185719697516 \n",
      "Epoch: 62400 : Reward eval/Train: -1399.5847161677852/-1365.42778793367 \n",
      "Epoch: 62600 : Reward eval/Train: -1399.2804133785041/-1364.3386118423389 \n",
      "Epoch: 62800 : Reward eval/Train: -1399.455739721463/-1364.448152688942 \n",
      "Epoch: 63000 : Reward eval/Train: -1399.412149897125/-1363.8470002154943 \n",
      "Epoch: 63200 : Reward eval/Train: -1399.5950471573651/-1363.1121393924964 \n",
      "Epoch: 63400 : Reward eval/Train: -1400.679710116308/-1363.1133761132985 \n",
      "Epoch: 63600 : Reward eval/Train: -1400.2262937205082/-1363.1301154156854 \n",
      "Epoch: 63800 : Reward eval/Train: -1399.8281496274226/-1363.379306133459 \n",
      "Epoch: 64000 : Reward eval/Train: -1398.859493972154/-1363.3792115425026 \n",
      "Epoch: 64200 : Reward eval/Train: -1399.9513112542675/-1363.5187867486009 \n",
      "Epoch: 64400 : Reward eval/Train: -1400.2449587474407/-1363.6738061621252 \n",
      "Epoch: 64600 : Reward eval/Train: -1400.539864707166/-1363.7568136230236 \n",
      "Epoch: 64800 : Reward eval/Train: -1400.710911097252/-1363.817710423595 \n",
      "Epoch: 65000 : Reward eval/Train: -1401.6764331073236/-1363.6742534916089 \n",
      "Epoch: 65200 : Reward eval/Train: -1401.6682965073953/-1363.9076077799384 \n",
      "Epoch: 65400 : Reward eval/Train: -1400.733972724135/-1363.9141118651337 \n",
      "Epoch: 65600 : Reward eval/Train: -1400.7576960063566/-1363.4606814612418 \n",
      "Epoch: 65800 : Reward eval/Train: -1400.7929856561216/-1363.965992857145 \n",
      "Epoch: 66000 : Reward eval/Train: -1400.8066497652185/-1363.2602765921258 \n",
      "Epoch: 66200 : Reward eval/Train: -1401.5525971614634/-1362.81594115369 \n",
      "Epoch: 66400 : Reward eval/Train: -1401.6269567113/-1362.43695612484 \n",
      "Epoch: 66600 : Reward eval/Train: -1401.4963529384358/-1362.9238267125454 \n",
      "Epoch: 66800 : Reward eval/Train: -1400.7941085863172/-1363.097588632589 \n",
      "Epoch: 67000 : Reward eval/Train: -1400.913830462937/-1364.051148712009 \n",
      "Epoch: 67200 : Reward eval/Train: -1400.6501171828463/-1363.7950544960343 \n",
      "Epoch: 67400 : Reward eval/Train: -1400.5185882893884/-1363.7130199691514 \n",
      "Epoch: 67600 : Reward eval/Train: -1400.2551536701274/-1364.164005834829 \n",
      "Epoch: 67800 : Reward eval/Train: -1399.813677550506/-1363.7845951801266 \n",
      "Epoch: 68000 : Reward eval/Train: -1399.2765128388407/-1363.7489729054746 \n",
      "Epoch: 68200 : Reward eval/Train: -1398.7587120975343/-1363.8264068584301 \n",
      "Epoch: 68400 : Reward eval/Train: -1398.7632412039611/-1364.0043332651765 \n",
      "Epoch: 68600 : Reward eval/Train: -1398.6671175531012/-1363.5238259955556 \n",
      "Epoch: 68800 : Reward eval/Train: -1398.5146413281389/-1363.7276863165469 \n",
      "Epoch: 69000 : Reward eval/Train: -1398.4284862048385/-1363.5568070626941 \n",
      "Epoch: 69200 : Reward eval/Train: -1398.7400041995058/-1363.1300641050398 \n",
      "Epoch: 69400 : Reward eval/Train: -1398.5756100638366/-1363.0223447213825 \n",
      "Epoch: 69600 : Reward eval/Train: -1398.2490146315959/-1362.774057281564 \n",
      "Epoch: 69800 : Reward eval/Train: -1398.9966074695992/-1362.3919391324757 \n",
      "Epoch: 70000 : Reward eval/Train: -1398.8199195393/-1362.443907974088 \n",
      "Epoch: 70200 : Reward eval/Train: -1398.685295962179/-1362.9384508498513 \n",
      "Epoch: 70400 : Reward eval/Train: -1398.7223509284975/-1362.392747393268 \n",
      "Epoch: 70600 : Reward eval/Train: -1398.9652852007748/-1361.8704142620988 \n",
      "Epoch: 70800 : Reward eval/Train: -1398.4949304723523/-1362.352025033578 \n",
      "Epoch: 71000 : Reward eval/Train: -1398.7370709029635/-1363.3265162256048 \n",
      "Epoch: 71200 : Reward eval/Train: -1398.7732569613345/-1362.9384487704306 \n",
      "Epoch: 71400 : Reward eval/Train: -1399.3234784662206/-1363.693726733665 \n",
      "Epoch: 71600 : Reward eval/Train: -1399.2664474392727/-1364.242986315909 \n",
      "Epoch: 71800 : Reward eval/Train: -1399.0960761481513/-1364.1240134484342 \n",
      "Epoch: 72000 : Reward eval/Train: -1399.2050518936546/-1364.8941235877076 \n",
      "Epoch: 72200 : Reward eval/Train: -1398.9296218531788/-1364.19125804279 \n",
      "Epoch: 72400 : Reward eval/Train: -1398.8210092123506/-1364.1370741211224 \n",
      "Epoch: 72600 : Reward eval/Train: -1399.0190410879839/-1365.1630051594202 \n",
      "Epoch: 72800 : Reward eval/Train: -1399.3005227065569/-1364.9631761265857 \n",
      "Epoch: 73000 : Reward eval/Train: -1400.028649553917/-1365.4551284735232 \n",
      "Epoch: 73200 : Reward eval/Train: -1400.4739191812344/-1365.9193584434042 \n",
      "Epoch: 73400 : Reward eval/Train: -1400.921099738694/-1366.1167429343598 \n",
      "Epoch: 73600 : Reward eval/Train: -1400.8630485750778/-1365.4990371053198 \n",
      "Epoch: 73800 : Reward eval/Train: -1400.7183199398678/-1366.033655600164 \n",
      "Epoch: 74000 : Reward eval/Train: -1400.8591669572913/-1365.4219298379692 \n",
      "Epoch: 74200 : Reward eval/Train: -1401.1362403789412/-1365.8984557498536 \n",
      "Epoch: 74400 : Reward eval/Train: -1401.182228498821/-1366.4079051391432 \n",
      "Epoch: 74600 : Reward eval/Train: -1401.3259554859258/-1365.468491607626 \n",
      "Epoch: 74800 : Reward eval/Train: -1400.9002305455845/-1366.0349559520312 \n",
      "Epoch: 75000 : Reward eval/Train: -1401.3609659698066/-1366.4684946701377 \n",
      "Epoch: 75200 : Reward eval/Train: -1401.3658390184246/-1366.7041917035654 \n",
      "Epoch: 75400 : Reward eval/Train: -1400.6831062434362/-1366.9921022487183 \n",
      "Epoch: 75600 : Reward eval/Train: -1400.9665293677854/-1366.598737306497 \n",
      "Epoch: 75800 : Reward eval/Train: -1401.2099446114255/-1366.6711977826164 \n",
      "Epoch: 76000 : Reward eval/Train: -1400.9796494671027/-1366.9332498236254 \n",
      "Epoch: 76200 : Reward eval/Train: -1400.5329736150272/-1367.1686047866503 \n",
      "Epoch: 76400 : Reward eval/Train: -1400.8257507566864/-1367.6591185688999 \n",
      "Epoch: 76600 : Reward eval/Train: -1400.3148460350965/-1367.5399178447801 \n",
      "Epoch: 76800 : Reward eval/Train: -1400.2317904305228/-1367.376547163978 \n",
      "Epoch: 77000 : Reward eval/Train: -1399.8683160678574/-1367.5575481131364 \n",
      "Epoch: 77200 : Reward eval/Train: -1399.5401507928098/-1367.552489480698 \n",
      "Epoch: 77400 : Reward eval/Train: -1399.4698704987636/-1367.3354922476374 \n",
      "Epoch: 77600 : Reward eval/Train: -1398.8664704376868/-1367.914227534404 \n",
      "Epoch: 77800 : Reward eval/Train: -1399.5551557388303/-1367.919334090662 \n",
      "Epoch: 78000 : Reward eval/Train: -1399.9077415235536/-1367.4973589157876 \n",
      "Epoch: 78200 : Reward eval/Train: -1400.0276114566566/-1368.3003793576156 \n",
      "Epoch: 78400 : Reward eval/Train: -1399.3305705350326/-1368.0318214789786 \n",
      "Epoch: 78600 : Reward eval/Train: -1399.4856130003088/-1368.0995804728204 \n",
      "Epoch: 78800 : Reward eval/Train: -1399.6736259942772/-1367.9230410782106 \n",
      "Epoch: 79000 : Reward eval/Train: -1400.175762180908/-1367.8666780622311 \n",
      "Epoch: 79200 : Reward eval/Train: -1400.225322326764/-1367.8488521231316 \n",
      "Epoch: 79400 : Reward eval/Train: -1399.7201321201705/-1367.5303352052558 \n",
      "Epoch: 79600 : Reward eval/Train: -1399.4963046695161/-1367.0729893048044 \n",
      "Epoch: 79800 : Reward eval/Train: -1399.8268968709497/-1366.728088159088 \n",
      "Epoch: 80000 : Reward eval/Train: -1399.3521412722487/-1366.7494403968292 \n",
      "Epoch: 80200 : Reward eval/Train: -1398.5896763934381/-1366.5713543455986 \n",
      "Epoch: 80400 : Reward eval/Train: -1399.2857355906747/-1366.4630085330512 \n",
      "Epoch: 80600 : Reward eval/Train: -1399.915639852556/-1365.5468249470186 \n",
      "Epoch: 80800 : Reward eval/Train: -1399.39731738293/-1365.2159348920557 \n",
      "Epoch: 81000 : Reward eval/Train: -1398.9642907551472/-1365.0789617865416 \n",
      "Epoch: 81200 : Reward eval/Train: -1399.4139061968528/-1364.645746781224 \n",
      "Epoch: 81400 : Reward eval/Train: -1400.1596537224636/-1363.938159568536 \n",
      "Epoch: 81600 : Reward eval/Train: -1400.222227386976/-1363.4029422140197 \n",
      "Epoch: 81800 : Reward eval/Train: -1400.6087659346902/-1363.0987467267362 \n",
      "Epoch: 82000 : Reward eval/Train: -1400.4620989201046/-1363.196343107542 \n",
      "Epoch: 82200 : Reward eval/Train: -1400.7636546539127/-1362.9618650633715 \n",
      "Epoch: 82400 : Reward eval/Train: -1400.7626007149256/-1362.535058583759 \n",
      "Epoch: 82600 : Reward eval/Train: -1401.1741477524738/-1362.765174521581 \n",
      "Epoch: 82800 : Reward eval/Train: -1401.3948457396612/-1363.1878207314637 \n",
      "Epoch: 83000 : Reward eval/Train: -1401.5192913434594/-1362.9068901945773 \n",
      "Epoch: 83200 : Reward eval/Train: -1401.6392660375495/-1362.9789176504196 \n",
      "Epoch: 83400 : Reward eval/Train: -1401.8219853451217/-1363.1638012203025 \n",
      "Epoch: 83600 : Reward eval/Train: -1401.780192769945/-1363.2585568492398 \n",
      "Epoch: 83800 : Reward eval/Train: -1401.2997353425696/-1363.3206698272372 \n",
      "Epoch: 84000 : Reward eval/Train: -1401.627836989454/-1363.25412013372 \n",
      "Epoch: 84200 : Reward eval/Train: -1401.5105947623756/-1363.996003457523 \n",
      "Epoch: 84400 : Reward eval/Train: -1401.6808934364753/-1364.4922369566373 \n",
      "Epoch: 84600 : Reward eval/Train: -1401.9275185938711/-1364.4648933877652 \n",
      "Epoch: 84800 : Reward eval/Train: -1402.044496384889/-1364.8290113672647 \n",
      "Epoch: 85000 : Reward eval/Train: -1402.4393670018883/-1365.5004353589982 \n",
      "Epoch: 85200 : Reward eval/Train: -1402.4677266153194/-1365.4852463563068 \n",
      "Epoch: 85400 : Reward eval/Train: -1403.0714994226882/-1366.2938646659293 \n",
      "Epoch: 85600 : Reward eval/Train: -1403.2295064732082/-1366.3030850367772 \n",
      "Epoch: 85800 : Reward eval/Train: -1403.1280028002923/-1366.6144134370447 \n",
      "Epoch: 86000 : Reward eval/Train: -1403.6923770986798/-1366.8795960341467 \n",
      "Epoch: 86200 : Reward eval/Train: -1404.011823849261/-1366.8187485231683 \n",
      "Epoch: 86400 : Reward eval/Train: -1404.6285742627838/-1367.5746361471936 \n",
      "Epoch: 86600 : Reward eval/Train: -1405.1025174973215/-1367.9868838866432 \n",
      "Epoch: 86800 : Reward eval/Train: -1405.2327774000362/-1368.698352103575 \n",
      "Epoch: 87000 : Reward eval/Train: -1405.4954846875455/-1368.8349293909055 \n",
      "Epoch: 87200 : Reward eval/Train: -1405.4760470836654/-1369.3262774750058 \n",
      "Epoch: 87400 : Reward eval/Train: -1405.2939452710223/-1369.3552238666296 \n",
      "Epoch: 87600 : Reward eval/Train: -1405.3886542175158/-1369.7266256661217 \n",
      "Epoch: 87800 : Reward eval/Train: -1405.6051172065593/-1369.9923625168135 \n",
      "Epoch: 88000 : Reward eval/Train: -1405.9881077522734/-1370.1821548164974 \n",
      "Epoch: 88200 : Reward eval/Train: -1405.8519903878114/-1370.5059175554595 \n",
      "Epoch: 88400 : Reward eval/Train: -1405.8931140161253/-1370.708174171882 \n",
      "Epoch: 88600 : Reward eval/Train: -1406.0594324284375/-1371.0003825965803 \n",
      "Epoch: 88800 : Reward eval/Train: -1406.5592773494648/-1371.1513635393444 \n",
      "Epoch: 89000 : Reward eval/Train: -1406.6316887306452/-1371.5578607577747 \n",
      "Epoch: 89200 : Reward eval/Train: -1406.7534278705598/-1372.0326365207989 \n",
      "Epoch: 89400 : Reward eval/Train: -1406.5186112374208/-1372.2302483302817 \n",
      "Epoch: 89600 : Reward eval/Train: -1406.5827270139732/-1372.9367179251906 \n",
      "Epoch: 89800 : Reward eval/Train: -1406.5798319306732/-1372.7852529775494 \n",
      "Epoch: 90000 : Reward eval/Train: -1407.1612458093966/-1373.0838120222252 \n",
      "Epoch: 90200 : Reward eval/Train: -1407.0797122189415/-1373.4132594247624 \n",
      "Epoch: 90400 : Reward eval/Train: -1407.2442121338042/-1373.8783075696845 \n",
      "Epoch: 90600 : Reward eval/Train: -1407.9207855291556/-1373.9888376110473 \n",
      "Epoch: 90800 : Reward eval/Train: -1408.532450723747/-1374.5974312033782 \n",
      "Epoch: 91000 : Reward eval/Train: -1408.6476076377842/-1374.8546211129785 \n",
      "Epoch: 91200 : Reward eval/Train: -1408.821806421277/-1375.281930329012 \n",
      "Epoch: 91400 : Reward eval/Train: -1409.1522137571212/-1375.9489612463233 \n",
      "Epoch: 91600 : Reward eval/Train: -1409.2516563021477/-1376.315982687797 \n",
      "Epoch: 91800 : Reward eval/Train: -1409.6266060410837/-1376.5255057290815 \n",
      "Epoch: 92000 : Reward eval/Train: -1410.0358522779854/-1377.044803093344 \n",
      "Epoch: 92200 : Reward eval/Train: -1410.3370789556473/-1377.6576483789331 \n",
      "Epoch: 92400 : Reward eval/Train: -1410.7805535370812/-1378.1647379099466 \n",
      "Epoch: 92600 : Reward eval/Train: -1411.3737766735721/-1378.6761915021907 \n",
      "Epoch: 92800 : Reward eval/Train: -1411.5780787152282/-1379.426141270247 \n",
      "Epoch: 93000 : Reward eval/Train: -1412.046919939128/-1379.6862778305501 \n",
      "Epoch: 93200 : Reward eval/Train: -1412.3803939178576/-1379.9848828485885 \n",
      "Epoch: 93400 : Reward eval/Train: -1412.1638958528454/-1380.057986293506 \n",
      "Epoch: 93600 : Reward eval/Train: -1412.6799517060415/-1380.4544113900101 \n",
      "Epoch: 93800 : Reward eval/Train: -1412.6979640926293/-1381.2668838836794 \n",
      "Epoch: 94000 : Reward eval/Train: -1412.8763034421238/-1381.5372559557104 \n",
      "Epoch: 94200 : Reward eval/Train: -1412.4020935178012/-1382.0151185811267 \n",
      "Epoch: 94400 : Reward eval/Train: -1412.3844254635367/-1382.2849684967473 \n",
      "Epoch: 94600 : Reward eval/Train: -1412.6243500231833/-1382.6435906032768 \n",
      "Epoch: 94800 : Reward eval/Train: -1413.001270153238/-1382.884098574619 \n",
      "Epoch: 95000 : Reward eval/Train: -1413.4196262982903/-1383.2131038690147 \n",
      "Epoch: 95200 : Reward eval/Train: -1413.742085557217/-1383.7817501927 \n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo/\", evaluation_epoch = env._max_episode_steps, training_steps = 700000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.95, discount_max = 0.99,\n",
    "                gae_min = 0.94, gae_max = 0.96,\n",
    "                lr_actor_min = 0.00001, lr_actor_max = 0.001,\n",
    "                lr_critic_min = 0.00001, lr_critic_max = 0.001,\n",
    "                entropy_factor_min = 0.01, entropy_factor_max = 0.1,\n",
    "                dense_min = 32, dense_max = 256,\n",
    "                environment_name=ENV,\n",
    "                reward_scaler = 1, num_layers_act = 2, num_layers_crit =2, num_layers_model = 2, training_epoch = 50, memory_size= env._max_episode_steps),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 50,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "        writer= \"Training/fit_ppo/\"\n",
    "\n",
    "        training_steps = 1000000\n",
    "        entropy_factor = 0.05\n",
    "        discount = 0.99\n",
    "        dense_units_actor = [128]#64, 32]\n",
    "        num_layers_actor = 1\n",
    "        dense_units_critic = [128]#64,32]\n",
    "        num_layers_crit =1\n",
    "        num_layer_m = 1\n",
    "        dense_units_model = [32]\n",
    "\n",
    "        model = run_training(training_steps,  discount, dense_units_actor,  dense_units_critic, dense_units_model,  num_layers_actor, num_layers_crit, num_layer_m, writer, \n",
    "                      environment_name = ENV,reward_scaler = 1, return_agent = True, lr_actor= 0.00001, lr_critic= 0.00001, \n",
    "                      gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=50):\n",
    "    if int(trials.trial_id) == 49:\n",
    "        print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = gym.make(ENV, render_mode = \"rgb_array\")\n",
    "dir = r\"Hyperparam_kt_ppo\"\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=1):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "\n",
    "    training_steps = 1000000\n",
    "    entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "    discount = trials.hyperparameters.values[\"discount\"]\n",
    "    dense_units_actor = [128]#64, 32]\n",
    "    num_layers_actor = 1\n",
    "    dense_units_critic = [128]#64,32]\n",
    "    num_layers_crit =1\n",
    "\n",
    "    model = run_training(training_steps,  discount, dense_units_actor,  dense_units_critic, num_layers_actor, num_layers_crit, writer, \n",
    "                    environment_name = ENV,reward_scaler = 1, return_agent = True, lr_actor= 0.00001, lr_critic= 0.00001, \n",
    "                    gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 50)\n",
    "\n",
    "    \n",
    "env_model = tuner.get_best_models()[0]\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\")\n",
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
