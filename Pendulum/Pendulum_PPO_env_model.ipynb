{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 08:59:07.997293: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Selected port: 58143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 08:59:10.450353: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:58143/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_env_model import * #PPO_Agent_v2 PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"Pendulum-v1\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PendulumEnv<Pendulum-v1>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.630153435842235, 3.9406927377157532)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "# Initialize an empty list to store rewards\n",
    "rewards = []\n",
    "\n",
    "# Run episodes and collect rewards\n",
    "for episode in range(100000):\n",
    "    done = False\n",
    "    while not done:\n",
    "        observation, reward, done, info = env.step(env.action_space.sample())\n",
    "        rewards.append(reward)\n",
    "\n",
    "# Calculate normalization factor (e.g., mean or standard deviation)\n",
    "mean_reward = np.mean(rewards)\n",
    "std_dev_reward = np.std(rewards)\n",
    "mean_reward, std_dev_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7181412, -0.7727178,  4.244884 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1. -1. -8.], [1. 1. 8.], (3,), float32), 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.70830816"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-2.0, 2.0, (1,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"logs_hyper/fit_PPO_env_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 Complete [01h 23m 50s]\n",
      "total_train_reward: -1224.7845143625848\n",
      "\n",
      "Best total_train_reward So Far: -582.1180362811813\n",
      "Total elapsed time: 1d 05h 32m 00s\n",
      "\n",
      "Search: Running Trial #12\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.93              |0.91              |discount\n",
      "0.9               |0.95              |gae_lambda\n",
      "0.00052037        |0.00043567        |lr_actor_critic\n",
      "0.00024119        |0.00022614        |lr_model\n",
      "0.17937           |0.019308          |entropy_coeff\n",
      "92                |98                |dense_units_act_crit_0\n",
      "134               |118               |dense_units_act_crit_1\n",
      "111               |63                |n_dense_layers_model0\n",
      "0.11923           |0.08225           |kl_divergence_target\n",
      "\n",
      "Trial number :  12\n",
      "Epoch: 2000 : Reward eval/Train: -1434.0959992726632/-1445.5199206754312 \n",
      "Epoch: 4000 : Reward eval/Train: -1462.5710875569798/-1536.1319632258242 \n",
      "Epoch: 6000 : Reward eval/Train: -1536.7572123636953/-1539.4081192122342 \n",
      "Epoch: 8000 : Reward eval/Train: -1520.6966916303404/-1559.1069701085437 \n",
      "Epoch: 10000 : Reward eval/Train: -1520.8835931725957/-1526.647446422457 \n",
      "Epoch: 12000 : Reward eval/Train: -1515.5817391510686/-1526.0452755098997 \n",
      "Epoch: 14000 : Reward eval/Train: -1501.2196832960328/-1521.4480113074528 \n",
      "Epoch: 16000 : Reward eval/Train: -1492.7014660593898/-1499.4820924295277 \n",
      "Epoch: 18000 : Reward eval/Train: -1470.5913098349204/-1491.651934224175 \n",
      "Epoch: 20000 : Reward eval/Train: -1465.2463622632486/-1474.2566674257098 \n",
      "Epoch: 22000 : Reward eval/Train: -1456.913219194379/-1465.2285054149788 \n",
      "Epoch: 24000 : Reward eval/Train: -1460.9478786178074/-1455.410420893442 \n",
      "Epoch: 26000 : Reward eval/Train: -1444.9730785813622/-1433.896569258817 \n",
      "Epoch: 28000 : Reward eval/Train: -1443.8949592551146/-1426.0310162316634 \n",
      "Epoch: 30000 : Reward eval/Train: -1433.0674928047274/-1416.707681977148 \n",
      "Epoch: 32000 : Reward eval/Train: -1427.5461025401898/-1409.3561700352825 \n",
      "Epoch: 34000 : Reward eval/Train: -1424.3457874744806/-1402.5703081976808 \n",
      "Epoch: 36000 : Reward eval/Train: -1424.458732742219/-1398.4869536315987 \n",
      "Epoch: 38000 : Reward eval/Train: -1417.1970281018264/-1391.6769997043425 \n",
      "Epoch: 40000 : Reward eval/Train: -1407.0727608326345/-1392.3208954453564 \n",
      "Epoch: 42000 : Reward eval/Train: -1402.9399414906184/-1386.624083588125 \n",
      "Epoch: 44000 : Reward eval/Train: -1409.286648276725/-1380.343977818368 \n",
      "Epoch: 46000 : Reward eval/Train: -1405.7001484602836/-1376.8171713735235 \n",
      "Epoch: 48000 : Reward eval/Train: -1401.285550004669/-1371.2324092026279 \n",
      "Epoch: 50000 : Reward eval/Train: -1402.145489909777/-1369.3489962325275 \n",
      "Epoch: 52000 : Reward eval/Train: -1400.3025328559124/-1365.3680988926494 \n",
      "Epoch: 54000 : Reward eval/Train: -1397.060603520776/-1366.5580392184784 \n",
      "Epoch: 56000 : Reward eval/Train: -1398.4589641367832/-1366.696952064385 \n",
      "Epoch: 58000 : Reward eval/Train: -1394.7631945855862/-1367.3840482613803 \n",
      "Epoch: 60000 : Reward eval/Train: -1395.2099480705783/-1365.0857173473314 \n",
      "Epoch: 62000 : Reward eval/Train: -1392.3364483781745/-1366.2785901947436 \n",
      "Epoch: 64000 : Reward eval/Train: -1385.742597166017/-1365.9271245976608 \n",
      "Epoch: 66000 : Reward eval/Train: -1383.1075615176605/-1367.2024993678374 \n",
      "Epoch: 68000 : Reward eval/Train: -1383.8758439733965/-1365.808284292854 \n",
      "Epoch: 70000 : Reward eval/Train: -1383.575190323255/-1362.0055096726333 \n",
      "Epoch: 72000 : Reward eval/Train: -1383.057536022541/-1360.207386149685 \n",
      "Epoch: 74000 : Reward eval/Train: -1377.4774870166607/-1360.3309310763398 \n",
      "Epoch: 76000 : Reward eval/Train: -1377.282533845496/-1358.0812672639177 \n",
      "Epoch: 78000 : Reward eval/Train: -1375.3362837524205/-1357.9198790895239 \n",
      "Epoch: 80000 : Reward eval/Train: -1372.7444290182432/-1352.7460014341334 \n",
      "Epoch: 82000 : Reward eval/Train: -1370.8650449299612/-1350.2414263387443 \n",
      "Epoch: 84000 : Reward eval/Train: -1367.3069114870086/-1350.7688522342903 \n",
      "Epoch: 86000 : Reward eval/Train: -1366.942272063247/-1348.628334931116 \n",
      "Epoch: 88000 : Reward eval/Train: -1365.1484558189416/-1345.208325025245 \n",
      "Epoch: 90000 : Reward eval/Train: -1362.8761155747457/-1344.7489369067598 \n",
      "Epoch: 92000 : Reward eval/Train: -1361.2843527275772/-1342.6127857360682 \n",
      "Epoch: 94000 : Reward eval/Train: -1361.3456468037218/-1341.8179870602296 \n",
      "Epoch: 96000 : Reward eval/Train: -1358.7592091912516/-1342.3518532868611 \n",
      "Epoch: 98000 : Reward eval/Train: -1355.2146723990727/-1339.778664801484 \n",
      "Epoch: 100000 : Reward eval/Train: -1354.4597272962508/-1337.659891923476 \n",
      "Epoch: 102000 : Reward eval/Train: -1354.08478371549/-1337.7263102911786 \n",
      "Epoch: 104000 : Reward eval/Train: -1350.0821262647526/-1339.7793508563093 \n",
      "Epoch: 106000 : Reward eval/Train: -1349.2930233058/-1338.3377472997481 \n",
      "Epoch: 108000 : Reward eval/Train: -1346.6532192959728/-1337.1584232837251 \n",
      "Epoch: 110000 : Reward eval/Train: -1344.9872629173167/-1335.2075314084743 \n",
      "Epoch: 112000 : Reward eval/Train: -1345.3103552587331/-1335.7887253538927 \n",
      "Epoch: 114000 : Reward eval/Train: -1341.5466920776294/-1333.6910202065894 \n",
      "Epoch: 116000 : Reward eval/Train: -1339.1572132089798/-1332.6147935096903 \n",
      "Epoch: 118000 : Reward eval/Train: -1335.9308146370602/-1331.6159377965982 \n",
      "Epoch: 120000 : Reward eval/Train: -1332.9676352095078/-1331.6350614744676 \n",
      "Epoch: 122000 : Reward eval/Train: -1330.8756263964917/-1329.8941244574028 \n",
      "Epoch: 124000 : Reward eval/Train: -1327.3613365471676/-1327.8571336022617 \n",
      "Epoch: 126000 : Reward eval/Train: -1325.1577132829682/-1328.7991683362495 \n",
      "Epoch: 128000 : Reward eval/Train: -1325.3031406289433/-1328.4873555429415 \n",
      "Epoch: 130000 : Reward eval/Train: -1323.129735435498/-1326.3864303525859 \n",
      "Epoch: 132000 : Reward eval/Train: -1322.6582360132672/-1326.7273657542248 \n",
      "Epoch: 134000 : Reward eval/Train: -1320.0044691693636/-1324.6439324341254 \n",
      "Epoch: 136000 : Reward eval/Train: -1317.636345936878/-1323.2655786773046 \n",
      "Epoch: 138000 : Reward eval/Train: -1315.9893466303736/-1321.878117437749 \n",
      "Epoch: 140000 : Reward eval/Train: -1314.1597753955418/-1321.8290595893372 \n",
      "Epoch: 142000 : Reward eval/Train: -1311.5615392917468/-1321.1014101019448 \n",
      "Epoch: 144000 : Reward eval/Train: -1311.3121954706658/-1322.0071016411734 \n",
      "Epoch: 146000 : Reward eval/Train: -1309.0113473224035/-1319.9243758380076 \n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING: \n",
    "    \n",
    "    # Trial id :08 | Score :-71.19572300865248 --> {'lr_actor_critic': 0.0001443924191964091, 'dense_units_act_crit_0': 138, 'dense_units_act_crit_1': 210, 'kl_divergence_target': 0.05}\n",
    "    # Trial id :02 | Score :-72.41986896912546 --> {'lr_actor_critic': 0.000829844022383942, 'dense_units_act_crit_0': 234, 'dense_units_act_crit_1': 215, 'kl_divergence_target': 0.1}\n",
    "    # Trial id :03 | Score :-74.04910396664376 --> {'lr_actor_critic': 0.000987130392464792, 'dense_units_act_crit_0': 255, 'dense_units_act_crit_1': 188, 'kl_divergence_target': 0.1}\n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo_env_model\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_env_model/\", evaluation_epoch = env._max_episode_steps, training_steps = 1000000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.9, discount_max = 0.99, \n",
    "                #discount= 0.90,#WAS THIS ONE\n",
    "                #gae_factor = 0.95, #WAS THIS ONE\n",
    "                gae_min = 0.85, gae_max = 0.96, \n",
    "                policy_clip =0.2,\n",
    "                lr_actor_crit_min = 0.00001, lr_actor_crit_max = 0.001,\n",
    "                #entropy_factor = 0.05, \n",
    "                entropy_factor_min = 0.001, entropy_factor_max =  0.5,\n",
    "                lr_model_min = 0.00001, lr_model_max =  0.001, kl_divergence_target = None,#0.1,#\n",
    "                #dense_layers = [128,128],\n",
    "                dense_min = 32, dense_max = 150,\n",
    "                environment_name=ENV, num_layers_act = 2, #max_num_layers_act = 2\n",
    "                num_layers_model = 1, #WAS THIS ONE\n",
    "                training_epoch = 1,\n",
    "                memory_size = env._max_episode_steps, \n",
    "                normalize_reward=False, normalize_advantage= False,\n",
    "                scaling_factor_reward = 0.1,\n",
    "                reward_norm_factor = 16,\n",
    "                #memory_size_max= env._max_episode_steps\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 40,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name,\n",
    "            #seed=0\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "\n",
    "        dense_units_act_crit = [128,64]\n",
    "        num_layers_actor_critic = len(dense_units_act_crit)\n",
    "        num_layer_m = None\n",
    "        dense_units_model = []\n",
    "\n",
    "        \n",
    "        model = run_training(training_steps = 500000,\n",
    "                            discount = 0.99, \n",
    "                            dense_units_act_crit = dense_units_act_crit,  \n",
    "                            dense_units_model = dense_units_model,  \n",
    "                            num_layer_a_c = num_layers_actor_critic,  \n",
    "                            num_layer_m = num_layer_m, \n",
    "                            writer = writer, \n",
    "                            environment_name = ENV, \n",
    "                            return_agent = True, \n",
    "                            lr_actor_critic= 0.0001,  \n",
    "                            lr_model = None,\n",
    "                            sucess_criteria_epochs=SUCESS_CRITERIA_EPOCH, \n",
    "                            sucess_criteria_value = SUCESS_CRITERIA_VALUE, \n",
    "                            gae_lambda = 0.95, \n",
    "                            entropy_coeff = 0.05, \n",
    "                            policy_clip = 0.2, training_epoch = 20, \n",
    "                            scaling_factor_reward = 0.1, \n",
    "                            kl_divergence_target = 0.01,\n",
    "                            memory_size =  env._max_episode_steps,\n",
    "                            normalize_reward=False, normalize_advantage= False, use_mlflow = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=5):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for best_hps in tuner.get_best_hyperparameters(num_trials=2):\n",
    "    #env_model = tuner.get_best_models()[0]\n",
    "    print(f\"Best Hyperparameters: {best_hps.__dict__}\")\n",
    "    env_model = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "    env_model = rerun_training(training_steps  = 1500000, \n",
    "                            model = env_model\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = gym.make(ENV)#, render_mode = \"rgb_array\"\n",
    "# dir = r\"Hyperparam_kt_ppo\"\n",
    "# for trials in tuner.oracle.get_best_trials(num_trials=1):\n",
    "#     print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)    \n",
    "#     training_steps = 1000000\n",
    "#     entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "#     discount = trials.hyperparameters.values[\"discount\"]\n",
    "#     gae = trials.hyperparameters.values[\"gae_lambda\"]\n",
    "#     policy_clip = 0.2#trials.hyperparameters.values[\"policy_clip\"]\n",
    "#     scaling_factor_reward= 0.1#trials.hyperparameters.values[\"scaling_factor_reward\"]\n",
    "    \n",
    "    \n",
    "#     lr_actor=  trials.hyperparameters.values[\"lr_actor_critic\"]\n",
    "#     lr_critic=  None#trials.hyperparameters.values[\"lr_critic\"]\n",
    "#     lr_model=  None#trials.hyperparameters.values[\"lr_model\"]\n",
    "    \n",
    "#     try:\n",
    "#         n_dense_layers_actor = trials.hyperparameters.values[\"n_dense_layers_actor\"]\n",
    "#     except : \n",
    "#         n_dense_layers_actor = 1\n",
    "        \n",
    "#     try:\n",
    "#         n_dense_layers_critic = trials.hyperparameters.values[\"n_dense_layers_critic\"]\n",
    "#     except:\n",
    "#         n_dense_layers_critic = 1\n",
    "        \n",
    "\n",
    "#     dense_layers_actor = []\n",
    "#     for i in range(n_dense_layers_actor):\n",
    "#         dense_layers_actor.append(trials.hyperparameters.values['dense_units_act_crit_'+str(i)])\n",
    "\n",
    "#     dense_layers_critic = []\n",
    "#     # for i in range(n_dense_layers_critic):\n",
    "#     #     dense_layers_critic.append(trials.hyperparameters.values['dense_units_crit_'+str(i)])\n",
    "\n",
    "    \n",
    "#     n_dense_layers_model = 1\n",
    "#     dense_layers_model = []\n",
    "#     for i in range(n_dense_layers_model):\n",
    "#         dense_layers_model.append(trials.hyperparameters.values['n_dense_layers_model'+str(i)])\n",
    "\n",
    "#     model = run_training(\n",
    "#         training_steps = training_steps,   \n",
    "#             discount = discount,\n",
    "#             dense_units_act = dense_layers_actor, \n",
    "#             dense_units_crit = dense_layers_critic,\n",
    "#             dense_units_model = dense_layers_model,\n",
    "#             num_layer_a = n_dense_layers_actor,\n",
    "#             num_layer_c = n_dense_layers_critic,\n",
    "#             num_layer_m = n_dense_layers_model,\n",
    "#             writer = writer,  \n",
    "#             save_factor=50000, \n",
    "#             sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, \n",
    "#             sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "#             environment_name=ENV,\n",
    "#             evaluation_epoch = env._max_episode_steps,\n",
    "#             return_agent = True,\n",
    "#             lr_actor= lr_actor, \n",
    "#             lr_critic= lr_critic,\n",
    "#             lr_model= lr_model,\n",
    "#             gae_lambda=gae,\n",
    "#             training_epoch= 200,\n",
    "#             entropy_coeff= entropy_factor,\n",
    "#             policy_clip = policy_clip,\n",
    "#             memory_size= env._max_episode_steps,\n",
    "#             id = int(trials.trial_id),\n",
    "#             scaling_factor_reward = scaling_factor_reward)\n",
    "        \n",
    "#     break\n",
    "\n",
    "\n",
    "def final_evaluation(eval_model, eval_env, n_tries=1, exploration =\"soft\", video_name = \"./PPO_soft_video.mp4\", sucess_criteria_epochs= 100):\n",
    "    rewards_history = []\n",
    "    log_dir = \"Evaluation_process/A3C_\"+str(exploration)+\"/\" +  datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tb_summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    for k in range(n_tries):\n",
    "\n",
    "        if k == 0 : video = VideoRecorder(eval_env, path=video_name)\n",
    "\n",
    "        obs = eval_env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        log_dir_trial = \"Evaluation_process/A3C_trial_\"+str(exploration)+\"/\" + str(k)+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tb_summary_writer_trial = tf.summary.create_file_writer(log_dir_trial)\n",
    "        \n",
    "        epoch = 0\n",
    "        while(True):\n",
    "    \n",
    "            if k == 0 : video.capture_frame()\n",
    "            actions, _, _, _ ,_= eval_model.actor_critic(obs.reshape((1,eval_model.obs_shape[0])))\n",
    "                     \n",
    "            obs, reward, done , info = eval_env.step(actions)\n",
    "            total_reward += reward\n",
    "            #done = truncated or terminated  #terminated, truncated \n",
    "\n",
    "            with tb_summary_writer_trial.as_default():\n",
    "                tf.summary.scalar('Final_eval_rewards', total_reward, step=int(epoch) )\n",
    "                tf.summary.scalar('Final_eval_state', obs[0], step=int(epoch) )\n",
    "                tf.summary.scalar('Final_eval_velocity', obs[1], step=int(epoch) )\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            epoch +=1\n",
    "\n",
    "        rewards_history.append(total_reward)\n",
    "        with tb_summary_writer.as_default():\n",
    "            tf.summary.scalar('Rewards_history', total_reward, step=int(k) )\n",
    "\n",
    "            if len(rewards_history) >sucess_criteria_epochs:\n",
    "                tf.summary.scalar('Average_rewards_history', np.mean(rewards_history[-sucess_criteria_epochs:]), step=int(k) )\n",
    "\n",
    "        eval_env.close()\n",
    "        if k == 0 : video.close()\n",
    "\n",
    "    return np.mean(rewards_history)\n",
    "\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_intrinsic_env_model_\"+exploration_tech+\"_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
