{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 16:57:36.678700: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_with_intrinsic_reward import * #PPO_Agent_v2  PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\BipedalWalker\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/BipedalWalker/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_ppo/' # Linux\n",
    "\n",
    "ENV = \"BipedalWalker-v3\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<BipedalWalker<BipedalWalker-v3>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.06142634e-01,  3.36703509e-01,  5.60719927e-04, -6.13951385e-01,\n",
       "        1.55297136e+00,  1.76780283e-01,  2.55007535e-01, -7.31230080e-01,\n",
       "       -5.47144413e-01, -2.16561854e-02,  5.51912904e-01,  3.63126211e-02,\n",
       "       -5.71376801e-01,  1.64752567e+00,  3.83927703e-01,  8.18679407e-02,\n",
       "       -1.04001856e+00,  1.82784185e-01,  2.03154445e+00,  2.79704285e+00,\n",
       "        9.48893070e-01, -7.42490232e-01,  6.32976472e-01, -1.12894274e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-inf, inf, (24,), float32), (24,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.7474037e-03, -1.3443230e-05,  1.0457467e-03, -1.5999908e-02,\n",
       "        9.1964334e-02, -1.3800329e-03,  8.6026824e-01,  2.4227477e-03,\n",
       "        1.0000000e+00,  3.2371152e-02, -1.3799301e-03,  8.5381544e-01,\n",
       "        9.7596686e-04,  1.0000000e+00,  4.4081402e-01,  4.4582012e-01,\n",
       "        4.6142277e-01,  4.8955020e-01,  5.3410280e-01,  6.0246104e-01,\n",
       "        7.0914888e-01,  8.8593185e-01,  1.0000000e+00,  1.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-1.0, 1.0, (4,), float32),\n",
       " Box(-1.0, 1.0, (4,), float32),\n",
       " (array([-5.37584489e-03, -2.29245634e-04, -4.76412056e-03, -1.83619815e-03,\n",
       "          4.37455863e-01,  8.13077167e-02,  1.12012625e-01, -9.95499268e-02,\n",
       "          1.00000000e+00,  3.40303123e-01,  1.19120017e-01,  1.13705158e-01,\n",
       "         -1.50200009e-01,  1.00000000e+00,  4.49633121e-01,  4.54739392e-01,\n",
       "          4.70654190e-01,  4.99344319e-01,  5.44788301e-01,  6.14514112e-01,\n",
       "          7.23336399e-01,  9.03656125e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "        dtype=float32),\n",
       "  -0.37337466075876635,\n",
       "  False,\n",
       "  {}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space, env.action_space, env.step(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "#sudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED\n",
    "\n",
    "%tensorboard --logdir logs_general/hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_PPO/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2457236999.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    reward_scaler = 1, num_layers_act = 2, num_layers_crit =2, training_epoch = 200, memory_size= memory_size= env._max_episode_steps),\u001b[0m\n\u001b[0m                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo\"\n",
    "    \n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo/\", evaluation_epoch = env._max_episode_steps, training_steps = 700000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.95, discount_max = 0.99,\n",
    "                gae_min = 0.93, gae_max = 0.98,\n",
    "                lr_actor_min = 0.00001, lr_actor_max = 0.001,\n",
    "                lr_critic_min = 0.00001, lr_critic_max = 0.001,\n",
    "                entropy_factor_min = 0.001, entropy_factor_max = 0.1,\n",
    "                dense_min = 32, dense_max = 256,\n",
    "                environment_name=ENV,\n",
    "                reward_scaler = 1, num_layers_act = 2, num_layers_crit =2, training_epoch = 200, memory_size=  env._max_episode_steps),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 50, \n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "        writer= \"logs_hyper/ppo/V0_\"#Training/fit_ppo/\"\n",
    "\n",
    "        training_steps = 1000000\n",
    "        entropy_factor = 0.05\n",
    "        discount = 0.99\n",
    "        dense_units_actor = [256,256]#64, 32]\n",
    "        num_layers_actor = 2\n",
    "        dense_units_critic = [256,256]#64,32]\n",
    "        num_layers_crit =2\n",
    "        num_layer_m = 1\n",
    "        dense_units_model = [64]\n",
    "        model = run_training(training_steps,  discount, dense_units_actor,  dense_units_critic, dense_units_model,  num_layers_actor, num_layers_crit, num_layer_m, writer, \n",
    "                      environment_name = ENV,reward_scaler = 1, return_agent = True, lr_actor= 0.00001, lr_critic= 0.00001, \n",
    "                      gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 50, id = 1, scaling_factor_reward = 0.1)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=10):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = gym.make(ENV)#, render_mode = \"rgb_array\"\n",
    "dir = r\"Hyperparam_kt_ppo\"\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=50):\n",
    "    if int(trials.trial_id) in [6,11,15,16,23,24,26]:\n",
    "        print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)    \n",
    "        training_steps = 2000000\n",
    "        entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "        discount = trials.hyperparameters.values[\"discount\"]\n",
    "        n_dense_layers = trials.hyperparameters.values[\"n_dense_layers\"]\n",
    "\n",
    "\n",
    "        dense_units =  [trials.hyperparameters.values['dense_units_'+str(i)] for i in range(n_dense_layers)]\n",
    "        gae = trials.hyperparameters.values[\"gae_lambda\"]\n",
    "        lr = trials.hyperparameters.values[\"learning_rate\"]\n",
    "        policy_clip = trials.hyperparameters.values[\"policy_clip\"]\n",
    "\n",
    "        model = run_training(training_steps,  discount = discount, dense_units = dense_units, num_layers = n_dense_layers, writer = writer, \n",
    "                        environment_name = ENV,reward_scaler = 1, return_agent = True, lr= lr, sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "                        evaluation_epoch = env._max_episode_steps, gae_lambda = gae, entropy_coeff = entropy_factor, \n",
    "                        policy_clip = policy_clip, training_epoch = 50, memory_size= env._max_episode_steps, id = int(trials.trial_id))\n",
    "        \n",
    "final_rewards = final_evaluation(model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = tuner.get_best_models()[0]\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
