{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 11:24:54.550300: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Selected port: 47195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 11:24:55.942955: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:47195/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_with_intrinsic_reward_V0 import * #PPO_Agent_v2  PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\BipedalWalker\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/BipedalWalker/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_ppo/' # Linux\n",
    "\n",
    "ENV = \"BipedalWalker-v3\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<BipedalWalker<BipedalWalker-v3>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4566026 ,  1.2366161 , -1.3154955 ,  1.3200742 ,  1.9649619 ,\n",
       "       -1.1010382 ,  0.52088755,  0.15153418, -1.2581639 ,  0.44086   ,\n",
       "        0.8954138 , -1.3308717 ,  0.0916764 ,  0.6512324 , -2.016662  ,\n",
       "       -0.5096049 , -0.93953717, -0.05540914, -0.9809887 ,  0.69532627,\n",
       "        1.6089568 ,  2.3321304 , -0.64416   , -1.2997762 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-inf, inf, (24,), float32), (24,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.7475364e-03, -3.9825372e-06,  3.0979741e-04, -1.5999973e-02,\n",
       "        9.2095330e-02, -4.0882971e-04,  8.6019182e-01,  1.7053873e-03,\n",
       "        1.0000000e+00,  3.2502260e-02, -4.0879875e-04,  8.5373855e-01,\n",
       "        2.8912639e-04,  1.0000000e+00,  4.4081402e-01,  4.4582012e-01,\n",
       "        4.6142277e-01,  4.8955020e-01,  5.3410280e-01,  6.0246104e-01,\n",
       "        7.0914888e-01,  8.8593185e-01,  1.0000000e+00,  1.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-1.0, 1.0, (4,), float32),\n",
       " Box(-1.0, 1.0, (4,), float32),\n",
       " (array([-5.3773387e-03, -2.3405088e-04, -4.6622381e-03, -1.8318227e-03,\n",
       "          4.3742281e-01,  8.1091568e-02,  1.1204553e-01, -9.9342249e-02,\n",
       "          1.0000000e+00,  3.4027529e-01,  1.1910359e-01,  1.1373204e-01,\n",
       "         -1.5025806e-01,  1.0000000e+00,  4.4963509e-01,  4.5474136e-01,\n",
       "          4.7065625e-01,  4.9934652e-01,  5.4479069e-01,  6.1451679e-01,\n",
       "          7.2333956e-01,  9.0366006e-01,  1.0000000e+00,  1.0000000e+00],\n",
       "        dtype=float32),\n",
       "  -0.3732849320993683,\n",
       "  False,\n",
       "  {}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space, env.action_space, env.step(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"logs_hyper/fit_PPO/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 00s]\n",
      "\n",
      "Best total_train_reward So Far: None\n",
      "Total elapsed time: 00h 00m 00s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.91              |0.96              |discount\n",
      "0.85              |0.91              |gae_lambda\n",
      "5.774e-05         |4.0193e-05        |lr_actor_critic\n",
      "0.096956          |0.076689          |entropy_coeff\n",
      "2                 |1                 |n_dense_layers_actor\n",
      "200               |99                |dense_units_act_crit_0\n",
      "0.49137           |0.14206           |kl_divergence_target\n",
      "77                |None              |dense_units_act_crit_1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 763, in fit\n",
      "    last_epoch = model.fit()\n",
      "  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 500, in fit\n",
      "    next_obs = self.run_agent(epoch, obs)\n",
      "  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 343, in run_agent\n",
      "    next_obs, reward, done, info = self.train_env.step(action)\n",
      "  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 17, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 13, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/envs/box2d/bipedal_walker.py\", line 460, in step\n",
      "    self.joints[0].motorSpeed = float(SPEED_HIP * np.sign(action[0]))\n",
      "TypeError: only size-1 arrays can be converted to Python scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial number :  3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 763, in fit\n    last_epoch = model.fit()\n  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 500, in fit\n    next_obs = self.run_agent(epoch, obs)\n  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 343, in run_agent\n    next_obs, reward, done, info = self.train_env.step(action)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 17, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 13, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/envs/box2d/bipedal_walker.py\", line 460, in step\n    self.joints[0].motorSpeed = float(SPEED_HIP * np.sign(action[0]))\nTypeError: only size-1 arrays can be converted to Python scalars\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m\n\u001b[1;32m      4\u001b[0m     project_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_tunning_ppo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mBayesianOptimization(\n\u001b[1;32m      8\u001b[0m             MyHyperModel( hyper_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mproject_name,  writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs_hyper/ppo/\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluation_epoch \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39m_max_episode_steps, training_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m700000\u001b[39m,\n\u001b[1;32m      9\u001b[0m                 sucess_criteria_epochs \u001b[38;5;241m=\u001b[39m SUCESS_CRITERIA_EPOCH, sucess_criteria_value\u001b[38;5;241m=\u001b[39m SUCESS_CRITERIA_VALUE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m             project_name\u001b[38;5;241m=\u001b[39mproject_name\n\u001b[1;32m     32\u001b[0m         )\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m : \n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcquiring parameters ....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:338\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/oracle.py:586\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/oracle.py:543\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    547\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 763, in fit\n    last_epoch = model.fit()\n  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 500, in fit\n    next_obs = self.run_agent(epoch, obs)\n  File \"/mnt/ECB0DA94B0DA649C/Artificial_Intelligence/Portfolio/Reinforcement Learning Projects/RL/BipedalWalker/../PPO_Agent_with_intrinsic_reward_V0.py\", line 343, in run_agent\n    next_obs, reward, done, info = self.train_env.step(action)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/wrappers/time_limit.py\", line 17, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py\", line 13, in step\n    observation, reward, done, info = self.env.step(action)\n  File \"/home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/gym/envs/box2d/bipedal_walker.py\", line 460, in step\n    self.joints[0].motorSpeed = float(SPEED_HIP * np.sign(action[0]))\nTypeError: only size-1 arrays can be converted to Python scalars\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo\"\n",
    "    \n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo/\", evaluation_epoch = env._max_episode_steps, training_steps = 700000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.90, discount_max = 0.99, \n",
    "                #discount= 0.99,\n",
    "                #gae_factor = 0.95, \n",
    "                gae_min = 0.85, gae_max = 0.96, \n",
    "                policy_clip =0.2,\n",
    "                lr_actor_crit_min = 0.00001, lr_actor_crit_max = 0.0001,\n",
    "                #entropy_factor = 0.05, \n",
    "                entropy_factor_min = 0.01, entropy_factor_max = 0.1,\n",
    "                lr_model_max = None, kl_divergence_target = None,#0.1,#\n",
    "                #dense_layers = [128,128],\n",
    "                dense_min = 32, dense_max = 256,\n",
    "                environment_name=ENV, max_num_layers_act = 2,\n",
    "                num_layers_model = None, training_epoch = 50,\n",
    "                memory_size = env._max_episode_steps, \n",
    "                normalize_reward=False, normalize_advantage= False,\n",
    "                scaling_factor_reward = 0.1\n",
    "                #memory_size_max= env._max_episode_steps\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 50,\n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "        writer= \"logs_hyper/ppo/V0_\"#Training/fit_ppo/\"\n",
    "\n",
    "        training_steps = 1000000\n",
    "        entropy_factor = 0.05\n",
    "        discount = 0.99\n",
    "        dense_units_actor = [256,256]#64, 32]\n",
    "        num_layers_actor = 2\n",
    "        dense_units_critic = [256,256]#64,32]\n",
    "        num_layers_crit =2\n",
    "        num_layer_m = 1\n",
    "        dense_units_model = [64]\n",
    "        model = run_training(training_steps,  discount, dense_units_actor,  dense_units_critic, dense_units_model,  num_layers_actor, num_layers_crit, num_layer_m, writer, \n",
    "                      environment_name = ENV,reward_scaler = 1, return_agent = True, lr_actor= 0.00001, lr_critic= 0.00001, \n",
    "                      gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 50, id = 1, scaling_factor_reward = 0.1)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=10):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = gym.make(ENV)#, render_mode = \"rgb_array\"\n",
    "dir = r\"Hyperparam_kt_ppo\"\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=50):\n",
    "    if int(trials.trial_id) in [6,11,15,16,23,24,26]:\n",
    "        print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)    \n",
    "        training_steps = 2000000\n",
    "        entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "        discount = trials.hyperparameters.values[\"discount\"]\n",
    "        n_dense_layers = trials.hyperparameters.values[\"n_dense_layers\"]\n",
    "\n",
    "\n",
    "        dense_units =  [trials.hyperparameters.values['dense_units_'+str(i)] for i in range(n_dense_layers)]\n",
    "        gae = trials.hyperparameters.values[\"gae_lambda\"]\n",
    "        lr = trials.hyperparameters.values[\"learning_rate\"]\n",
    "        policy_clip = trials.hyperparameters.values[\"policy_clip\"]\n",
    "\n",
    "        model = run_training(training_steps,  discount = discount, dense_units = dense_units, num_layers = n_dense_layers, writer = writer, \n",
    "                        environment_name = ENV,reward_scaler = 1, return_agent = True, lr= lr, sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "                        evaluation_epoch = env._max_episode_steps, gae_lambda = gae, entropy_coeff = entropy_factor, \n",
    "                        policy_clip = policy_clip, training_epoch = 50, memory_size= env._max_episode_steps, id = int(trials.trial_id))\n",
    "        \n",
    "final_rewards = final_evaluation(model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = tuner.get_best_models()[0]\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
