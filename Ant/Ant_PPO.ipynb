{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 10:07:51.347346: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Selected port: 50951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 10:07:53.885525: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:50951/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_env_model_v2 import * #PPO_Agent_v2  PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"Ant-v2\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<AntEnv<Ant-v2>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.70760421e-01,  1.29156417e+00, -2.52654686e-01, -7.04029356e-01,\n",
       "       -1.92016841e-01, -2.28403631e-01, -9.88240265e-01, -1.98240335e-02,\n",
       "       -1.37666011e-01,  1.52531331e-01,  1.22271855e+00,  2.29286422e-01,\n",
       "        9.96697375e-01, -8.81689141e-01,  2.78486132e-01, -7.39449676e-01,\n",
       "        4.78217328e-01,  9.90931282e-02,  1.11899928e+00, -7.24817972e-01,\n",
       "        3.61573147e-01, -6.03622890e-02,  1.19120139e+00, -3.22074261e-02,\n",
       "       -3.40348033e-01,  3.02807986e-01, -6.98451653e-01, -3.22106459e-01,\n",
       "        2.61762610e-01, -1.86934968e-04, -6.89081161e-02, -1.81791615e+00,\n",
       "       -7.62946035e-02, -1.26693614e+00,  8.67775519e-01, -1.16401866e+00,\n",
       "       -9.42469214e-02, -8.57539928e-01, -8.78361255e-02,  1.31636263e+00,\n",
       "        6.26267054e-01,  1.58642776e+00,  3.85776902e-01,  4.58174737e-01,\n",
       "        1.22104371e+00, -1.04627686e+00,  1.05919816e-01, -2.20947323e-01,\n",
       "       -8.23233424e-01,  1.26086582e+00, -1.26085443e+00,  1.34126558e+00,\n",
       "        7.25036944e-01,  3.31098641e-01, -3.20653942e-01,  8.64267316e-01,\n",
       "       -2.34233520e-01, -1.31766497e+00, -1.94261312e-01,  7.12017921e-01,\n",
       "       -5.12391428e-01,  6.55207950e-01,  8.02304205e-01, -6.01369854e-01,\n",
       "        9.03985973e-01, -7.97034081e-01, -3.03136027e-01,  4.26807735e-01,\n",
       "        3.38589318e-02, -2.06534143e-01,  4.21692342e-01,  2.79357467e-01,\n",
       "       -1.79106407e-01,  2.01231413e-01,  1.24490224e-01, -6.49375121e-01,\n",
       "       -8.49605751e-02, -5.36302895e-01, -7.99334793e-01,  1.40699279e-01,\n",
       "       -8.91808917e-01, -1.22260734e+00,  1.44191868e+00, -3.15194598e-01,\n",
       "       -1.36131844e+00, -1.38867607e+00,  1.74340621e+00,  3.68209021e-01,\n",
       "       -3.54314028e-01, -5.27243041e-02,  6.58599693e-01, -1.15324210e+00,\n",
       "       -2.22328513e+00, -8.23562611e-01, -9.37427590e-01, -8.84962093e-01,\n",
       "        8.80636378e-01, -1.60948811e+00,  5.47346953e-01,  3.65573822e-01,\n",
       "        1.76882525e+00, -1.26012184e+00, -1.07403749e+00, -5.07759582e-01,\n",
       "        7.63216000e-02,  1.48387201e+00,  1.10854672e-01, -1.79725170e-01,\n",
       "       -1.39922780e+00,  1.35319733e-01,  4.12246091e-01])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-inf, inf, (111,), float64), (111,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72214307,  0.99120184, -0.07265375, -0.0723707 , -0.08368284,\n",
       "        0.09251095,  0.05884427, -0.03274768, -0.0426363 ,  0.01780594,\n",
       "        0.0678261 , -0.06571997, -0.05444822, -0.10631354,  0.0744088 ,\n",
       "       -0.07900672,  0.05308476,  0.10282043, -0.04686528,  0.13348025,\n",
       "        0.00748234,  0.04446588,  0.04169424,  0.09687196,  0.02211004,\n",
       "       -0.0240676 , -0.09594564,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-1.0, 1.0, (8,), float32), Box(-1.0, 1.0, (8,), float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space, env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_PPO/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tunning parameters range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_range/Random_search/tuner0.json\n",
      "\n",
      "Search: Running Trial #7\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.96              |0.91              |discount\n",
      "0.89              |0.89              |gae_lambda\n",
      "0.00093988        |0.00041057        |lr_actor_critic\n",
      "0.00080556        |0.00044901        |lr_model\n",
      "0.11734           |0.022262          |entropy_coeff\n",
      "243               |204               |dense_units_act_crit_0\n",
      "226               |246               |dense_units_act_crit_1\n",
      "151               |137               |n_dense_layers_model0\n",
      "0.11013           |0.17381           |kl_divergence_target\n",
      "\n",
      "Trial number :  7\n",
      "Epoch: 2000 : Reward eval/Train: -76.23060105378478/-473.88834315875465 \n",
      "Epoch: 4000 : Reward eval/Train: -137.64224966236281/-309.1032602769617 \n",
      "Epoch: 6000 : Reward eval/Train: -131.9041929835044/-293.1177786393178 \n",
      "Epoch: 8000 : Reward eval/Train: -280.93565610575797/-328.73646725565754 \n",
      "Epoch: 10000 : Reward eval/Train: -385.84240565615875/-274.97555173367573 \n",
      "Epoch: 12000 : Reward eval/Train: -359.2039724908302/-299.3218922334111 \n",
      "Epoch: 14000 : Reward eval/Train: -416.428676837641/-339.499596789449 \n",
      "Epoch: 16000 : Reward eval/Train: -533.351758367146/-384.14302902944377 \n",
      "Epoch: 18000 : Reward eval/Train: -620.8233071356904/-423.71489094755316 \n",
      "Epoch: 20000 : Reward eval/Train: -687.7580737002211/-458.70558542715366 \n",
      "Epoch: 22000 : Reward eval/Train: -751.5463610905408/-492.67868651168453 \n",
      "Epoch: 24000 : Reward eval/Train: -813.1765986758797/-528.1026700357268 \n",
      "Epoch: 26000 : Reward eval/Train: -865.4891327019702/-562.0718123608692 \n",
      "Epoch: 28000 : Reward eval/Train: -911.6107613171615/-593.6942615918568 \n",
      "Epoch: 30000 : Reward eval/Train: -924.9272280052745/-620.2092980904494 \n",
      "Epoch: 32000 : Reward eval/Train: -977.1373502008436/-622.5888698848572 \n",
      "Epoch: 34000 : Reward eval/Train: -1020.88235746046/-630.5092114429656 \n",
      "Epoch: 36000 : Reward eval/Train: -969.4893188672281/-614.573584708549 \n",
      "Epoch: 38000 : Reward eval/Train: -963.2402445661152/-556.7256656704907 \n",
      "Epoch: 40000 : Reward eval/Train: -963.6179929512215/-558.9236118317648 \n",
      "Epoch: 42000 : Reward eval/Train: -921.6462036121054/-444.52584440641414 \n",
      "Epoch: 44000 : Reward eval/Train: -881.9860642132239/-394.1936550189139 \n",
      "Epoch: 46000 : Reward eval/Train: -846.4264642051268/-395.5416428813965 \n",
      "Epoch: 48000 : Reward eval/Train: -814.506551158304/-332.9149548089491 \n",
      "Epoch: 50000 : Reward eval/Train: -787.7418292431807/-292.38156797383135 \n",
      "Epoch: 52000 : Reward eval/Train: -760.8024799788217/-272.91978639838027 \n",
      "Epoch: 54000 : Reward eval/Train: -734.3637776650108/-260.7056193149024 \n",
      "Epoch: 56000 : Reward eval/Train: -710.7198435082034/-239.17020151790572 \n",
      "Epoch: 58000 : Reward eval/Train: -688.1280551059945/-230.21467650232577 \n",
      "Epoch: 60000 : Reward eval/Train: -667.6815286444361/-235.26031550655716 \n",
      "Epoch: 62000 : Reward eval/Train: -647.5486199618509/-232.80947564627235 \n",
      "Epoch: 64000 : Reward eval/Train: -630.1643881575658/-228.32295793032597 \n",
      "Epoch: 66000 : Reward eval/Train: -614.4395340330225/-228.35332287240414 \n",
      "Epoch: 68000 : Reward eval/Train: -597.0999454055014/-227.4081164828076 \n",
      "Epoch: 70000 : Reward eval/Train: -581.6232208983388/-217.48785906951545 \n",
      "Epoch: 72000 : Reward eval/Train: -567.7038822302178/-209.99032353924105 \n",
      "Epoch: 74000 : Reward eval/Train: -555.6589909982915/-201.1844176746136 \n",
      "Epoch: 76000 : Reward eval/Train: -546.2428082955673/-196.6437231016171 \n",
      "Epoch: 78000 : Reward eval/Train: -533.0870265960705/-195.13113497256856 \n",
      "Epoch: 80000 : Reward eval/Train: -556.8573635541119/-195.2112566090063 \n",
      "Epoch: 82000 : Reward eval/Train: -544.2006992873823/-188.5151196915487 \n",
      "Epoch: 84000 : Reward eval/Train: -532.5085945222988/-182.21024719508708 \n",
      "Epoch: 86000 : Reward eval/Train: -522.7582699549624/-186.81431281166104 \n",
      "Epoch: 88000 : Reward eval/Train: -514.2820926607752/-188.31670851831555 \n",
      "Epoch: 90000 : Reward eval/Train: -504.9125563436531/-181.56841616446607 \n",
      "Epoch: 92000 : Reward eval/Train: -495.87806418484394/-175.36512831061214 \n",
      "Epoch: 94000 : Reward eval/Train: -487.35790476826946/-169.02828473321577 \n",
      "Epoch: 96000 : Reward eval/Train: -478.4016755836869/-169.16156894672952 \n",
      "Epoch: 98000 : Reward eval/Train: -471.036204504371/-166.0223354766445 \n",
      "Epoch: 100000 : Reward eval/Train: -462.70125315721634/-162.27874922788618 \n",
      "Epoch: 102000 : Reward eval/Train: -454.32961272434477/-162.4585850098405 \n",
      "Epoch: 104000 : Reward eval/Train: -451.1055789666479/-159.9465262545302 \n",
      "Epoch: 106000 : Reward eval/Train: -444.80449062119277/-163.32504414759848 \n",
      "Epoch: 108000 : Reward eval/Train: -437.83890464013575/-162.9945732154697 \n",
      "Epoch: 110000 : Reward eval/Train: -431.99921503285594/-161.75596835417727 \n",
      "Epoch: 112000 : Reward eval/Train: -425.4016481352729/-161.56804773692303 \n",
      "Epoch: 114000 : Reward eval/Train: -419.2522824075172/-162.1588927065613 \n",
      "Epoch: 116000 : Reward eval/Train: -438.4284304481162/-163.9244966856915 \n",
      "Epoch: 118000 : Reward eval/Train: -431.9735454184438/-164.17583921837274 \n",
      "Epoch: 120000 : Reward eval/Train: -425.30574784785784/-165.16280729197666 \n",
      "Epoch: 122000 : Reward eval/Train: -418.8868066374826/-163.4231303805691 \n",
      "Epoch: 124000 : Reward eval/Train: -413.9846392623098/-162.13830308419503 \n",
      "Epoch: 126000 : Reward eval/Train: -408.18862125019444/-160.4747828398434 \n",
      "Epoch: 128000 : Reward eval/Train: -402.5374196432565/-160.55373478995256 \n",
      "Epoch: 130000 : Reward eval/Train: -397.85668389567473/-159.0467007712939 \n",
      "Epoch: 132000 : Reward eval/Train: -392.6811968648401/-159.060461429578 \n",
      "Epoch: 134000 : Reward eval/Train: -388.13930496467043/-157.72248101746206 \n",
      "Epoch: 136000 : Reward eval/Train: -383.4389891499226/-155.91344424862655 \n",
      "Epoch: 138000 : Reward eval/Train: -379.2927560556762/-158.5777341390228 \n",
      "Epoch: 140000 : Reward eval/Train: -374.53759052722984/-158.36260809484057 \n",
      "Epoch: 142000 : Reward eval/Train: -369.9278173646267/-158.40223895583884 \n",
      "Epoch: 144000 : Reward eval/Train: -366.37894535015016/-158.95476183799067 \n",
      "Epoch: 146000 : Reward eval/Train: -362.491457369669/-158.11166254251333 \n",
      "Epoch: 148000 : Reward eval/Train: -358.6672585564755/-157.98434913446698 \n",
      "Epoch: 150000 : Reward eval/Train: -355.6324968859298/-156.3321660902924 \n",
      "Epoch: 152000 : Reward eval/Train: -351.7582784013227/-156.4904408810876 \n",
      "Epoch: 154000 : Reward eval/Train: -348.4999065500945/-156.64547985246267 \n",
      "Epoch: 156000 : Reward eval/Train: -344.8340289751362/-154.47131656405895 \n",
      "Epoch: 158000 : Reward eval/Train: -342.05071249081766/-153.91963399654853 \n",
      "Epoch: 160000 : Reward eval/Train: -338.26954824098846/-152.41593548123456 \n",
      "Epoch: 162000 : Reward eval/Train: -334.86384788844424/-150.558076619461 \n",
      "Epoch: 164000 : Reward eval/Train: -331.8059847504139/-149.04973778917133 \n",
      "Epoch: 166000 : Reward eval/Train: -346.2051295499504/-151.05415481851236 \n",
      "Epoch: 168000 : Reward eval/Train: -343.75144299414745/-151.13263557500517 \n",
      "Epoch: 170000 : Reward eval/Train: -340.38561907115593/-150.36027841272636 \n",
      "Epoch: 172000 : Reward eval/Train: -354.3284474697061/-149.8589776580753 \n",
      "Epoch: 174000 : Reward eval/Train: -351.17201916451086/-150.20094977991732 \n",
      "Epoch: 176000 : Reward eval/Train: -347.94988921331384/-150.2015030904203 \n",
      "Epoch: 178000 : Reward eval/Train: -344.7800166318777/-150.00501014818587 \n",
      "Epoch: 180000 : Reward eval/Train: -341.83807147460067/-148.40450967045848 \n",
      "Epoch: 182000 : Reward eval/Train: -339.5526937272651/-148.41429581193933 \n",
      "Epoch: 184000 : Reward eval/Train: -344.5095428054181/-148.16351035026418 \n",
      "Epoch: 186000 : Reward eval/Train: -341.5534315185912/-149.0147660149934 \n",
      "Epoch: 188000 : Reward eval/Train: -339.98660043176045/-148.53956307936394 \n",
      "Epoch: 190000 : Reward eval/Train: -337.2407853572689/-147.47148209205676 \n",
      "Epoch: 192000 : Reward eval/Train: -334.42797675660927/-147.55197554131624 \n",
      "Epoch: 194000 : Reward eval/Train: -332.05169819127957/-147.5149869488246 \n",
      "Epoch: 196000 : Reward eval/Train: -329.1808980968918/-146.82613003014478 \n",
      "Epoch: 198000 : Reward eval/Train: -326.25830544180315/-146.37165507150016 \n",
      "Epoch: 200000 : Reward eval/Train: -323.56374570459855/-145.78268720085416 \n",
      "Epoch: 202000 : Reward eval/Train: -321.20251469607325/-144.9243199259399 \n",
      "Epoch: 204000 : Reward eval/Train: -328.9172379686242/-145.16370570713752 \n",
      "Epoch: 206000 : Reward eval/Train: -326.4931350483356/-145.74047092682036 \n",
      "Epoch: 208000 : Reward eval/Train: -323.78383580702433/-144.82471099296592 \n",
      "Epoch: 210000 : Reward eval/Train: -321.45861156777744/-145.5130271060297 \n",
      "Epoch: 212000 : Reward eval/Train: -319.0124897850969/-145.58047073461563 \n",
      "Epoch: 214000 : Reward eval/Train: -316.4155038654561/-146.01774246018192 \n",
      "Epoch: 216000 : Reward eval/Train: -314.0979642101161/-146.88723245598064 \n",
      "Epoch: 218000 : Reward eval/Train: -311.9529183992658/-147.1719035223168 \n",
      "Epoch: 220000 : Reward eval/Train: -310.0910502501532/-148.50941186427528 \n",
      "Epoch: 222000 : Reward eval/Train: -308.1694806548658/-147.54530760064281 \n",
      "Epoch: 224000 : Reward eval/Train: -306.0055398573835/-147.59234388603346 \n",
      "Epoch: 226000 : Reward eval/Train: -303.58219900279573/-148.79873422501055 \n",
      "Epoch: 228000 : Reward eval/Train: -301.68424340698107/-148.7496408523504 \n",
      "Epoch: 230000 : Reward eval/Train: -299.6916159143651/-148.44628142486866 \n",
      "Epoch: 232000 : Reward eval/Train: -297.9946265936115/-149.36915904904205 \n",
      "Epoch: 234000 : Reward eval/Train: -295.89528040238434/-148.7854824343877 \n",
      "Epoch: 236000 : Reward eval/Train: -294.2389341363362/-149.22330537057684 \n",
      "Epoch: 238000 : Reward eval/Train: -292.48859030440155/-149.3684455998766 \n",
      "Epoch: 240000 : Reward eval/Train: -290.582638341149/-148.08370690881472 \n",
      "Epoch: 242000 : Reward eval/Train: -288.6138291354962/-147.09625437693418 \n",
      "Epoch: 244000 : Reward eval/Train: -286.90371292694147/-147.45578623700894 \n",
      "Epoch: 246000 : Reward eval/Train: -285.041811450866/-146.64704615261982 \n",
      "Epoch: 248000 : Reward eval/Train: -283.4021500310015/-147.4865474789794 \n",
      "Epoch: 250000 : Reward eval/Train: -281.58275621402925/-147.75514523977168 \n",
      "Epoch: 252000 : Reward eval/Train: -280.02088427907853/-147.88588880425695 \n",
      "Epoch: 254000 : Reward eval/Train: -278.2358934566567/-147.86182679342912 \n",
      "Epoch: 256000 : Reward eval/Train: -276.32410282711396/-148.31672059664876 \n",
      "Epoch: 258000 : Reward eval/Train: -274.82250903082144/-147.98076559175874 \n",
      "Epoch: 260000 : Reward eval/Train: -273.51155446696424/-148.07340832888127 \n",
      "Epoch: 262000 : Reward eval/Train: -272.2625333233447/-147.3305173548428 \n",
      "Epoch: 264000 : Reward eval/Train: -270.84232524815025/-147.5206524638267 \n",
      "Epoch: 266000 : Reward eval/Train: -269.6380417835253/-147.72016128055958 \n",
      "Epoch: 268000 : Reward eval/Train: -268.0800967387726/-146.5134756492251 \n",
      "Epoch: 270000 : Reward eval/Train: -266.8970307845662/-146.43103080472548 \n",
      "Epoch: 272000 : Reward eval/Train: -265.85797310438176/-147.59540906488567 \n",
      "Epoch: 274000 : Reward eval/Train: -264.1484213820204/-147.68167924505875 \n",
      "Epoch: 276000 : Reward eval/Train: -263.2675279860174/-148.18595306460512 \n",
      "Epoch: 278000 : Reward eval/Train: -261.80024410872534/-147.87703990922176 \n",
      "Epoch: 280000 : Reward eval/Train: -260.37335937719746/-146.96658872476002 \n",
      "Epoch: 282000 : Reward eval/Train: -259.2076458165981/-146.43939273761126 \n",
      "Epoch: 284000 : Reward eval/Train: -257.9337260926964/-146.2736814119909 \n",
      "Epoch: 286000 : Reward eval/Train: -256.64867977684685/-147.05862675695906 \n",
      "Epoch: 288000 : Reward eval/Train: -265.60722818102283/-147.14878106779463 \n",
      "Epoch: 290000 : Reward eval/Train: -264.27300159048735/-146.37756550948578 \n",
      "Epoch: 292000 : Reward eval/Train: -262.71391447335276/-146.492202787834 \n",
      "Epoch: 294000 : Reward eval/Train: -261.22600848928096/-146.91684355520684 \n",
      "Epoch: 296000 : Reward eval/Train: -260.34813087721176/-147.1102470624149 \n",
      "Epoch: 298000 : Reward eval/Train: -259.25939787018956/-147.7660599098313 \n",
      "Epoch: 300000 : Reward eval/Train: -258.2010165305499/-147.84748257894134 \n",
      "Epoch: 302000 : Reward eval/Train: -257.1270390748878/-147.359751654645 \n",
      "Epoch: 304000 : Reward eval/Train: -257.0508505809174/-147.32049911224067 \n",
      "Epoch: 306000 : Reward eval/Train: -255.53737795492833/-147.5601210029195 \n",
      "Epoch: 308000 : Reward eval/Train: -254.50444373007693/-146.96014917769526 \n",
      "Epoch: 310000 : Reward eval/Train: -253.19163624472753/-146.73859361750786 \n",
      "Epoch: 312000 : Reward eval/Train: -251.8301613085842/-146.4728459213806 \n",
      "Epoch: 314000 : Reward eval/Train: -251.16504986187402/-146.44284841239894 \n",
      "Epoch: 316000 : Reward eval/Train: -250.15293958645302/-146.8917891623171 \n",
      "Epoch: 318000 : Reward eval/Train: -249.3135108665056/-147.13673748150512 \n",
      "Epoch: 320000 : Reward eval/Train: -247.93624983288964/-146.5474326022677 \n",
      "Epoch: 322000 : Reward eval/Train: -246.7032810059952/-146.7984883808161 \n",
      "Epoch: 324000 : Reward eval/Train: -245.37836836048828/-146.24639701056816 \n",
      "Epoch: 326000 : Reward eval/Train: -245.0436484258685/-145.690889283834 \n",
      "Epoch: 328000 : Reward eval/Train: -243.79608557121753/-145.5276400411558 \n",
      "Epoch: 330000 : Reward eval/Train: -242.52907598792208/-146.0884934391639 \n",
      "Epoch: 332000 : Reward eval/Train: -242.38332270118764/-145.7623019462893 \n",
      "Epoch: 334000 : Reward eval/Train: -241.25563165831787/-145.34638620545897 \n",
      "Epoch: 336000 : Reward eval/Train: -240.7412800943304/-144.99567722172225 \n",
      "Epoch: 338000 : Reward eval/Train: -239.60559949862966/-144.73080937287014 \n",
      "Epoch: 340000 : Reward eval/Train: -239.27088134614638/-144.8940775537983 \n",
      "Epoch: 342000 : Reward eval/Train: -238.33845506829041/-145.7228649234298 \n",
      "Epoch: 344000 : Reward eval/Train: -237.62599758149858/-145.2101384848264 \n",
      "Epoch: 346000 : Reward eval/Train: -236.62434028786925/-145.8673362200284 \n",
      "Epoch: 348000 : Reward eval/Train: -235.7933447147444/-146.03712736362237 \n",
      "Epoch: 350000 : Reward eval/Train: -235.05306369071153/-145.47787886977946 \n",
      "Epoch: 352000 : Reward eval/Train: -242.70591588110645/-144.9895570286095 \n",
      "Epoch: 354000 : Reward eval/Train: -241.52380558810015/-144.9400030329256 \n",
      "Epoch: 356000 : Reward eval/Train: -240.53476452209546/-144.64875501640472 \n",
      "Epoch: 358000 : Reward eval/Train: -239.47207623985102/-145.02383878269083 \n",
      "Epoch: 360000 : Reward eval/Train: -238.50827447337036/-145.16674513783215 \n",
      "Epoch: 362000 : Reward eval/Train: -237.7256646303233/-144.70559528955843 \n",
      "Epoch: 364000 : Reward eval/Train: -236.99698759626992/-144.95506507580293 \n",
      "Epoch: 366000 : Reward eval/Train: -235.94753735156164/-144.92415304319158 \n",
      "Epoch: 368000 : Reward eval/Train: -234.9778961421965/-144.3386813213885 \n",
      "Epoch: 370000 : Reward eval/Train: -234.74920181157216/-144.5242719192742 \n",
      "Epoch: 372000 : Reward eval/Train: -233.9343747239161/-144.75133386762215 \n",
      "Epoch: 374000 : Reward eval/Train: -233.43443277489703/-144.76471373635445 \n",
      "Epoch: 376000 : Reward eval/Train: -240.23570608569162/-144.61131522748744 \n",
      "Epoch: 378000 : Reward eval/Train: -239.5755702853182/-144.41780324010873 \n",
      "Epoch: 380000 : Reward eval/Train: -238.68521743241706/-143.97359691216548 \n",
      "Epoch: 382000 : Reward eval/Train: -237.88960480471397/-143.97434261944247 \n",
      "Epoch: 384000 : Reward eval/Train: -237.6217404948486/-144.14197383260213 \n",
      "Epoch: 386000 : Reward eval/Train: -236.55922159866225/-144.33430081849923 \n",
      "Epoch: 388000 : Reward eval/Train: -235.5785322136996/-144.34746641039726 \n",
      "Epoch: 390000 : Reward eval/Train: -234.8310797401815/-144.55788666168957 \n",
      "Epoch: 392000 : Reward eval/Train: -234.0688153078749/-144.47114438513142 \n",
      "Epoch: 394000 : Reward eval/Train: -233.22734584545952/-144.36133891595486 \n",
      "Epoch: 396000 : Reward eval/Train: -232.41112938539789/-144.1257778838833 \n",
      "Epoch: 398000 : Reward eval/Train: -231.63217743246045/-144.27105399803503 \n",
      "Epoch: 400000 : Reward eval/Train: -231.06787885182877/-144.87533297511786 \n",
      "Epoch: 402000 : Reward eval/Train: -230.14120512266408/-144.68230825453963 \n",
      "Epoch: 404000 : Reward eval/Train: -229.24146568380317/-144.86981657287336 \n",
      "Epoch: 406000 : Reward eval/Train: -228.42191264436104/-145.08099071413133 \n",
      "Epoch: 408000 : Reward eval/Train: -227.7669180791802/-145.08369931105452 \n",
      "Epoch: 410000 : Reward eval/Train: -227.02772838806456/-144.97848116979765 \n",
      "Epoch: 412000 : Reward eval/Train: -226.11908442079255/-145.0715875021544 \n",
      "Epoch: 414000 : Reward eval/Train: -225.2904785999316/-145.28493638416543 \n",
      "Epoch: 416000 : Reward eval/Train: -224.58233034471274/-145.14197747928472 \n",
      "Epoch: 418000 : Reward eval/Train: -223.75356774774014/-144.81463539350406 \n",
      "Epoch: 420000 : Reward eval/Train: -223.2764805272663/-145.39783773520128 \n",
      "Epoch: 422000 : Reward eval/Train: -222.49255977445424/-145.48165602455506 \n",
      "Epoch: 424000 : Reward eval/Train: -221.94018749276316/-145.08460747481305 \n",
      "Epoch: 426000 : Reward eval/Train: -221.667664397057/-144.64480505521968 \n",
      "Epoch: 428000 : Reward eval/Train: -227.7530037640712/-144.7478707417487 \n",
      "Epoch: 430000 : Reward eval/Train: -227.02776629138248/-144.7975378335207 \n",
      "Epoch: 432000 : Reward eval/Train: -226.1599596193962/-144.95175464244872 \n",
      "Epoch: 434000 : Reward eval/Train: -225.41341283258419/-145.22970649162366 \n",
      "Epoch: 436000 : Reward eval/Train: -224.93035485485385/-145.40175937150093 \n",
      "Epoch: 438000 : Reward eval/Train: -224.15138129832195/-145.46419526030402 \n",
      "Epoch: 440000 : Reward eval/Train: -223.6765553617825/-145.44492312137558 \n",
      "Epoch: 442000 : Reward eval/Train: -222.9399727759088/-144.96803511548248 \n",
      "Epoch: 444000 : Reward eval/Train: -222.28200496668182/-145.0196186782071 \n",
      "Epoch: 446000 : Reward eval/Train: -221.62066284159187/-144.82865427377266 \n",
      "Epoch: 448000 : Reward eval/Train: -220.9945089505301/-144.47287876146387 \n",
      "Epoch: 450000 : Reward eval/Train: -221.12328679593173/-144.19480713871977 \n",
      "Epoch: 452000 : Reward eval/Train: -220.45947699668469/-144.02698081638235 \n",
      "Epoch: 454000 : Reward eval/Train: -219.7447536498521/-144.10746734216093 \n",
      "Epoch: 456000 : Reward eval/Train: -219.14083011386788/-144.28241546043122 \n",
      "Epoch: 458000 : Reward eval/Train: -218.38444124116322/-144.35295648203376 \n",
      "Epoch: 460000 : Reward eval/Train: -217.81897714643935/-144.3647931424741 \n",
      "Epoch: 462000 : Reward eval/Train: -217.3418658791523/-144.4729514622657 \n",
      "Epoch: 464000 : Reward eval/Train: -216.90540378007105/-144.6730651753602 \n",
      "Epoch: 466000 : Reward eval/Train: -216.3969646875228/-144.38680101915457 \n",
      "Epoch: 468000 : Reward eval/Train: -215.69643504730985/-144.64872850140364 \n",
      "Epoch: 470000 : Reward eval/Train: -214.99225161301513/-145.05671435087146 \n",
      "Epoch: 472000 : Reward eval/Train: -214.4163770629595/-145.64443814556518 \n",
      "Epoch: 474000 : Reward eval/Train: -214.34564195185249/-145.1869148020614 \n",
      "Epoch: 476000 : Reward eval/Train: -213.90082845453847/-144.8666059527468 \n",
      "Epoch: 478000 : Reward eval/Train: -213.27282387689064/-144.69840610172807 \n",
      "Epoch: 480000 : Reward eval/Train: -212.76602705386935/-144.65104078453226 \n",
      "Epoch: 482000 : Reward eval/Train: -212.40776705265003/-144.80459872534684 \n",
      "Epoch: 484000 : Reward eval/Train: -211.72620125692328/-144.92196826813563 \n",
      "Epoch: 486000 : Reward eval/Train: -211.30569842759013/-145.11610877811884 \n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING: \n",
    "\n",
    "    dir = r\"Hyperparam_range\"\n",
    "    project_name = \"Random_search\"\n",
    "\n",
    "    tuner = kt.RandomSearch(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_hyperparameter_range/\", evaluation_epoch = env._max_episode_steps, training_steps = 600000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.90, discount_max = 0.99, \n",
    "                gae_min = 0.85, gae_max = 0.96, \n",
    "                policy_clip =0.2,\n",
    "                lr_actor_crit_min = 0.00001, lr_actor_crit_max = 0.001,\n",
    "                #entropy_factor = 0.05,  # WAS THIS\n",
    "                entropy_factor_min = 0.001, entropy_factor_max = 0.5,\n",
    "                lr_model_min = 0.000001, lr_model_max =  0.001, kl_divergence_target = None,\n",
    "                #dense_layers = [42,62],\n",
    "                dense_min = 32, dense_max = 256, \n",
    "                environment_name=ENV, num_layers_act = 2, \n",
    "                num_layers_model = 1,\n",
    "                training_epoch = 1,\n",
    "                memory_size = env._max_episode_steps, \n",
    "                normalize_reward=False, normalize_advantage= True,\n",
    "                scaling_factor_reward = 0.1\n",
    "                #memory_size_max= env._max_episode_steps\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials=10, \n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "\n",
    "    for trials in tuner.oracle.get_best_trials(num_trials=3):\n",
    "        print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying tunning range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPERPARAM_TUNING: \n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo_env_model\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo/\", evaluation_epoch = env._max_episode_steps, training_steps = 1000000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.90, discount_max = 0.94, \n",
    "                #discount= 0.99,\n",
    "                #gae_factor = 0.95, \n",
    "                gae_min = 0.88, gae_max = 0.90, \n",
    "                policy_clip =0.2,\n",
    "                lr_actor_crit_min = 0.0002, lr_actor_crit_max = 0.001,\n",
    "                #entropy_factor = 0.05, \n",
    "                entropy_factor_min = 0.01, entropy_factor_max = 0.03,\n",
    "                lr_model_min = 0.0002, lr_model_max =  0.0007, kl_divergence_target = 0.17,\n",
    "                #dense_layers = [128,128],\n",
    "                dense_min = 128, dense_max = 256,\n",
    "                environment_name=ENV, num_layers_act = 2, #max_num_layers_act = 2\n",
    "                num_layers_model = 1, training_epoch = 1,\n",
    "                memory_size = env._max_episode_steps, \n",
    "                normalize_reward=False, normalize_advantage= True,\n",
    "                scaling_factor_reward = 0.1\n",
    "                #memory_size_max= env._max_episode_steps\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 20,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name,\n",
    "            #seed=0\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "\n",
    "        dense_units_act_crit = [128,64]\n",
    "        num_layers_actor_critic = len(dense_units_act_crit)\n",
    "        num_layer_m = 1\n",
    "        dense_units_model = [12]\n",
    "\n",
    "        model = run_training(training_steps = 500000,\n",
    "                            discount = 0.99, \n",
    "                            dense_units_act_crit = dense_units_act_crit,  \n",
    "                            dense_units_model = dense_units_model,  \n",
    "                            num_layer_a_c = num_layers_actor_critic,  \n",
    "                            num_layer_m = num_layer_m, \n",
    "                            writer = writer, \n",
    "                            environment_name = ENV, \n",
    "                            return_agent = True, \n",
    "                            lr_actor_critic= 0.0001,  \n",
    "                            lr_model = 0.01,\n",
    "                            sucess_criteria_epochs=SUCESS_CRITERIA_EPOCH, \n",
    "                            sucess_criteria_value = SUCESS_CRITERIA_VALUE, \n",
    "                            gae_lambda = 0.95, \n",
    "                            entropy_coeff = 0.05, \n",
    "                            policy_clip = 0.2, training_epoch = 20, \n",
    "                            scaling_factor_reward = 0.1, \n",
    "                            kl_divergence_target = 0.01,\n",
    "                            memory_size =  env._max_episode_steps,\n",
    "                            normalize_reward=False, normalize_advantage= False, use_mlflow = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=10):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = gym.make(ENV)#, render_mode = \"rgb_array\"\n",
    "dir = r\"Hyperparam_kt_ppo\"\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=50):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)    \n",
    "    training_steps = 2000000\n",
    "    entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "    discount = trials.hyperparameters.values[\"discount\"]\n",
    "    gae = trials.hyperparameters.values[\"gae_lambda\"]\n",
    "    policy_clip = trials.hyperparameters.values[\"policy_clip\"]\n",
    "    scaling_factor_reward= trials.hyperparameters.values[\"scaling_factor_reward\"]\n",
    "    \n",
    "    \n",
    "    lr_actor=  trials.hyperparameters.values[\"lr_actor\"]\n",
    "    lr_critic=  trials.hyperparameters.values[\"lr_critic\"]\n",
    "    lr_model=  trials.hyperparameters.values[\"lr_model\"]\n",
    "    \n",
    "    try:\n",
    "        n_dense_layers_actor = trials.hyperparameters.values[\"n_dense_layers_actor\"]\n",
    "    except : \n",
    "        n_dense_layers_actor = 1\n",
    "        \n",
    "    try:\n",
    "        n_dense_layers_critic = trials.hyperparameters.values[\"n_dense_layers_critic\"]\n",
    "    except:\n",
    "        n_dense_layers_critic = 1\n",
    "    \n",
    "\n",
    "    dense_layers_actor = []\n",
    "    for i in range(n_dense_layers_actor):\n",
    "        dense_layers_actor.append(trials.hyperparameters.values['dense_units_act_'+str(i)])\n",
    "\n",
    "    dense_layers_critic = []\n",
    "    for i in range(n_dense_layers_critic):\n",
    "        dense_layers_critic.append(trials.hyperparameters.values['dense_units_crit_'+str(i)])\n",
    "\n",
    "    \n",
    "    n_dense_layers_model = 1\n",
    "    dense_layers_model = []\n",
    "    for i in range(n_dense_layers_model):\n",
    "        dense_layers_model.append(trials.hyperparameters.values['n_dense_layers_model'+str(i)])\n",
    "\n",
    "    model = run_training(\n",
    "        training_steps = training_steps,   \n",
    "            discount = discount,\n",
    "            dense_units_act = dense_layers_actor, \n",
    "            dense_units_crit = dense_layers_critic,\n",
    "            dense_units_model = dense_layers_model,\n",
    "            num_layer_a = n_dense_layers_actor,\n",
    "            num_layer_c = n_dense_layers_critic,\n",
    "            num_layer_m = n_dense_layers_model,\n",
    "            writer = writer,  \n",
    "            save_factor=50000, \n",
    "            sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, \n",
    "            sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "            environment_name=ENV,\n",
    "            reward_scaler = 1, \n",
    "            evaluation_epoch = env._max_episode_steps,\n",
    "            return_agent = True,\n",
    "            lr_actor= lr_actor, \n",
    "            lr_critic= lr_critic,\n",
    "            lr_model= lr_model,\n",
    "            gae_lambda=gae,\n",
    "            training_epoch= 200,\n",
    "            entropy_coeff= entropy_factor,\n",
    "            policy_clip = policy_clip,\n",
    "            memory_size= env._max_episode_steps,\n",
    "            id = int(trials.trial_id),\n",
    "            scaling_factor_reward = scaling_factor_reward)\n",
    "        \n",
    "final_rewards = final_evaluation(model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = tuner.get_best_models()[0]\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
