{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 19:47:47.670647: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Selected port: 37647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 19:47:50.179633: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:37647/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_env_model_v2 import * #PPO_Agent_v2  PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"Ant-v2\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<AntEnv<Ant-v2>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.08792531, -0.29635189, -0.44762209,  0.50426208,  0.59140099,\n",
       "       -0.11150847,  0.03198041, -0.86277575,  0.31130517,  0.4970839 ,\n",
       "        0.84836422, -0.09278206,  1.35520174, -1.2353033 ,  0.7015194 ,\n",
       "        1.46390142,  0.6725944 , -0.49279252, -0.50244316, -0.95914696,\n",
       "        2.00422543,  1.16276881,  0.47958527,  0.07371542, -0.85467976,\n",
       "       -0.81677467, -1.13175461,  0.29238508,  0.5169921 , -0.39676577,\n",
       "       -0.52049487,  1.63481387, -0.34365855, -0.08577749, -1.10731558,\n",
       "        0.17539998, -0.70575745, -0.33006325,  1.88751031, -0.88429754,\n",
       "        0.23244635, -2.53789871, -0.90742981,  0.69837193, -1.68891222,\n",
       "        0.98638459, -0.06750871,  0.47772436,  2.07411392, -0.83500376,\n",
       "        2.12604605, -1.3351905 , -2.09651741, -0.24200982,  1.85534515,\n",
       "       -0.31354958,  0.58229347, -1.81749607, -0.51276247,  0.67659263,\n",
       "        0.11723784, -0.07826109,  0.22468299,  0.12211615,  1.14182122,\n",
       "        0.73504896, -0.10520361, -1.11568192, -0.74955201, -0.12356528,\n",
       "        0.01288504, -0.67323134, -0.6636076 , -1.39159106, -0.67110712,\n",
       "       -0.60583147, -0.51552652, -0.77357007, -0.12302115, -0.61630134,\n",
       "       -0.20966964, -0.41754222, -1.39259429,  0.4521354 ,  0.57867897,\n",
       "       -0.13982673,  0.17855096,  0.47246575,  0.44600055,  0.21195981,\n",
       "        0.04602173, -0.87305937,  0.89965017, -0.74223562,  0.64589923,\n",
       "       -0.25099793,  0.34819451,  0.13474716, -0.09806599,  0.0268381 ,\n",
       "       -0.3072936 ,  0.01474656, -1.10279071,  0.44332481,  0.04990246,\n",
       "       -1.62267153,  1.71977301, -1.82996295, -1.25545579, -0.10658295,\n",
       "       -0.46233099])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-inf, inf, (111,), float64), (111,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.68937016,  0.99335686, -0.06590261, -0.06598153, -0.06741987,\n",
       "       -0.06225599, -0.02283051, -0.00647859,  0.02235212,  0.06714531,\n",
       "       -0.0886551 ,  0.09654004, -0.08430423,  0.00604475, -0.05113899,\n",
       "       -0.0135994 , -0.05960314, -0.0049479 ,  0.01363203,  0.21761968,\n",
       "       -0.03197016,  0.10740787,  0.2243877 ,  0.0228236 , -0.0334111 ,\n",
       "       -0.01556218, -0.0507538 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-1.0, 1.0, (8,), float32), Box(-1.0, 1.0, (8,), float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space, env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_PPO/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tunning parameters range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if HYPERPARAM_TUNING: \n",
    "\n",
    "#     dir = r\"Hyperparam_range\"\n",
    "#     project_name = \"Random_search\"\n",
    "\n",
    "#     tuner = kt.RandomSearch(\n",
    "#             MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_hyperparameter_range/\", evaluation_epoch = env._max_episode_steps, training_steps = 600000,\n",
    "#                 sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "#                 discount_min = 0.90, discount_max = 0.99, \n",
    "#                 gae_min = 0.85, gae_max = 0.96, \n",
    "#                 policy_clip =0.2,\n",
    "#                 lr_actor_crit_min = 0.00001, lr_actor_crit_max = 0.001,\n",
    "#                 #entropy_factor = 0.05,  # WAS THIS\n",
    "#                 entropy_factor_min = 0.001, entropy_factor_max = 0.5,\n",
    "#                 lr_model_min = 0.000001, lr_model_max =  0.001, kl_divergence_target = None,\n",
    "#                 #dense_layers = [42,62],\n",
    "#                 dense_min = 32, dense_max = 256, \n",
    "#                 environment_name=ENV, num_layers_act = 2, \n",
    "#                 num_layers_model = 1,\n",
    "#                 training_epoch = 1,\n",
    "#                 memory_size = env._max_episode_steps, \n",
    "#                 normalize_reward=False, normalize_advantage= True,\n",
    "#                 scaling_factor_reward = 0.1\n",
    "#                 #memory_size_max= env._max_episode_steps\n",
    "#                 ),\n",
    "#             objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "#             max_trials=10, \n",
    "#             directory=dir,\n",
    "#             project_name=project_name\n",
    "#         )\n",
    "#     tuner.search(x=[0], y=[1])\n",
    "\n",
    "#     for trials in tuner.oracle.get_best_trials(num_trials=3):\n",
    "#         print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying tunning range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_kt_ppo/keras_tunning_ppo_env_model_deep_dive/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING: \n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo_env_model_deep_dive\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_deep_dive/\", evaluation_epoch = env._max_episode_steps, training_steps = 800000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.90, discount_max = 0.94, \n",
    "                #discount= 0.9,\n",
    "                gae_factor = 0.89, \n",
    "                #gae_min = 0.89, gae_max = 0.90, \n",
    "                policy_clip =0.2,\n",
    "                lr_actor_crit_min = 0.0002, lr_actor_crit_max = 0.001,\n",
    "                entropy_factor = 0.015, \n",
    "                #entropy_factor_min = 0.01, entropy_factor_max = 0.03,\n",
    "                lr_model_min = 0.0002, lr_model_max =  0.0007, kl_divergence_target = 0.17,\n",
    "                dense_min = 128, dense_max = 256,\n",
    "                environment_name=ENV, num_layers_act = 2, #max_num_layers_act = 2\n",
    "                num_layers_model = 1, training_epoch = 1,\n",
    "                memory_size = env._max_episode_steps, \n",
    "                normalize_reward=False, normalize_advantage= True,\n",
    "                scaling_factor_reward = 0.1\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 10,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name,\n",
    "            #seed=0\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "\n",
    "        dense_units_act_crit = [128,64]\n",
    "        num_layers_actor_critic = len(dense_units_act_crit)\n",
    "        num_layer_m = 1\n",
    "        dense_units_model = [12]\n",
    "\n",
    "        model = run_training(training_steps = 500000,\n",
    "                            discount = 0.99, \n",
    "                            dense_units_act_crit = dense_units_act_crit,  \n",
    "                            dense_units_model = dense_units_model,  \n",
    "                            num_layer_a_c = num_layers_actor_critic,  \n",
    "                            num_layer_m = num_layer_m, \n",
    "                            writer = writer, \n",
    "                            environment_name = ENV, \n",
    "                            return_agent = True, \n",
    "                            lr_actor_critic= 0.0001,  \n",
    "                            lr_model = 0.01,\n",
    "                            sucess_criteria_epochs=SUCESS_CRITERIA_EPOCH, \n",
    "                            sucess_criteria_value = SUCESS_CRITERIA_VALUE, \n",
    "                            gae_lambda = 0.95, \n",
    "                            entropy_coeff = 0.05, \n",
    "                            policy_clip = 0.2, training_epoch = 20, \n",
    "                            scaling_factor_reward = 0.1, \n",
    "                            kl_divergence_target = 0.01,\n",
    "                            memory_size =  env._max_episode_steps,\n",
    "                            normalize_reward=False, normalize_advantage= False, use_mlflow = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial id :04 | Score :433.7122055783411 --> {'discount': 0.9, 'lr_actor_critic': 0.0005134394132368825, 'lr_model': 0.0003268575286591344, 'dense_units_act_crit_0': 220, 'dense_units_act_crit_1': 198, 'n_dense_layers_model0': 219}\n",
      "Trial id :07 | Score :322.04213972786926 --> {'discount': 0.91, 'lr_actor_critic': 0.0003529097867523926, 'lr_model': 0.0006339329907047511, 'dense_units_act_crit_0': 147, 'dense_units_act_crit_1': 205, 'n_dense_layers_model0': 234}\n",
      "Trial id :09 | Score :306.31369484503904 --> {'discount': 0.91, 'lr_actor_critic': 0.00044957085455382803, 'lr_model': 0.0003458313661277884, 'dense_units_act_crit_0': 131, 'dense_units_act_crit_1': 188, 'n_dense_layers_model0': 154}\n",
      "Trial id :02 | Score :250.43993040486293 --> {'discount': 0.92, 'lr_actor_critic': 0.00028712352969576015, 'lr_model': 0.0005988145071461011, 'dense_units_act_crit_0': 212, 'dense_units_act_crit_1': 246, 'n_dense_layers_model0': 250}\n",
      "Trial id :00 | Score :222.71912150699106 --> {'discount': 0.9, 'lr_actor_critic': 0.0008364624564365652, 'lr_model': 0.0003657492649363281, 'dense_units_act_crit_0': 157, 'dense_units_act_crit_1': 146, 'n_dense_layers_model0': 174}\n",
      "Trial id :08 | Score :210.84585616449039 --> {'discount': 0.9, 'lr_actor_critic': 0.00046890597393610693, 'lr_model': 0.00023649873905717638, 'dense_units_act_crit_0': 128, 'dense_units_act_crit_1': 148, 'n_dense_layers_model0': 133}\n",
      "Trial id :01 | Score :146.8488002415129 --> {'discount': 0.9, 'lr_actor_critic': 0.0003509175890643797, 'lr_model': 0.0002221712898706433, 'dense_units_act_crit_0': 174, 'dense_units_act_crit_1': 156, 'n_dense_layers_model0': 150}\n",
      "Trial id :03 | Score :-421.2758967549245 --> {'discount': 0.93, 'lr_actor_critic': 0.0006725769669347701, 'lr_model': 0.0004047464027875262, 'dense_units_act_crit_0': 254, 'dense_units_act_crit_1': 235, 'n_dense_layers_model0': 192}\n",
      "Trial id :06 | Score :-660.2867405266487 --> {'discount': 0.92, 'lr_actor_critic': 0.0006849423082579009, 'lr_model': 0.0005025696750190292, 'dense_units_act_crit_0': 184, 'dense_units_act_crit_1': 155, 'n_dense_layers_model0': 136}\n",
      "Trial id :05 | Score :-976.1747077240841 --> {'discount': 0.92, 'lr_actor_critic': 0.0009197997101377477, 'lr_model': 0.0006858465640128873, 'dense_units_act_crit_0': 233, 'dense_units_act_crit_1': 237, 'n_dense_layers_model0': 181}\n"
     ]
    }
   ],
   "source": [
    "for trials in tuner.oracle.get_best_trials(num_trials=10):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actorcritic_agent = tuner.get_best_models(num_models = 1)[0]\n",
    "# actorcritic_agent.training_steps = 2000000\n",
    "# actorcritic_agent.writer =  \"logs_hyper/ppo_training/\"\n",
    "# actorcritic_agent.train_agent(monitoring_epoch = 10000)\n",
    "# final_rewards = final_evaluation(actorcritic_agent, val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", sucess_criteria_epochs= SUCESS_CRITERIA_EPOCH)\n",
    "# results.append(final_rewards)\n",
    "# print(\"Final mean reward '\", final_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorcritic_agent = tuner.get_best_models(num_models = 2)[1]\n",
    "actorcritic_agent.training_steps = 2000000\n",
    "actorcritic_agent.writer =  \"logs_hyper/ppo_training/\"\n",
    "actorcritic_agent.train_agent(monitoring_epoch = 10000)\n",
    "final_rewards = final_evaluation(actorcritic_agent, val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video_v2.mp4\", sucess_criteria_epochs= SUCESS_CRITERIA_EPOCH)\n",
    "results.append(final_rewards)\n",
    "print(\"Final mean reward '\", final_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actorcritic_agent = tuner.get_best_models(num_models = 1)[0]\n",
    "# actorcritic_agent.training_steps = 2000000\n",
    "# actorcritic_agent.writer =  \"logs_hyper/ppo_training/\"\n",
    "# actorcritic_agent.train_agent(monitoring_epoch = 10000)\n",
    "# final_rewards = final_evaluation(actorcritic_agent, val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", sucess_criteria_epochs= SUCESS_CRITERIA_EPOCH)\n",
    "# results.append(final_rewards)\n",
    "# print(\"Final mean reward '\", final_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial number :  10\n",
      "Epoch: 2000 : Reward eval/Train: -86.84746123751128/-114.90911496950007 \n",
      "Epoch: 4000 : Reward eval/Train: -60.50770143376253/-129.56236768860128 \n",
      "Epoch: 6000 : Reward eval/Train: -70.36720529915142/-155.24393418812687 \n"
     ]
    }
   ],
   "source": [
    "val_env = gym.make(ENV) aaaaaa\n",
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "\n",
    "#results = []\n",
    "for best_hps in tuner.get_best_hyperparameters(num_trials=1):\n",
    "    actorcritic_agent = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "    #actorcritic_agent = rerun_training(training_steps  = 2000000, model = env_model)\n",
    "\n",
    "# for trials in tuner.oracle.get_best_trials(num_trials=1):\n",
    "#     print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "\n",
    "#     discount = trials.hyperparameters.values[\"discount\"]\n",
    "#     gae_lambda = 0.89\n",
    "#     lr_actor_critic = trials.hyperparameters.values['lr_actor_critic']\n",
    "#     lr_model = trials.hyperparameters.values['lr_model']\n",
    "#     policy_clip =0.2\n",
    "#     scaling_factor_reward = 0.1\n",
    "#     entropy_coeff = 0.015\n",
    "\n",
    "#     num_layer_act_crit = 2\n",
    "#     dense_units_act_crit =  [trials.hyperparameters.values['dense_units_act_crit_'+str(i)] for i in range(num_layer_act_crit)]\n",
    "\n",
    "#     num_layer_m = 1\n",
    "#     dense_units_model =  [trials.hyperparameters.values['n_dense_layers_model'+str(i)] for i in range(num_layer_m)]\n",
    "\n",
    "#     actorcritic_agent = PPO(\n",
    "#         training_steps = 2000000, \n",
    "#         sucess_criteria_epochs  = SUCESS_CRITERIA_EPOCH, \n",
    "#         sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "#         discount = discount, \n",
    "#         dense_units_act_crit = dense_units_act_crit,\n",
    "#         dense_units_model = dense_units_model,\n",
    "#         num_layer_act_crit  = num_layer_act_crit, \n",
    "#         num_layer_model = num_layer_m,\n",
    "#         writer =  \"logs_hyper/ppo_training/\",\n",
    "#         evaluation_epoch = env._max_episode_steps,\n",
    "#         environment_name = ENV,\n",
    "#         lr_actor_critic = lr_actor_critic,  lr_model = lr_model, \n",
    "#         gae_lambda=gae_lambda,\n",
    "#         policy_clip = policy_clip,\n",
    "#         training_epoch = 1,\n",
    "#         entropy_coeff = entropy_coeff,\n",
    "#         memory_size = env._max_episode_steps,\n",
    "#         scaling_factor_reward = scaling_factor_reward,\n",
    "#         normalize_reward = False,\n",
    "#         normalize_advantage = True, \n",
    "#         kl_divergence_target = 0.17,\n",
    "#         reward_norm_factor = 0.1,\n",
    "#         force_extreme_exploration = False\n",
    "#         )\n",
    "    actorcritic_agent.training_steps = 2000000\n",
    "    actorcritic_agent.writer =  \"logs_hyper/ppo_training/\"\n",
    "    actorcritic_agent.train_agent(monitoring_epoch = 10000)\n",
    "\n",
    "    final_rewards = final_evaluation(actorcritic_agent, val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", sucess_criteria_epochs= SUCESS_CRITERIA_EPOCH)\n",
    "    results.append(final_rewards)\n",
    "    print(\"Final mean reward '\", final_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = \"keras_tunning_ppo_env_model_deep_dive\"\n",
    "\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#         MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_deep_dive/\", evaluation_epoch = env._max_episode_steps, training_steps = 1000000,\n",
    "#             sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "#             discount_min = 0.90, discount_max = 0.94, \n",
    "#             #discount= 0.99,\n",
    "#             #gae_factor = 0.95, \n",
    "#             gae_min = 0.88, gae_max = 0.90, \n",
    "#             policy_clip =0.2,\n",
    "#             lr_actor_crit_min = 0.0002, lr_actor_crit_max = 0.001,\n",
    "#             #entropy_factor = 0.05, \n",
    "#             entropy_factor_min = 0.01, entropy_factor_max = 0.03,\n",
    "#             lr_model_min = 0.0002, lr_model_max =  0.0007, kl_divergence_target = 0.17,\n",
    "#             #dense_layers = [128,128],\n",
    "#             dense_min = 128, dense_max = 256,\n",
    "#             environment_name=ENV, num_layers_act = 2, #max_num_layers_act = 2\n",
    "#             num_layers_model = 1, training_epoch = 1,\n",
    "#             memory_size = env._max_episode_steps, \n",
    "#             normalize_reward=False, normalize_advantage= True,\n",
    "#             scaling_factor_reward = 0.1\n",
    "#             #memory_size_max= env._max_episode_steps\n",
    "#             ),\n",
    "#         objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "#         max_trials = 10,\n",
    "#         # distribution_strategy= strategy,\n",
    "#         directory=dir,\n",
    "#         project_name=project_name,\n",
    "#         #seed=0\n",
    "#     )\n",
    "# #tuner.search(x=[0], y=[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
