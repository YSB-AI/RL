{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 09:37:34.339558: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Events folder does not exist!\n",
      "Selected port: 36799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 09:37:35.688002: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:36799/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_env_model import * #PPO_Agent_v2  PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\BipedalWalker\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/BipedalWalker/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_ppo/' # Linux\n",
    "\n",
    "ENV = \"Ant-v2\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<AntEnv<Ant-v2>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.45479524e+00, -1.53065249e+00, -7.92889939e-01,  9.28724821e-01,\n",
       "        3.08269260e-01, -1.51761182e-01,  2.40834056e-01,  5.25195614e-02,\n",
       "        3.39225845e-01,  1.89492765e+00, -6.05264071e-02,  8.17127251e-01,\n",
       "        8.73538166e-04, -4.61673279e-02, -7.60940104e-01, -1.22212220e+00,\n",
       "        4.39845633e-01,  4.14043899e-01, -4.94624242e-01, -5.78520129e-01,\n",
       "        5.06189346e-01, -3.92888732e-01,  7.33314554e-01, -1.18867463e+00,\n",
       "       -7.75260339e-01,  1.30919518e+00, -6.61964526e-02, -5.49151380e-01,\n",
       "       -1.02795523e+00,  1.50005397e+00, -1.20637998e+00, -1.32257711e+00,\n",
       "       -1.62348438e+00,  3.39045132e-01, -4.17800993e-02, -7.98703478e-01,\n",
       "       -4.66436581e-01,  1.97794556e-01, -8.03240905e-03,  7.17562210e-01,\n",
       "        1.16710293e+00,  4.21897026e-01, -1.01036666e+00, -6.63143712e-01,\n",
       "        1.60707802e+00,  2.12708619e-02,  7.99419192e-01,  1.51825676e+00,\n",
       "        1.26677827e+00,  1.52242155e-02, -2.61864347e+00,  2.70411297e-03,\n",
       "       -4.12719341e-01,  6.58544320e-01,  7.22376478e-01,  2.80218030e+00,\n",
       "        6.25301068e-02,  2.33821237e+00,  7.77109115e-02, -1.54484494e+00,\n",
       "       -3.14388844e-01, -7.88672558e-01, -9.77512219e-01, -1.00936661e+00,\n",
       "        6.37922119e-01,  1.06260813e+00,  1.88375926e-01,  2.57122733e-01,\n",
       "        3.21724982e-01,  1.05843913e+00,  1.82142601e+00, -2.92494010e-01,\n",
       "        3.08659202e-01,  6.17931736e-01, -3.70970552e-03, -5.19424126e-01,\n",
       "       -2.00707856e-01,  4.17140712e-01, -6.86464152e-01, -3.09305143e-01,\n",
       "       -2.07669712e-01,  7.69315422e-01, -1.13782510e+00,  8.35294584e-02,\n",
       "       -9.53536753e-01, -1.25585771e+00, -1.36760123e+00,  1.00641260e-01,\n",
       "       -3.99330756e-01,  2.00880029e+00, -9.89540461e-01,  1.86812555e-01,\n",
       "        3.07099470e-01, -5.42647157e-01, -7.24848553e-01,  7.30757511e-01,\n",
       "        1.12092257e+00,  6.72148371e-01,  2.00844978e-02,  7.39681343e-01,\n",
       "        2.75394986e-01,  1.42394322e+00, -1.75950769e+00, -1.11880978e+00,\n",
       "        1.20682466e+00,  1.85990723e+00,  1.84083853e+00, -3.05160761e-01,\n",
       "       -9.71990788e-01,  3.72146806e-01,  1.44085650e+00])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-inf, inf, (111,), float64), (111,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.68393216e-01,  9.94634842e-01,  8.73653838e-02, -4.21350898e-02,\n",
       "        3.59646405e-02,  5.43209713e-02, -3.34332111e-02,  7.56738422e-03,\n",
       "        7.44652437e-02,  7.24762481e-02,  4.23837150e-02,  5.06712135e-02,\n",
       "        5.67820186e-02,  4.42276470e-02,  1.10765746e-02,  8.18355079e-02,\n",
       "        7.27670873e-02,  7.28691249e-02,  9.63159593e-03,  1.45693951e-01,\n",
       "       -1.41471270e-01,  5.17516963e-02, -4.82238532e-02,  6.35736884e-04,\n",
       "        3.56541311e-02, -1.35801483e-01,  3.18499097e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-1.0, 1.0, (8,), float32), Box(-1.0, 1.0, (8,), float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space, env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_PPO_intrinsic_reward/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial number :  0\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.99              |0.99              |discount\n",
      "0.92              |0.92              |gae_lambda\n",
      "0.00031023        |0.00031023        |lr_actor_critic\n",
      "0.00072479        |0.00072479        |lr_model\n",
      "0.33147           |0.33147           |entropy_coeff\n",
      "81                |81                |dense_units_act_crit_0\n",
      "112               |112               |dense_units_act_crit_1\n",
      "1                 |1                 |n_dense_layers_model\n",
      "62                |62                |n_dense_layers_model0\n",
      "\n",
      "Trial number :  1\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING: \n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo_env_model\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_env_model/\", evaluation_epoch = env._max_episode_steps, training_steps = 1000000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.95, discount_max = 0.99, \n",
    "                #discount= 0.99,\n",
    "                #gae_factor = 0.95, \n",
    "                gae_min = 0.85, gae_max = 0.96, \n",
    "                policy_clip =0.2,\n",
    "                lr_actor_crit_min = 0.00001, lr_actor_crit_max = 0.001,\n",
    "                #entropy_factor = 0.05, \n",
    "                entropy_factor_min = 0.001, entropy_factor_max = 0.5,\n",
    "                lr_model_min = 0.00001, lr_model_max =  0.001, kl_divergence_target = 0.1,\n",
    "                #dense_layers = [128,128],\n",
    "                dense_min = 32, dense_max = 150,\n",
    "                environment_name=ENV, num_layers_act = 2, #max_num_layers_act = 2\n",
    "                num_layers_model = 2, training_epoch = 50,\n",
    "                memory_size = env._max_episode_steps, \n",
    "                normalize_reward=False, normalize_advantage= False,\n",
    "                scaling_factor_reward = 0.1\n",
    "                #memory_size_max= env._max_episode_steps\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 30,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name,\n",
    "            #seed=0\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "\n",
    "        dense_units_act_crit = [128,64]\n",
    "        num_layers_actor_critic = len(dense_units_act_crit)\n",
    "        num_layer_m = 1\n",
    "        dense_units_model = [12]\n",
    "\n",
    "        model = run_training(training_steps = 500000,\n",
    "                            discount = 0.99, \n",
    "                            dense_units_act_crit = dense_units_act_crit,  \n",
    "                            dense_units_model = dense_units_model,  \n",
    "                            num_layer_a_c = num_layers_actor_critic,  \n",
    "                            num_layer_m = num_layer_m, \n",
    "                            writer = writer, \n",
    "                            environment_name = ENV, \n",
    "                            return_agent = True, \n",
    "                            lr_actor_critic= 0.0001,  \n",
    "                            lr_model = 0.01,\n",
    "                            sucess_criteria_epochs=SUCESS_CRITERIA_EPOCH, \n",
    "                            sucess_criteria_value = SUCESS_CRITERIA_VALUE, \n",
    "                            gae_lambda = 0.95, \n",
    "                            entropy_coeff = 0.05, \n",
    "                            policy_clip = 0.2, training_epoch = 20, \n",
    "                            scaling_factor_reward = 0.1, \n",
    "                            kl_divergence_target = 0.01,\n",
    "                            memory_size =  env._max_episode_steps,\n",
    "                            normalize_reward=False, normalize_advantage= False, use_mlflow = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=10):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = gym.make(ENV)#, render_mode = \"rgb_array\"\n",
    "dir = r\"Hyperparam_kt_ppo\"\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=50):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)    \n",
    "    training_steps = 2000000\n",
    "    entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "    discount = trials.hyperparameters.values[\"discount\"]\n",
    "    gae = trials.hyperparameters.values[\"gae_lambda\"]\n",
    "    policy_clip = trials.hyperparameters.values[\"policy_clip\"]\n",
    "    scaling_factor_reward= trials.hyperparameters.values[\"scaling_factor_reward\"]\n",
    "    \n",
    "    \n",
    "    lr_actor=  trials.hyperparameters.values[\"lr_actor\"]\n",
    "    lr_critic=  trials.hyperparameters.values[\"lr_critic\"]\n",
    "    lr_model=  trials.hyperparameters.values[\"lr_model\"]\n",
    "    \n",
    "    try:\n",
    "        n_dense_layers_actor = trials.hyperparameters.values[\"n_dense_layers_actor\"]\n",
    "    except : \n",
    "        n_dense_layers_actor = 1\n",
    "        \n",
    "    try:\n",
    "        n_dense_layers_critic = trials.hyperparameters.values[\"n_dense_layers_critic\"]\n",
    "    except:\n",
    "        n_dense_layers_critic = 1\n",
    "    \n",
    "\n",
    "    dense_layers_actor = []\n",
    "    for i in range(n_dense_layers_actor):\n",
    "        dense_layers_actor.append(trials.hyperparameters.values['dense_units_act_'+str(i)])\n",
    "\n",
    "    dense_layers_critic = []\n",
    "    for i in range(n_dense_layers_critic):\n",
    "        dense_layers_critic.append(trials.hyperparameters.values['dense_units_crit_'+str(i)])\n",
    "\n",
    "    \n",
    "    n_dense_layers_model = 1\n",
    "    dense_layers_model = []\n",
    "    for i in range(n_dense_layers_model):\n",
    "        dense_layers_model.append(trials.hyperparameters.values['n_dense_layers_model'+str(i)])\n",
    "\n",
    "    model = run_training(\n",
    "        training_steps = training_steps,   \n",
    "            discount = discount,\n",
    "            dense_units_act = dense_layers_actor, \n",
    "            dense_units_crit = dense_layers_critic,\n",
    "            dense_units_model = dense_layers_model,\n",
    "            num_layer_a = n_dense_layers_actor,\n",
    "            num_layer_c = n_dense_layers_critic,\n",
    "            num_layer_m = n_dense_layers_model,\n",
    "            writer = writer,  \n",
    "            save_factor=50000, \n",
    "            sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, \n",
    "            sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "            environment_name=ENV,\n",
    "            reward_scaler = 1, \n",
    "            evaluation_epoch = env._max_episode_steps,\n",
    "            return_agent = True,\n",
    "            lr_actor= lr_actor, \n",
    "            lr_critic= lr_critic,\n",
    "            lr_model= lr_model,\n",
    "            gae_lambda=gae,\n",
    "            training_epoch= 200,\n",
    "            entropy_coeff= entropy_factor,\n",
    "            policy_clip = policy_clip,\n",
    "            memory_size= env._max_episode_steps,\n",
    "            id = int(trials.trial_id),\n",
    "            scaling_factor_reward = scaling_factor_reward)\n",
    "        \n",
    "final_rewards = final_evaluation(model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = tuner.get_best_models()[0]\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
