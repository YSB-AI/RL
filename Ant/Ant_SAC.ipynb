{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 08:59:15.106472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 08:59:15.186401: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-08 08:59:15.215828: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-08 08:59:15.696415: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/n/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/n/.mujoco/mujoco210/bin\n",
      "2024-05-08 08:59:15.697011: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/n/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/n/.mujoco/mujoco210/bin\n",
      "2024-05-08 08:59:15.697015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-05-08 08:59:16.881383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:16.954203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:16.954413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:16.956318: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 08:59:16.958375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:16.958517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:16.958564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:17.026070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:17.026422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:17.026537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:17.026649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20861 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPU\n",
      "Selected port: 54219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 08:59:17.562069: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 08:59:17.707505: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-08 08:59:17.742757: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-08 08:59:18.130064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/cv2/../../lib64::/home/n/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/n/.mujoco/mujoco210/bin\n",
      "2024-05-08 08:59:18.131052: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/n/anaconda3/envs/RL_env/lib/python3.10/site-packages/cv2/../../lib64::/home/n/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/n/.mujoco/mujoco210/bin\n",
      "2024-05-08 08:59:18.131057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-05-08 08:59:18.689455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:18.764099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:18.764305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:54219/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from SAC_Agent import *\n",
    "from ENV_DETAILS import *\n",
    "import os\n",
    "#os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-12.1/lib64:/usr/local/cuda/lib64:/usr/local/cuda-12.1/lib64:/usr/local/cuda/lib64:/usr/local/cuda-12.1/lib64:/usr/local/cuda/lib64\"\n",
    "\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9298473450749419981\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 21874802688\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11434643560400663326\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 08:59:22.049974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:22.050153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:22.050250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:22.050395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:22.050495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-08 08:59:22.050565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 20861 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\BipedalWalker\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/BipedalWalker/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_sac/' # Linux\n",
    "\n",
    "ENV = \"Ant-v2\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<AntEnv<Ant-v2>>>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_steps = env._max_episode_steps\n",
    "max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.01183277, -0.84935299,  0.17322328,  1.36387071, -2.21440172,\n",
       "        0.23142667,  0.22460939, -1.76664972,  0.32785036,  0.54733435,\n",
       "       -1.6457494 , -2.53769359,  0.14480469, -1.40492632,  0.47469273,\n",
       "        0.11935798,  0.67997021,  1.29335923, -0.52284461, -0.06411533,\n",
       "       -1.83497624, -0.57746062,  0.04302837, -0.65248822,  1.38106218,\n",
       "        0.41944777, -1.56732062, -0.19780135, -0.63891108, -0.55325713,\n",
       "        0.52308925,  0.09038974, -0.77988933, -1.15240668, -0.28785562,\n",
       "       -0.06614999,  0.20195633,  1.09053719,  0.57199304, -0.67643305,\n",
       "        1.2760276 ,  0.20713408, -0.417529  ,  0.21499038,  0.22786539,\n",
       "       -0.68340365, -1.02931388,  0.74939501,  0.19893234, -0.42292584,\n",
       "        0.16049534, -1.42284092,  0.15372774,  0.15539783, -1.47741845,\n",
       "       -0.64181205, -0.59844535, -1.59828583, -0.19730624,  0.50949868,\n",
       "       -0.25339652, -0.07126207,  0.03455291,  2.58881126, -0.88802036,\n",
       "       -0.40773808,  1.62464458,  1.0820146 ,  0.65765172,  0.76669542,\n",
       "        0.30996698, -0.06707745, -0.20820857,  0.7493392 , -1.72663509,\n",
       "       -0.24505533,  0.6758105 ,  0.12334065, -0.77898833, -0.24564942,\n",
       "       -0.53946806, -0.34631664, -0.77556498,  0.75598757, -0.59279577,\n",
       "        0.40980914, -0.1531947 , -1.27887313, -0.244787  , -1.71153228,\n",
       "        0.46947835, -0.63657592,  0.69611582,  0.54637316, -1.94613784,\n",
       "        1.42199801,  1.13251031,  0.05682012,  0.31970953,  0.12206781,\n",
       "       -1.1582338 , -0.84037836,  1.0326373 , -0.04875061,  2.79818609,\n",
       "       -0.47755631, -0.49399017, -0.59958215,  0.22440646, -2.29456294,\n",
       "       -0.24631795])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (111,), float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6993234291330553"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.04596584,  0.10526047,  0.9153065 , -0.9851934 , -0.05714063,\n",
       "         0.7082809 ,  0.9613625 ,  0.30562714], dtype=float32),\n",
       " (8,),\n",
       " (1, 8))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = env.action_space.sample()\n",
    "a, a.shape, a.reshape((1, len(a))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.14907919e-01,  9.92275071e-01,  7.29042082e-02,  7.90443754e-02,\n",
       "        -6.18639404e-02,  1.47326621e-01,  3.52684326e-01, -1.12545121e-01,\n",
       "        -3.87291763e-01,  2.66432539e-01, -4.12086189e-01, -8.11570194e-03,\n",
       "         4.14331410e-01,  8.22603467e-02, -2.14929258e-01, -5.86654329e-03,\n",
       "        -3.21103398e-01, -5.66047218e-01, -1.99667376e+00,  6.62403145e+00,\n",
       "         7.17804625e+00, -4.43479078e-01, -6.24939860e+00,  7.07250183e+00,\n",
       "        -7.20654656e+00,  4.14938448e-01,  1.15117489e+01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]),\n",
       " -0.565444375607121,\n",
       " False,\n",
       " {'reward_forward': 0.10662433948839908,\n",
       "  'reward_ctrl': -1.67206871509552,\n",
       "  'reward_contact': -0.0,\n",
       "  'reward_survive': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6013701 , -0.38249943, -0.2612928 ,  0.57558364,  0.14169   ,\n",
       "        0.34746322,  0.2178693 ,  0.723077  ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"logs_hyper/fit_sac/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_kt_sac/keras_tunning_soft/tuner0.json\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.00087595        |0.00029228        |lr_actor\n",
      "0.00067272        |0.00050344        |lr_critic_1\n",
      "0.00046265        |0.00046415        |lr_alpha\n",
      "0.029099          |0.045398          |tau\n",
      "287               |139               |dense_units_act_0\n",
      "164               |336               |dense_units_act_1\n",
      "254               |387               |dense_units_crit_0\n",
      "344               |256               |dense_units_crit_1\n",
      "\n",
      "Trial number :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 08:59:25.192019: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000 : Reward eval/Train: -2983.706137402726/-20.641672813722405 \n",
      "Epoch: 8000 : Reward eval/Train: -2991.6895426799592/-34.74351005401232 \n",
      "Epoch: 12000 : Reward eval/Train: -2992.2227530705677/-112.5667928137562 \n",
      "Epoch: 16000 : Reward eval/Train: -2285.5363528103303/-244.1993849467787 \n",
      "Epoch: 20000 : Reward eval/Train: -1836.6106896012138/-359.991413123291 \n",
      "Epoch: 24000 : Reward eval/Train: -1948.2230742840782/-419.0487067272886 \n",
      "Epoch: 28000 : Reward eval/Train: -2005.661838837008/-472.51349705114427 \n",
      "Epoch: 32000 : Reward eval/Train: -1887.3539732126596/-519.1607772206557 \n",
      "Epoch: 36000 : Reward eval/Train: -1680.409608345798/-541.3992716713119 \n",
      "Epoch: 40000 : Reward eval/Train: -1516.1032470299929/-550.8048475279087 \n",
      "Epoch: 44000 : Reward eval/Train: -1381.94935806246/-514.5781323485677 \n",
      "Epoch: 48000 : Reward eval/Train: -1275.3478248838974/-447.7995644975602 \n",
      "Epoch: 52000 : Reward eval/Train: -1179.0818328660102/-447.0483133979889 \n",
      "Epoch: 56000 : Reward eval/Train: -1100.3654708690099/-443.3464950287718 \n",
      "Epoch: 60000 : Reward eval/Train: -1029.3400000558856/-432.4269358604009 \n",
      "Epoch: 64000 : Reward eval/Train: -1065.0434037532168/-431.3829097053007 \n",
      "Epoch: 68000 : Reward eval/Train: -1004.2682788189135/-428.12764397227454 \n",
      "Epoch: 72000 : Reward eval/Train: -1023.2819181934075/-420.80824280292643 \n",
      "Epoch: 76000 : Reward eval/Train: -972.4648435698272/-395.2128168550948 \n",
      "Epoch: 80000 : Reward eval/Train: -986.0471561484159/-389.8446975611131 \n",
      "Epoch: 84000 : Reward eval/Train: -941.228006183603/-362.16162947201224 \n",
      "Epoch: 88000 : Reward eval/Train: -902.7127653130494/-361.81107099276426 \n",
      "Epoch: 92000 : Reward eval/Train: -900.4183007322148/-360.22056916901766 \n",
      "Epoch: 96000 : Reward eval/Train: -868.2439505035841/-364.47773668940465 \n",
      "Epoch: 100000 : Reward eval/Train: -837.0917938621449/-361.39190544519636 \n",
      "Epoch: 104000 : Reward eval/Train: -860.5703979650972/-360.48830058860165 \n",
      "Epoch: 108000 : Reward eval/Train: -830.1886992313292/-356.40094956830967 \n",
      "Epoch: 112000 : Reward eval/Train: -801.2344312125833/-360.22058385075036 \n",
      "Epoch: 116000 : Reward eval/Train: -777.1361044680001/-360.5458520669186 \n",
      "Epoch: 120000 : Reward eval/Train: -752.0866727411325/-355.82491537490546 \n",
      "Epoch: 124000 : Reward eval/Train: -732.2637962736916/-358.01490683794594 \n",
      "Epoch: 128000 : Reward eval/Train: -710.8167273742797/-352.91097081833317 \n",
      "Epoch: 132000 : Reward eval/Train: -693.2835618102334/-348.94573375757005 \n",
      "Epoch: 136000 : Reward eval/Train: -714.7438841770681/-352.4309603810714 \n",
      "Epoch: 140000 : Reward eval/Train: -733.1269188550947/-350.94744042534757 \n",
      "Epoch: 144000 : Reward eval/Train: -714.0824469181085/-350.84436064952416 \n",
      "Epoch: 148000 : Reward eval/Train: -734.096517921839/-347.7946975120563 \n",
      "Epoch: 152000 : Reward eval/Train: -714.8697737929419/-345.3535956178088 \n",
      "Epoch: 156000 : Reward eval/Train: -716.9220201625552/-340.023990120924 \n",
      "Epoch: 160000 : Reward eval/Train: -735.2781377054083/-338.6068731352777 \n",
      "Epoch: 164000 : Reward eval/Train: -718.2104632877977/-337.92234795291574 \n",
      "Epoch: 168000 : Reward eval/Train: -701.3334004396075/-333.7995431779456 \n",
      "Epoch: 172000 : Reward eval/Train: -686.314823563012/-337.2491311392939 \n",
      "Epoch: 176000 : Reward eval/Train: -703.0431058773356/-338.06296625039994 \n",
      "Epoch: 180000 : Reward eval/Train: -690.0769317586959/-338.4162591689888 \n",
      "Epoch: 184000 : Reward eval/Train: -675.802888840176/-337.94633014849387 \n",
      "Epoch: 188000 : Reward eval/Train: -661.4118596181853/-336.00822789910177 \n",
      "Epoch: 192000 : Reward eval/Train: -650.0585281530008/-330.84129302789813 \n",
      "Epoch: 196000 : Reward eval/Train: -637.1194690163246/-330.29609817873546 \n",
      "Epoch: 200000 : Reward eval/Train: -627.4123781801678/-323.55760238853156 \n",
      "Epoch: 204000 : Reward eval/Train: -618.4863629811243/-323.9731651860354 \n",
      "Epoch: 208000 : Reward eval/Train: -609.0478187491183/-323.7711828409856 \n",
      "Epoch: 212000 : Reward eval/Train: -598.3496610623872/-321.7285240181281 \n",
      "Epoch: 216000 : Reward eval/Train: -589.6545097830576/-315.5988792824664 \n",
      "Epoch: 220000 : Reward eval/Train: -579.5604687091396/-311.41872554382974 \n",
      "Epoch: 224000 : Reward eval/Train: -569.8421949234822/-307.6671788502336 \n",
      "Epoch: 228000 : Reward eval/Train: -578.661879857425/-305.1567116451876 \n",
      "Epoch: 232000 : Reward eval/Train: -569.3620819400334/-306.00144409479475 \n",
      "Epoch: 236000 : Reward eval/Train: -560.728006528767/-302.1114784884481 \n",
      "Epoch: 240000 : Reward eval/Train: -552.7069739628724/-302.4340002483263 \n",
      "Epoch: 244000 : Reward eval/Train: -544.8918780595611/-299.53760118691474 \n",
      "Epoch: 248000 : Reward eval/Train: -536.3207783701291/-298.8813432451611 \n",
      "Epoch: 252000 : Reward eval/Train: -528.0805263443469/-294.60538986631104 \n",
      "Epoch: 256000 : Reward eval/Train: -520.2972935253549/-296.7251988162075 \n",
      "Epoch: 260000 : Reward eval/Train: -513.0224205094893/-298.63025490165865 \n",
      "Epoch: 264000 : Reward eval/Train: -514.213820133736/-297.5950334167822 \n",
      "Epoch: 268000 : Reward eval/Train: -506.9703913318677/-294.0432940869702 \n",
      "Epoch: 272000 : Reward eval/Train: -500.7783662845862/-293.7925796262927 \n",
      "Epoch: 276000 : Reward eval/Train: -514.1871538038358/-292.35110235206866 \n",
      "Epoch: 280000 : Reward eval/Train: -527.8063932088824/-289.7116980721741 \n",
      "Epoch: 284000 : Reward eval/Train: -521.006756747333/-289.2033098081626 \n",
      "Epoch: 288000 : Reward eval/Train: -531.9661361230818/-291.1054452038239 \n",
      "Epoch: 292000 : Reward eval/Train: -543.5169799931556/-288.770778928678 \n",
      "Epoch: 296000 : Reward eval/Train: -536.9759500258167/-289.4932644217464 \n",
      "Epoch: 300000 : Reward eval/Train: -530.4795068022769/-289.84866980359243 \n",
      "Epoch: 304000 : Reward eval/Train: -523.8940647020097/-291.3991063916494 \n",
      "Epoch: 308000 : Reward eval/Train: -517.7123810926591/-291.3236321238991 \n",
      "Epoch: 312000 : Reward eval/Train: -511.490820311712/-290.0573431616867 \n",
      "Epoch: 316000 : Reward eval/Train: -504.98241558424985/-285.9506394332359 \n",
      "Epoch: 320000 : Reward eval/Train: -499.2908881747935/-280.31508139900654 \n",
      "Epoch: 324000 : Reward eval/Train: -494.43739076967535/-281.0801316137022 \n",
      "Epoch: 328000 : Reward eval/Train: -489.11768200571373/-280.4610814319877 \n",
      "Epoch: 332000 : Reward eval/Train: -484.74918029394115/-279.18945241207416 \n",
      "Epoch: 336000 : Reward eval/Train: -479.20440194240666/-273.7228173946439 \n",
      "Epoch: 340000 : Reward eval/Train: -475.01211447086126/-268.8449888675199 \n",
      "Epoch: 344000 : Reward eval/Train: -471.26147514363157/-267.66772694697414 \n",
      "Epoch: 348000 : Reward eval/Train: -466.39660411218017/-264.58688000147373 \n",
      "Epoch: 352000 : Reward eval/Train: -462.73871667306594/-263.0670989098909 \n",
      "Epoch: 356000 : Reward eval/Train: -458.78838801056486/-263.399121361368 \n",
      "Epoch: 360000 : Reward eval/Train: -456.16044520995484/-264.38494286410463 \n",
      "Epoch: 364000 : Reward eval/Train: -453.3892418869437/-265.5241364381386 \n",
      "Epoch: 368000 : Reward eval/Train: -448.6777995300146/-264.74053080051857 \n",
      "Epoch: 372000 : Reward eval/Train: -444.53469098383766/-265.02669449833274 \n",
      "Epoch: 376000 : Reward eval/Train: -440.26056286068007/-261.6468064738972 \n",
      "Epoch: 380000 : Reward eval/Train: -436.0340701878306/-261.5172965299028 \n",
      "Epoch: 384000 : Reward eval/Train: -432.34770317594257/-259.4803983966167 \n",
      "Epoch: 388000 : Reward eval/Train: -428.15323965117943/-258.59696597490137 \n",
      "Epoch: 392000 : Reward eval/Train: -423.92218567396486/-258.60085588165 \n",
      "Epoch: 396000 : Reward eval/Train: -420.1188756965306/-257.36373553350967 \n",
      "Epoch: 400000 : Reward eval/Train: -423.0071305219714/-257.894286056124 \n",
      "Epoch: 404000 : Reward eval/Train: -419.03728458108174/-257.31494672358497 \n",
      "Epoch: 408000 : Reward eval/Train: -415.11975889814187/-256.46720673986914 \n",
      "Epoch: 412000 : Reward eval/Train: -411.55241597802745/-252.6818837882569 \n",
      "Epoch: 416000 : Reward eval/Train: -408.7571866856998/-251.55409005418934 \n",
      "Epoch: 420000 : Reward eval/Train: -405.06542646397577/-250.34221405846748 \n",
      "Epoch: 424000 : Reward eval/Train: -413.39309461202856/-250.26377131040297 \n",
      "Epoch: 428000 : Reward eval/Train: -409.94613543986185/-249.75832109497338 \n",
      "Epoch: 432000 : Reward eval/Train: -406.53135634202386/-248.21016764208576 \n",
      "Epoch: 436000 : Reward eval/Train: -403.74624519045324/-248.0994971135531 \n",
      "Epoch: 440000 : Reward eval/Train: -411.62298315169215/-246.04621076538353 \n",
      "Epoch: 444000 : Reward eval/Train: -408.20538422046184/-245.79912312314275 \n",
      "Epoch: 448000 : Reward eval/Train: -404.8214003434786/-244.90372416849644 \n",
      "Epoch: 452000 : Reward eval/Train: -401.4692118557222/-243.2505089256796 \n",
      "Epoch: 456000 : Reward eval/Train: -398.41340676569826/-239.53779080407224 \n",
      "Epoch: 460000 : Reward eval/Train: -395.1508826254905/-237.7183660443267 \n",
      "Epoch: 464000 : Reward eval/Train: -392.6765681733518/-238.21665820856546 \n",
      "Epoch: 468000 : Reward eval/Train: -389.8991814299873/-236.85618023462175 \n",
      "Epoch: 472000 : Reward eval/Train: -387.2060957936113/-236.4526169612249 \n",
      "Epoch: 476000 : Reward eval/Train: -384.37216137032476/-235.7047735403183 \n",
      "Epoch: 480000 : Reward eval/Train: -381.51737017366776/-235.40781190921652 \n",
      "Epoch: 484000 : Reward eval/Train: -378.72035424052115/-235.0270823968503 \n",
      "Epoch: 488000 : Reward eval/Train: -375.77582605025646/-234.1922782989112 \n",
      "Epoch: 492000 : Reward eval/Train: -372.84911923545/-233.78333061596683 \n",
      "Epoch: 496000 : Reward eval/Train: -370.15027823054317/-233.70103724830122 \n",
      "Epoch: 500000 : Reward eval/Train: -368.0693938182532/-231.7565656996653 \n",
      "Epoch: 504000 : Reward eval/Train: -365.63515461359236/-231.51911041519475 \n",
      "Epoch: 508000 : Reward eval/Train: -363.41955113178415/-229.2071104724864 \n",
      "Epoch: 512000 : Reward eval/Train: -361.5067283690959/-226.44854497291976 \n",
      "Epoch: 516000 : Reward eval/Train: -359.7389980413295/-225.1561701752398 \n",
      "Epoch: 520000 : Reward eval/Train: -357.2884525234093/-224.26391340839479 \n",
      "Epoch: 524000 : Reward eval/Train: -355.0740759218517/-220.48068481623753 \n",
      "Epoch: 528000 : Reward eval/Train: -352.5747826219645/-219.5087448625025 \n",
      "Epoch: 532000 : Reward eval/Train: -362.07777763631333/-218.10753413492154 \n",
      "Epoch: 536000 : Reward eval/Train: -359.85514652111635/-217.9561066124437 \n",
      "Epoch: 540000 : Reward eval/Train: -357.6645016794601/-217.82907726087674 \n",
      "Epoch: 544000 : Reward eval/Train: -355.95212499128485/-217.90316097920902 \n",
      "Epoch: 548000 : Reward eval/Train: -353.63797952554603/-216.78269631908023 \n",
      "Epoch: 552000 : Reward eval/Train: -360.6312536454138/-213.61797049134964 \n",
      "Epoch: 556000 : Reward eval/Train: -366.7047170662732/-213.27712065364523 \n",
      "Epoch: 560000 : Reward eval/Train: -365.2326165225978/-211.5752681631885 \n",
      "Epoch: 564000 : Reward eval/Train: -362.86713664547966/-211.41309430471887 \n",
      "Epoch: 568000 : Reward eval/Train: -360.4680749247417/-209.87322294869966 \n",
      "Epoch: 572000 : Reward eval/Train: -358.7138071841369/-208.93686387220808 \n",
      "Epoch: 576000 : Reward eval/Train: -356.6364409556088/-207.5081646248245 \n",
      "Epoch: 580000 : Reward eval/Train: -354.3512852768206/-205.77516357978368 \n",
      "Epoch: 584000 : Reward eval/Train: -352.17312303787185/-204.76681796564336 \n",
      "Epoch: 588000 : Reward eval/Train: -350.0496301502952/-201.40292399579235 \n",
      "Epoch: 592000 : Reward eval/Train: -347.9485075181458/-198.52911139822626 \n",
      "Epoch: 596000 : Reward eval/Train: -345.9137825428522/-197.4044539830282 \n",
      "Epoch: 600000 : Reward eval/Train: -344.0735653369117/-197.70486499901958 \n",
      "Epoch: 604000 : Reward eval/Train: -341.89951093066713/-195.9702675703148 \n",
      "Epoch: 608000 : Reward eval/Train: -347.75264022191305/-193.50704114159024 \n",
      "Epoch: 612000 : Reward eval/Train: -345.3775151684669/-192.8363950883929 \n",
      "Epoch: 616000 : Reward eval/Train: -343.230835134969/-191.44189151044387 \n",
      "Epoch: 620000 : Reward eval/Train: -341.2478803722728/-190.02798917474928 \n",
      "Epoch: 624000 : Reward eval/Train: -339.2971052941478/-189.25597612532871 \n",
      "Epoch: 628000 : Reward eval/Train: -337.30467710956384/-185.85718472269158 \n",
      "Epoch: 632000 : Reward eval/Train: -335.56796948734467/-184.05309999133894 \n",
      "Epoch: 636000 : Reward eval/Train: -333.7148368552288/-179.7758866712997 \n",
      "Epoch: 640000 : Reward eval/Train: -332.12484175153133/-178.26017349235858 \n",
      "Epoch: 644000 : Reward eval/Train: -330.5635569953982/-176.009458711664 \n",
      "Epoch: 648000 : Reward eval/Train: -328.6692742980878/-173.81021615652986 \n",
      "Epoch: 652000 : Reward eval/Train: -326.72169598318203/-172.2405935580858 \n",
      "Epoch: 656000 : Reward eval/Train: -325.0538002495874/-169.59354584981355 \n",
      "Epoch: 660000 : Reward eval/Train: -323.1482602831412/-167.7608468445156 \n",
      "Epoch: 664000 : Reward eval/Train: -321.512416526321/-167.2771202765856 \n",
      "Epoch: 668000 : Reward eval/Train: -323.1222387956438/-165.07671753365693 \n",
      "Epoch: 672000 : Reward eval/Train: -321.4798705285178/-163.9946507959001 \n",
      "Epoch: 676000 : Reward eval/Train: -319.68090646065497/-161.61998628372612 \n",
      "Epoch: 680000 : Reward eval/Train: -318.02104492026956/-160.05353683483673 \n",
      "Epoch: 684000 : Reward eval/Train: -316.3677229012396/-157.65945676307285 \n",
      "Epoch: 688000 : Reward eval/Train: -314.60652891644423/-154.56581712126936 \n",
      "Epoch: 692000 : Reward eval/Train: -312.9013942588925/-151.754002189099 \n",
      "Epoch: 696000 : Reward eval/Train: -311.2821574563009/-150.28848791271832 \n",
      "Epoch: 700000 : Reward eval/Train: -309.68118797958186/-148.88192066823524 \n",
      "Epoch: 704000 : Reward eval/Train: -308.1700331076008/-146.94387679479317 \n",
      "Epoch: 708000 : Reward eval/Train: -306.5144328076682/-144.93794784138487 \n",
      "Epoch: 712000 : Reward eval/Train: -305.459112793148/-143.34026452952824 \n",
      "Epoch: 716000 : Reward eval/Train: -303.9715975583813/-140.47311641573003 \n",
      "Epoch: 720000 : Reward eval/Train: -302.90371568295166/-138.5612603315863 \n",
      "Epoch: 724000 : Reward eval/Train: -301.30949033955216/-136.72220518765423 \n",
      "Epoch: 728000 : Reward eval/Train: -299.77852502718866/-135.31690180438017 \n",
      "Epoch: 732000 : Reward eval/Train: -298.4408590118257/-133.70408813522104 \n",
      "Epoch: 736000 : Reward eval/Train: -297.0141717327678/-132.27903644750614 \n",
      "Epoch: 740000 : Reward eval/Train: -295.52452209890544/-129.99857076218584 \n",
      "Epoch: 744000 : Reward eval/Train: -294.17178179321473/-128.28148436271283 \n",
      "Epoch: 748000 : Reward eval/Train: -293.010483822626/-126.71746560461871 \n",
      "Epoch: 752000 : Reward eval/Train: -291.59404267621136/-125.54164589290622 \n",
      "Epoch: 756000 : Reward eval/Train: -290.10994526961906/-123.79892948579702 \n",
      "Epoch: 760000 : Reward eval/Train: -288.6602287316031/-122.0708074693279 \n",
      "Epoch: 764000 : Reward eval/Train: -287.4779353088875/-120.52309141366955 \n",
      "Epoch: 768000 : Reward eval/Train: -286.0466438185241/-119.4941367513525 \n",
      "Epoch: 772000 : Reward eval/Train: -284.71490393111253/-117.63012771504157 \n",
      "Epoch: 776000 : Reward eval/Train: -283.3096108526087/-115.8590773168289 \n",
      "Epoch: 780000 : Reward eval/Train: -282.03018710826655/-114.34893382992755 \n",
      "Epoch: 784000 : Reward eval/Train: -280.67219859172775/-113.50126904866863 \n",
      "Epoch: 788000 : Reward eval/Train: -279.6329310607089/-112.49972126961714 \n",
      "Epoch: 792000 : Reward eval/Train: -278.28392824502635/-111.68758093614137 \n",
      "Epoch: 796000 : Reward eval/Train: -277.4867336650274/-110.64864153445893 \n",
      "Epoch: 800000 : Reward eval/Train: -276.57470674532834/-110.26270588758779 \n",
      "Epoch: 804000 : Reward eval/Train: -275.3469671229285/-109.75420622695495 \n",
      "Epoch: 808000 : Reward eval/Train: -274.11612297704096/-109.15638348829951 \n",
      "Epoch: 812000 : Reward eval/Train: -272.8995839999062/-108.43522071542804 \n",
      "Epoch: 816000 : Reward eval/Train: -271.68355949014455/-107.64789088735306 \n",
      "Epoch: 820000 : Reward eval/Train: -270.4398120537372/-106.5188994380532 \n",
      "Epoch: 824000 : Reward eval/Train: -269.5215530299402/-105.71824917751543 \n",
      "Epoch: 828000 : Reward eval/Train: -268.3476475466118/-104.83209152590472 \n",
      "Epoch: 832000 : Reward eval/Train: -267.150016889132/-104.39360872283208 \n",
      "Epoch: 836000 : Reward eval/Train: -265.99831545011773/-103.40280700003942 \n",
      "Epoch: 840000 : Reward eval/Train: -264.8601304032958/-102.4323475543102 \n",
      "Epoch: 844000 : Reward eval/Train: -263.8342050544598/-101.85643762467176 \n",
      "Epoch: 848000 : Reward eval/Train: -262.74009288632107/-101.68608049820565 \n",
      "Epoch: 852000 : Reward eval/Train: -261.74908369659096/-100.74633682138989 \n",
      "Epoch: 856000 : Reward eval/Train: -260.6861592437903/-100.20987631799572 \n",
      "Epoch: 860000 : Reward eval/Train: -259.5515049930493/-99.29761252279914 \n",
      "Epoch: 864000 : Reward eval/Train: -258.6237463969785/-98.7412969593834 \n",
      "Epoch: 868000 : Reward eval/Train: -257.5850193235795/-98.27498640848046 \n",
      "Epoch: 872000 : Reward eval/Train: -256.4881129437792/-97.58658850534445 \n",
      "Epoch: 876000 : Reward eval/Train: -255.5838970785835/-97.29176516738356 \n",
      "Epoch: 880000 : Reward eval/Train: -254.58273031194437/-96.87510940181494 \n",
      "Epoch: 884000 : Reward eval/Train: -253.52432668921378/-95.97796318845113 \n",
      "Epoch: 888000 : Reward eval/Train: -252.43260221592018/-95.11176933888264 \n",
      "Epoch: 892000 : Reward eval/Train: -251.53305498114042/-94.28119739782258 \n",
      "Epoch: 896000 : Reward eval/Train: -250.4839475973824/-93.34799481928954 \n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_sac\"\n",
    "    project_name = \"keras_tunning_soft\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/sac/\",\n",
    "                        end_of_episode = EPISODES, evaluation_epoch = 4000, training_steps = 1000000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,        \n",
    "                discount = 0.99,\n",
    "                #discount_min = 0.95, discount_max = 0.99,      \n",
    "                      \n",
    "                #min_lr_actor_critic = 0.00001, max_lr_actor_critic= 0.001,\n",
    "                lr_actor_min = 0.0001, lr_actor_max = 0.001,\n",
    "                lr_critic_1_min = 0.0001, lr_critic_1_max = 0.001, \n",
    "                \n",
    "                #dense_unit=256,\n",
    "                dense_min = 128, dense_max = 400,\n",
    "                \n",
    "                environment_name=ENV,\n",
    "                #tau = 0.005,\n",
    "                tau_min = 0.001, tau_max = 0.1,\n",
    "                \n",
    "                num_layers_act = 2, num_layers_crit =2,\n",
    "                #max_num_layers_act = 2, max_num_layers_crit=2\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 30,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name,\n",
    "            #seed=0\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "\n",
    "        \n",
    "        run_training(\n",
    "            training_steps = 1500000,  \n",
    "            train_epochs = 50,\n",
    "            evaluation_epoch = 4000,\n",
    "            environment_name = ENV,\n",
    "            end_of_episode= EPISODES,\n",
    "            sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH,\n",
    "            sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "            discount= 0.99, \n",
    "            dense_units_act = [256,256],  \n",
    "            dense_units_crit = [256,256], \n",
    "            num_layer_a= 2, \n",
    "            num_layer_c= 2, \n",
    "            writer = writer, \n",
    "            reward_scaler = 1, \n",
    "            lr_actor= 0.001,\n",
    "            lr_critic_1= 0.001, \n",
    "            lr_alpha = 0.001, \n",
    "            tau = 0.005,\n",
    "            return_agent = False\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=5):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_env = gym.make(ENV)\n",
    "dir = r\"Hyperparam_kt_sac\"\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=4):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "    \n",
    "#env_model = tuner.get_best_models()[0]\n",
    "best_hps = tuner.get_best_hyperparameters(4)[-1]  # Access best hyperparameter configuration\n",
    "print(f\"Best Hyperparameters: {best_hps}\")\n",
    "\n",
    "env_model = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "env_model = rerun_training(training_steps  = 2000000, \n",
    "                           save_factor = 50000,  \n",
    "                           model_path = './checkpoints/SACagent',\n",
    "                           sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, \n",
    "                           sucess_criteria_value = SUCESS_CRITERIA_VALUE, \n",
    "                           return_agent = True, \n",
    "                           model = env_model\n",
    "                           )\n",
    "\n",
    "final_rewards = final_evaluation(env_model, val_env, n_tries=200, exploration=exploration_tech, sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH,  video_name = \"./sac_\"+exploration_tech+\"_video.mp4\")\n",
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "training_steps = 2000000\n",
    "discount  = best_hp[\"discount\"]\n",
    "lr_actor  = best_hp[\"lr_actor\"]\n",
    "lr_critic_1  = best_hp[\"lr_critic_1\"]\n",
    "tau  = best_hp[\"tau\"]\n",
    "\n",
    "end_of_episode = EPISODES\n",
    "sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH\n",
    "sucess_criteria_value = SUCESS_CRITERIA_VALUE\n",
    "save_factor=50000\n",
    "return_agent = True\n",
    "reward_scaler = 100\n",
    "environment_name=ENV\n",
    "\n",
    "n_dense_layers_actor = trials.hyperparameters.values[\"n_dense_layers_actor\"]\n",
    "dense_layers_actor = []\n",
    "for i in range(n_dense_layers_actor):\n",
    "    dense_layers_actor.append(trials.hyperparameters.values['dense_units_act_'+str(i)])\n",
    "\n",
    "n_dense_layers_critic = trials.hyperparameters.values[\"n_dense_layers_critic\"]\n",
    "dense_layers_critic = []\n",
    "for i in range(n_dense_layers_critic):\n",
    "    dense_layers_critic.append(trials.hyperparameters.values['dense_units_crit_'+str(i)])\n",
    "\n",
    "env_model = run_training(training_steps = training_steps,\n",
    "                         discount = discount,  \n",
    "                         dense_units_act = dense_layers_actor,  \n",
    "                         dense_units_crit = dense_layers_critic, \n",
    "                         num_layer_a = n_dense_layers_actor, \n",
    "                         num_layer_c = n_dense_layers_critic, \n",
    "                         writer = writer, \n",
    "                         end_of_episode = end_of_episode, \n",
    "                         save_factor = save_factor, \n",
    "                         sucess_criteria_epochs = sucess_criteria_epochs , \n",
    "                         sucess_criteria_value = sucess_criteria_value, \n",
    "                         environment_name = environment_name, \n",
    "                         reward_scaler = reward_scaler , \n",
    "                         return_agent = return_agent,\n",
    "                         lr_actor = lr_actor, \n",
    "                         lr_critic_1 = lr_critic_1, \n",
    "                         tau = tau )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./sac_\"+exploration_tech+\"_video.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
