{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from SAC_Agent_tf import *\n",
    "from ENV_DETAILS import *\n",
    "import os\n",
    "#os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-12.1/lib64:/usr/local/cuda/lib64:/usr/local/cuda-12.1/lib64:/usr/local/cuda/lib64:/usr/local/cuda-12.1/lib64:/usr/local/cuda/lib64\"\n",
    "\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\BipedalWalker\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/BipedalWalker/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_sac/' # Linux\n",
    "\n",
    "ENV = \"Ant-v2\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_steps = env._max_episode_steps\n",
    "max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.action_space.sample()\n",
    "a, a.shape, a.reshape((1, len(a))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"logs_hyper/fit_sac/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_sac\"\n",
    "    project_name = \"keras_tunning_soft_tf\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/sac_tf/\",\n",
    "                        end_of_episode = EPISODES, evaluation_epoch = 4000, training_steps = 700000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,        \n",
    "                discount = 0.99,\n",
    "                #discount_min = 0.98, discount_max = 0.99,      \n",
    "                      \n",
    "                min_lr_actor_critic = 0.00001, max_lr_actor_critic= 0.001,\n",
    "                #lr_actor_min = 0.0001, lr_actor_max = 0.001,\n",
    "                #lr_critic_1_min = 0.0001, lr_critic_1_max = 0.001, \n",
    "                \n",
    "                dense_unit=256,\n",
    "                #dense_min = 128, dense_max = 256,\n",
    "                \n",
    "                environment_name=ENV,\n",
    "                #tau = 0.005,\n",
    "                tau_min = 0.001, tau_max = 0.1,\n",
    "                \n",
    "                num_layers_act = 2, num_layers_crit =2,\n",
    "                #max_num_layers_act = 2, max_num_layers_crit=2\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 20,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "        writer= \"logs_hyper/fit_sac_tf/\"\n",
    "\n",
    "        training_steps = 1000000\n",
    "        learning_rate= 0.001\n",
    "        entropy_factor = 0.1\n",
    "        discount = 0.99\n",
    "        dense_units_actor = [128,64]\n",
    "        num_layers_actor = 2\n",
    "        dense_units_critic = [128,64]\n",
    "        num_layers_crit = 2\n",
    "        time_to_update= 100\n",
    "        end_of_episode = 300\n",
    "        tau = 0.01\n",
    "        \n",
    "        run_training(training_steps,  discount, dense_units_actor,  dense_units_critic, num_layers_actor, num_layers_crit, writer, end_of_episode,\n",
    "                      environment_name = ENV,reward_scaler = 10, return_agent = False,lr_actor= 0.001, lr_critic_1= 0.001, lr_alpha = 0.001, tau = 0.001)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=5):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_env = gym.make(ENV)\n",
    "dir = r\"Hyperparam_kt_sac\"\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=4):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "    \n",
    "#env_model = tuner.get_best_models()[0]\n",
    "best_hps = tuner.get_best_hyperparameters(4)[-1]  # Access best hyperparameter configuration\n",
    "print(f\"Best Hyperparameters: {best_hps}\")\n",
    "\n",
    "env_model = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "env_model = rerun_training(training_steps  = 2000000, \n",
    "                           save_factor = 50000,  \n",
    "                           model_path = './checkpoints/SACagent',\n",
    "                           sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, \n",
    "                           sucess_criteria_value = SUCESS_CRITERIA_VALUE, \n",
    "                           return_agent = True, \n",
    "                           model = env_model\n",
    "                           )\n",
    "\n",
    "final_rewards = final_evaluation(env_model, val_env, n_tries=200, exploration=exploration_tech, sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH,  video_name = \"./sac_\"+exploration_tech+\"_video.mp4\")\n",
    "print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "training_steps = 2000000\n",
    "discount  = best_hp[\"discount\"]\n",
    "lr_actor  = best_hp[\"lr_actor\"]\n",
    "lr_critic_1  = best_hp[\"lr_critic_1\"]\n",
    "tau  = best_hp[\"tau\"]\n",
    "\n",
    "end_of_episode = EPISODES\n",
    "sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH\n",
    "sucess_criteria_value = SUCESS_CRITERIA_VALUE\n",
    "save_factor=50000\n",
    "return_agent = True\n",
    "reward_scaler = 100\n",
    "environment_name=ENV\n",
    "\n",
    "n_dense_layers_actor = trials.hyperparameters.values[\"n_dense_layers_actor\"]\n",
    "dense_layers_actor = []\n",
    "for i in range(n_dense_layers_actor):\n",
    "    dense_layers_actor.append(trials.hyperparameters.values['dense_units_act_'+str(i)])\n",
    "\n",
    "n_dense_layers_critic = trials.hyperparameters.values[\"n_dense_layers_critic\"]\n",
    "dense_layers_critic = []\n",
    "for i in range(n_dense_layers_critic):\n",
    "    dense_layers_critic.append(trials.hyperparameters.values['dense_units_crit_'+str(i)])\n",
    "\n",
    "env_model = run_training(training_steps = training_steps,\n",
    "                         discount = discount,  \n",
    "                         dense_units_act = dense_layers_actor,  \n",
    "                         dense_units_crit = dense_layers_critic, \n",
    "                         num_layer_a = n_dense_layers_actor, \n",
    "                         num_layer_c = n_dense_layers_critic, \n",
    "                         writer = writer, \n",
    "                         end_of_episode = end_of_episode, \n",
    "                         save_factor = save_factor, \n",
    "                         sucess_criteria_epochs = sucess_criteria_epochs , \n",
    "                         sucess_criteria_value = sucess_criteria_value, \n",
    "                         environment_name = environment_name, \n",
    "                         reward_scaler = reward_scaler , \n",
    "                         return_agent = return_agent,\n",
    "                         lr_actor = lr_actor, \n",
    "                         lr_critic_1 = lr_critic_1, \n",
    "                         tau = tau )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./sac_\"+exploration_tech+\"_video.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
