{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 08:29:17.058625: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_merged import * #PPO_Agent_v2 PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TESNORBOARD import *\n",
    "\n",
    "# events_folder = \"./logs_hyper\"\n",
    "# main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\MountainCar\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/MountainCar/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_a3c/' # Linux\n",
    "\n",
    "ENV = \"MountainCarContinuous-v0\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]\n",
    "SUCESS_CRITERIA_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<Continuous_MountainCarEnv<MountainCarContinuous-v0>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6651659 , -0.03973157], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1.2  -0.07], [0.6  0.07], (2,), float32), 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.40082148"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir ./logs_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_PPO_guided/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 Complete [01h 30m 03s]\n",
      "total_train_reward: -80.14733135559602\n",
      "\n",
      "Best total_train_reward So Far: -52.069832466313024\n",
      "Total elapsed time: 4d 10h 04m 48s\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo_guided\"\n",
    "  \n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_guided/\", evaluation_epoch = env._max_episode_steps, training_steps = 700000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.90, discount_max = 0.99,\n",
    "                #discount = 0.99,\n",
    "                policy_clip = 0.2,\n",
    "                gae_min = 0.94, gae_max = 0.96,\n",
    "                lr_agent_min = 0.00001, lr_agent_max = 0.001,\n",
    "                entropy_factor_min = 0.001, entropy_factor_max = 0.1,\n",
    "                dense_min = 32, dense_max = 256,\n",
    "                environment_name=ENV,\n",
    "                num_layers_agent = 2, training_epoch = 100,  normalize = False,\n",
    "                memory_size= env._max_episode_steps\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 40,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            beta = 10,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "\n",
    "        training_steps = 1000000\n",
    "        entropy_factor = 0.05\n",
    "        discount = 0.99\n",
    "        dense_units_agent = [128,128] #64, 32]\n",
    "        num_layer_agent = 2\n",
    "\n",
    "        model = run_training(training_steps,  discount, dense_units_agent,  num_layer_agent, writer, \n",
    "                      environment_name = ENV,return_agent = True, agent_lr= 0.0001, normalize = True,\n",
    "                      gae_lambda = 0.95, entropy_coeff = entropy_factor, policy_clip = 0.2, training_epoch = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial id :08 | Score :-52.069832466313024 --> {'discount': 0.9, 'gae_lambda': 0.95, 'agent_lr': 0.00010577186590904286, 'entropy_coeff': 0.043667988955104375, 'num_layer_agent': 1, 'dense_units_agent_0': 158, 'dense_units_agent_1': 175}\n",
      "Trial id :05 | Score :-62.91236911926401 --> {'discount': 0.97, 'gae_lambda': 0.96, 'agent_lr': 0.00036367149574077746, 'entropy_coeff': 0.0695328842691358, 'num_layer_agent': 1, 'dense_units_agent_0': 86, 'dense_units_agent_1': 125}\n",
      "Trial id :04 | Score :-65.54745874504796 --> {'discount': 0.9500000000000001, 'gae_lambda': 0.94, 'agent_lr': 0.00016471050757726357, 'entropy_coeff': 0.02718591705211787, 'num_layer_agent': 2, 'dense_units_agent_0': 109, 'dense_units_agent_1': 51}\n",
      "Trial id :18 | Score :-67.64265664538723 --> {'discount': 0.91, 'gae_lambda': 0.94, 'agent_lr': 4.697922200117869e-05, 'entropy_coeff': 0.06415108752076276, 'num_layer_agent': 2, 'dense_units_agent_0': 73, 'dense_units_agent_1': 226}\n",
      "Trial id :16 | Score :-67.84943951772239 --> {'discount': 0.97, 'gae_lambda': 0.94, 'agent_lr': 0.00039104759486295745, 'entropy_coeff': 0.05057489309216712, 'num_layer_agent': 1, 'dense_units_agent_0': 195, 'dense_units_agent_1': 132}\n",
      "Trial id :14 | Score :-70.80777428981531 --> {'discount': 0.91, 'gae_lambda': 0.95, 'agent_lr': 0.00016199112216650827, 'entropy_coeff': 0.07413990148797216, 'num_layer_agent': 1, 'dense_units_agent_0': 95, 'dense_units_agent_1': 143}\n",
      "Trial id :02 | Score :-72.13851224398708 --> {'discount': 0.9500000000000001, 'gae_lambda': 0.94, 'agent_lr': 0.0007318603212529882, 'entropy_coeff': 0.006579921501998329, 'num_layer_agent': 1, 'dense_units_agent_0': 157, 'dense_units_agent_1': 46}\n",
      "Trial id :13 | Score :-76.01224449334369 --> {'discount': 0.9500000000000001, 'gae_lambda': 0.95, 'agent_lr': 0.0006959229572754088, 'entropy_coeff': 0.05845967428220667, 'num_layer_agent': 2, 'dense_units_agent_0': 33, 'dense_units_agent_1': 36}\n",
      "Trial id :26 | Score :-76.11530076383602 --> {'discount': 0.97, 'gae_lambda': 0.95, 'agent_lr': 0.0005761752128610849, 'entropy_coeff': 0.08867492034113249, 'num_layer_agent': 2, 'dense_units_agent_0': 156, 'dense_units_agent_1': 115}\n",
      "Trial id :28 | Score :-76.35905131301625 --> {'discount': 0.9500000000000001, 'gae_lambda': 0.96, 'agent_lr': 0.00035074463897381196, 'entropy_coeff': 0.04088805396997601, 'num_layer_agent': 1, 'dense_units_agent_0': 198, 'dense_units_agent_1': 256}\n",
      "Trial id :19 | Score :-76.89131678873551 --> {'discount': 0.9400000000000001, 'gae_lambda': 0.96, 'agent_lr': 0.0006296925826612796, 'entropy_coeff': 0.08772527437248229, 'num_layer_agent': 2, 'dense_units_agent_0': 132, 'dense_units_agent_1': 188}\n",
      "Trial id :10 | Score :-77.31053109449519 --> {'discount': 0.98, 'gae_lambda': 0.96, 'agent_lr': 0.00038976441486876756, 'entropy_coeff': 0.04866211963915229, 'num_layer_agent': 2, 'dense_units_agent_0': 230, 'dense_units_agent_1': 98}\n",
      "Trial id :25 | Score :-78.28463486355005 --> {'discount': 0.98, 'gae_lambda': 0.95, 'agent_lr': 0.00032937300899884317, 'entropy_coeff': 0.0693899081440744, 'num_layer_agent': 1, 'dense_units_agent_0': 177, 'dense_units_agent_1': 37}\n",
      "Trial id :15 | Score :-79.73966882598276 --> {'discount': 0.9, 'gae_lambda': 0.96, 'agent_lr': 0.0009517647050591607, 'entropy_coeff': 0.036246502984454546, 'num_layer_agent': 1, 'dense_units_agent_0': 145, 'dense_units_agent_1': 74}\n",
      "Trial id :03 | Score :-79.80956851245357 --> {'discount': 0.91, 'gae_lambda': 0.95, 'agent_lr': 0.0007285344281839102, 'entropy_coeff': 0.0786096224081549, 'num_layer_agent': 1, 'dense_units_agent_0': 166, 'dense_units_agent_1': 234}\n",
      "Trial id :29 | Score :-80.42532622165595 --> {'discount': 0.91, 'gae_lambda': 0.94, 'agent_lr': 6.659555919993412e-05, 'entropy_coeff': 0.01695513441906634, 'num_layer_agent': 2, 'dense_units_agent_0': 65, 'dense_units_agent_1': 247}\n",
      "Trial id :07 | Score :-80.4613099729227 --> {'discount': 0.9400000000000001, 'gae_lambda': 0.94, 'agent_lr': 0.000842159749902946, 'entropy_coeff': 0.02668587362809966, 'num_layer_agent': 1, 'dense_units_agent_0': 248, 'dense_units_agent_1': 59}\n",
      "Trial id :00 | Score :-80.71438645538683 --> {'discount': 0.97, 'gae_lambda': 0.96, 'agent_lr': 0.0009946651983734752, 'entropy_coeff': 0.042072207203751766, 'num_layer_agent': 2, 'dense_units_agent_0': 97, 'dense_units_agent_1': 32}\n",
      "Trial id :09 | Score :-80.87795287989127 --> {'discount': 0.96, 'gae_lambda': 0.96, 'agent_lr': 0.0008586115499439706, 'entropy_coeff': 0.07804540030661795, 'num_layer_agent': 2, 'dense_units_agent_0': 51, 'dense_units_agent_1': 35}\n",
      "Trial id :27 | Score :-81.00666124995942 --> {'discount': 0.92, 'gae_lambda': 0.95, 'agent_lr': 0.00031645075836455255, 'entropy_coeff': 0.07418610359134076, 'num_layer_agent': 2, 'dense_units_agent_0': 233, 'dense_units_agent_1': 229}\n",
      "Trial id :21 | Score :-82.11003205130142 --> {'discount': 0.97, 'gae_lambda': 0.94, 'agent_lr': 0.0009596128097960411, 'entropy_coeff': 0.08981125909045241, 'num_layer_agent': 1, 'dense_units_agent_0': 96, 'dense_units_agent_1': 176}\n",
      "Trial id :22 | Score :-82.37304149490431 --> {'discount': 0.96, 'gae_lambda': 0.94, 'agent_lr': 0.0007207033017941817, 'entropy_coeff': 0.027042681705825272, 'num_layer_agent': 1, 'dense_units_agent_0': 145, 'dense_units_agent_1': 65}\n",
      "Trial id :24 | Score :-82.70391271306356 --> {'discount': 0.96, 'gae_lambda': 0.96, 'agent_lr': 0.0008179626879399864, 'entropy_coeff': 0.04625721363068279, 'num_layer_agent': 2, 'dense_units_agent_0': 176, 'dense_units_agent_1': 194}\n",
      "Trial id :17 | Score :-82.75930461172389 --> {'discount': 0.97, 'gae_lambda': 0.94, 'agent_lr': 0.0008110465275644194, 'entropy_coeff': 0.0532409483301455, 'num_layer_agent': 2, 'dense_units_agent_0': 112, 'dense_units_agent_1': 156}\n",
      "Trial id :20 | Score :-82.86357882515013 --> {'discount': 0.98, 'gae_lambda': 0.95, 'agent_lr': 0.00021280338555410466, 'entropy_coeff': 0.07690705104654186, 'num_layer_agent': 2, 'dense_units_agent_0': 152, 'dense_units_agent_1': 198}\n",
      "Trial id :11 | Score :-83.07036457927131 --> {'discount': 0.9, 'gae_lambda': 0.95, 'agent_lr': 0.0002798157288169807, 'entropy_coeff': 0.09323254572734024, 'num_layer_agent': 1, 'dense_units_agent_0': 225, 'dense_units_agent_1': 98}\n",
      "Trial id :23 | Score :-83.79737437460079 --> {'discount': 0.9500000000000001, 'gae_lambda': 0.94, 'agent_lr': 9.901685518161663e-05, 'entropy_coeff': 0.08737451761272254, 'num_layer_agent': 1, 'dense_units_agent_0': 228, 'dense_units_agent_1': 46}\n",
      "Trial id :12 | Score :-84.2048393259663 --> {'discount': 0.97, 'gae_lambda': 0.95, 'agent_lr': 0.0004942757524050235, 'entropy_coeff': 0.008228977626465513, 'num_layer_agent': 1, 'dense_units_agent_0': 251, 'dense_units_agent_1': 186}\n",
      "Trial id :01 | Score :-84.62482634078115 --> {'discount': 0.9500000000000001, 'gae_lambda': 0.96, 'agent_lr': 9.840744512498106e-05, 'entropy_coeff': 0.017415090827875717, 'num_layer_agent': 2, 'dense_units_agent_0': 151, 'dense_units_agent_1': 216}\n",
      "Trial id :06 | Score :-85.23115341720434 --> {'discount': 0.92, 'gae_lambda': 0.94, 'agent_lr': 0.000891568954021779, 'entropy_coeff': 0.05878948961853992, 'num_layer_agent': 2, 'dense_units_agent_0': 240, 'dense_units_agent_1': 249}\n"
     ]
    }
   ],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=50):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial id :08 | Score :-52.069832466313024 --> {'discount': 0.9, 'gae_lambda': 0.95, 'agent_lr': 0.00010577186590904286, 'entropy_coeff': 0.043667988955104375, 'num_layer_agent': 1, 'dense_units_agent_0': 158, 'dense_units_agent_1': 175}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lr_actor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m :\n\u001b[1;32m     12\u001b[0m     policy_clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m---> 14\u001b[0m lr_actor\u001b[38;5;241m=\u001b[39m  \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr_actor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m lr_critic\u001b[38;5;241m=\u001b[39m  trials\u001b[38;5;241m.\u001b[39mhyperparameters\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_critic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m lr_model\u001b[38;5;241m=\u001b[39m  trials\u001b[38;5;241m.\u001b[39mhyperparameters\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lr_actor'"
     ]
    }
   ],
   "source": [
    "val_env = gym.make(ENV)#, render_mode = \"rgb_array\"\n",
    "dir = r\"Hyperparam_kt_ppo\"\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=1):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)    \n",
    "    training_steps = 1000000\n",
    "    entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "    discount = trials.hyperparameters.values[\"discount\"]\n",
    "    gae = trials.hyperparameters.values[\"gae_lambda\"]\n",
    "    try:\n",
    "        policy_clip = trials.hyperparameters.values[\"policy_clip\"]\n",
    "    except :\n",
    "        policy_clip = 0.2\n",
    "    \n",
    "    lr_actor=  trials.hyperparameters.values[\"lr_actor\"]\n",
    "    lr_critic=  trials.hyperparameters.values[\"lr_critic\"]\n",
    "    lr_model=  trials.hyperparameters.values[\"lr_model\"]\n",
    "    \n",
    "    try:\n",
    "        n_dense_layers_actor = trials.hyperparameters.values[\"n_dense_layers_actor\"]\n",
    "    except : \n",
    "        n_dense_layers_actor = 1\n",
    "        \n",
    "    try:\n",
    "        n_dense_layers_critic = trials.hyperparameters.values[\"n_dense_layers_critic\"]\n",
    "    except:\n",
    "        n_dense_layers_critic = 1\n",
    "        \n",
    "\n",
    "    dense_layers_actor = []\n",
    "    for i in range(n_dense_layers_actor):\n",
    "        dense_layers_actor.append(trials.hyperparameters.values['dense_units_act_'+str(i)])\n",
    "\n",
    "    dense_layers_critic = []\n",
    "    for i in range(n_dense_layers_critic):\n",
    "        dense_layers_critic.append(trials.hyperparameters.values['dense_units_crit_'+str(i)])\n",
    "\n",
    "    \n",
    "    n_dense_layers_model = 1\n",
    "    dense_layers_model = []\n",
    "    for i in range(n_dense_layers_model):\n",
    "        dense_layers_model.append(trials.hyperparameters.values['n_dense_layers_model'+str(i)])\n",
    "\n",
    "    model = run_training(\n",
    "        training_steps = training_steps,   \n",
    "            discount = discount,\n",
    "            dense_units_act = dense_layers_actor, \n",
    "            dense_units_crit = dense_layers_critic,\n",
    "            dense_units_model = dense_layers_model,\n",
    "            num_layer_a = n_dense_layers_actor,\n",
    "            num_layer_c = n_dense_layers_critic,\n",
    "            num_layer_m = n_dense_layers_model,\n",
    "            writer = writer,  \n",
    "            save_factor=50000, \n",
    "            sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, \n",
    "            sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "            environment_name=ENV,\n",
    "            reward_scaler = 1, \n",
    "            evaluation_epoch = env._max_episode_steps,\n",
    "            return_agent = True,\n",
    "            lr_actor= lr_actor, \n",
    "            lr_critic= lr_critic,\n",
    "            lr_model= lr_model,\n",
    "            gae_lambda=gae,\n",
    "            training_epoch= 200,\n",
    "            entropy_coeff= entropy_factor,\n",
    "            policy_clip = policy_clip,\n",
    "            memory_size= env._max_episode_steps,\n",
    "            id = int(trials.trial_id))\n",
    "        \n",
    "final_rewards = final_evaluation(model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_guided_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
