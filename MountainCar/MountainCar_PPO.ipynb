{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 14:52:07.165131: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Selected port: 53767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 14:52:09.727176: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:53767/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_env_model import * #PPO_Agent_v2 PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"MountainCarContinuous-v0\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<Continuous_MountainCarEnv<MountainCarContinuous-v0>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92.15160670082223, 26.840920728403027)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "# Initialize an empty list to store rewards\n",
    "rewards = []\n",
    "\n",
    "# Run episodes and collect rewards\n",
    "for episode in range(100000):\n",
    "    done = False\n",
    "    while not done:\n",
    "        observation, reward, done, info = env.step(env.action_space.sample())\n",
    "        rewards.append(reward)\n",
    "\n",
    "# Calculate normalization factor (e.g., mean or standard deviation)\n",
    "mean_reward = np.mean(rewards)\n",
    "std_dev_reward = np.std(rewards)\n",
    "mean_reward, std_dev_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47566143, 0.03575775], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1.2  -0.07], [0.6  0.07], (2,), float32), 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5166163"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"logs_hyper/fit_PPO_deep_dive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Hyperparam_kt_ppo/keras_tunning_ppo_deep_dive/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING: \n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo_deep_dive\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_deep_dive/\", evaluation_epoch = env._max_episode_steps, training_steps = 1000000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                discount_min = 0.96, discount_max = 0.99, \n",
    "                #discount= 0.99, # WAS THIS\n",
    "                gae_factor = 0.95,  # WAS THIS\n",
    "                #gae_min = 0.92, gae_max = 0.96, \n",
    "                policy_clip =0.2,\n",
    "                lr_actor_crit_min = 0.0005, lr_actor_crit_max = 0.005,\n",
    "                #entropy_factor = 0.05,  # WAS THIS\n",
    "                entropy_factor_min = 0.001, entropy_factor_max = 0.2,\n",
    "                lr_model_min = 0.0001, lr_model_max =  0.001, kl_divergence_target = 0.1, ##### Try 0.1 after##############\n",
    "                #dense_layers = [42,62],\n",
    "                dense_min = 32, dense_max = 128, #WAS THIS ONE\n",
    "                environment_name=ENV, num_layers_act = 2, #max_num_layers_act = 2\n",
    "                num_layers_model = 1,  # WAS 2\n",
    "                training_epoch = 1,\n",
    "                memory_size = env._max_episode_steps, \n",
    "                normalize_reward=False, normalize_advantage= False,\n",
    "                scaling_factor_reward = 0.1\n",
    "                #memory_size_max= env._max_episode_steps\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 60,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name,\n",
    "            #seed=0\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "\n",
    "        dense_units_act_crit = [128,64]\n",
    "        num_layers_actor_critic = len(dense_units_act_crit)\n",
    "        num_layer_m = 1\n",
    "        dense_units_model = [32]\n",
    "\n",
    "        model = run_training(training_steps = 120000,\n",
    "                            discount = 0.99, \n",
    "                            dense_units_act_crit = dense_units_act_crit,  \n",
    "                            dense_units_model = dense_units_model,  \n",
    "                            num_layer_a_c = num_layers_actor_critic,  \n",
    "                            num_layer_m = num_layer_m, \n",
    "                            writer = writer, \n",
    "                            environment_name = ENV, \n",
    "                            return_agent = True, \n",
    "                            lr_actor_critic= 0.0001,  \n",
    "                            lr_model = 0.001,\n",
    "                            sucess_criteria_epochs=SUCESS_CRITERIA_EPOCH, \n",
    "                            sucess_criteria_value = SUCESS_CRITERIA_VALUE, \n",
    "                            gae_lambda = 0.95, \n",
    "                            entropy_coeff = 0.05, \n",
    "                            policy_clip = 0.2, training_epoch = 20, \n",
    "                            scaling_factor_reward = 0.1, \n",
    "                            kl_divergence_target = 0.1,\n",
    "                            memory_size =  env._max_episode_steps,\n",
    "                            normalize_reward=False, normalize_advantage= False, use_mlflow = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial id :33 | Score :43.05638682043875 --> {'discount': 0.96, 'lr_actor_critic': 0.004567871160273846, 'lr_model': 0.0002748035222914486, 'entropy_coeff': 0.0785056939938031, 'dense_units_act_crit_0': 52, 'dense_units_act_crit_1': 119, 'n_dense_layers_model0': 51}\n",
      "Trial id :35 | Score :21.429890302897984 --> {'discount': 0.96, 'lr_actor_critic': 0.004567871160273846, 'lr_model': 0.0002748035222914486, 'entropy_coeff': 0.0785056939938031, 'dense_units_act_crit_0': 52, 'dense_units_act_crit_1': 119, 'n_dense_layers_model0': 51}\n",
      "Trial id :23 | Score :-1.8389723276779273 --> {'discount': 0.97, 'lr_actor_critic': 0.0036705858845969106, 'lr_model': 0.0001, 'entropy_coeff': 0.001, 'dense_units_act_crit_0': 101, 'dense_units_act_crit_1': 103, 'n_dense_layers_model0': 88}\n"
     ]
    }
   ],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=3):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial number :  60\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model.next_state.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model.next_state.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.d_state_value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.d_state_value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.mean_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.mean_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.sigma_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.sigma_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model.dense_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.dense_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.dense_layers.1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic.dense_layers.1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.d_state_value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.d_state_value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.mean_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.mean_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.sigma_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.sigma_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.dense_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.dense_layers.1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'm' for (root).actor_critic.dense_layers.1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.d_state_value.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.d_state_value.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.mean_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.mean_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.sigma_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.sigma_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.dense_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.dense_layers.1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).actor_critic_optimizer's state 'v' for (root).actor_critic.dense_layers.1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer's state 'm' for (root).env_model.next_state.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer's state 'm' for (root).env_model.next_state.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer's state 'm' for (root).env_model.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer's state 'm' for (root).env_model.dense_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer's state 'v' for (root).env_model.next_state.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer's state 'v' for (root).env_model.next_state.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer's state 'v' for (root).env_model.dense_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).env_model_optimizer's state 'v' for (root).env_model.dense_layers.0.bias\n"
     ]
    }
   ],
   "source": [
    "env_model = tuner.get_best_models()[0]\n",
    "val_env = gym.make(ENV)#, render_mode = \"rgb_array\"\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_intrinsic_\"+exploration_tech+\"_video.mp4\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
