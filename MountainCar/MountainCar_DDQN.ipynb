{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 22:05:21.515969: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from DDQN_Agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    main_hyper_dir = \"D:\\\\Artificial_Intelligence\\\\Portfolio\\\\RL_updated\\\\MountainCar\\\\\" # Windows\n",
    "    conda_python_exec = 'C:\\\\Users\\\\yanie\\\\anaconda3\\\\envs\\\\ai_dev\\\\python.exe '# Windows\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning\\\\' # Windows\n",
    "else:\n",
    "    main_hyper_dir = \"/media/n/NewDisk/Artificial_Intelligence/Portfolio/RL_updated/MountainCar/\" # Linux\n",
    "    conda_python_exec = '/home/n/anaconda3/envs/ai_dev/bin/python '# Linux\n",
    "    logs_dir = main_hyper_dir+'Hyperparam_tuning_ddqn/' # Linux\n",
    "\n",
    "ENV = \"MountainCar-v0\"\n",
    "SUCESS_CRITERIA_VALUE = -100\n",
    "SUCESS_CRITERIA_EPOCH = 100\n",
    "EPISODES = 1000   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<MountainCarEnv<MountainCar-v0>>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(ENV)#,new_step_api=True\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.19794798,  0.05583682], dtype=float32), (2,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s, s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1.2  -0.07], [0.6  0.07], (2,), float32), (2,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " Discrete(3),\n",
       " (array([-4.6636066e-01, -4.3035913e-04], dtype=float32),\n",
       "  -1.0,\n",
       "  False,\n",
       "  False,\n",
       "  {}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.action_space.n, env.action_space, env.step(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs_general/hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"Training/fit_DDQN/\"\n",
    "\n",
    "\n",
    "if TUNING_TYPE == \"MANUAL\":\n",
    "    params = {}\n",
    "    params[\"discount\"] =[0.94, 0.99]\n",
    "    params[\"end_of_episode\"] = [400]\n",
    "    params[\"learning_rate\"] = [1e-6, 0.001]\n",
    "    params[\"tau_update_network\"] = [0.001, 1e-4]\n",
    "    params[\"exploration_technique\"] = [ 'epsilon', 'boltzman' ]\n",
    "    params[\"training_steps\"] = [3000000]\n",
    "    params[\"dense_units\"] = [74,32]\n",
    "    params[\"time_to_update\"] = [500] #400,\n",
    "\n",
    "    hyperparam_combination = list(itertools.product(*list(params.values())))\n",
    "\n",
    "    try:\n",
    "\n",
    "        files = [name for name in os.listdir(logs_dir) if os.path.isfile(os.path.join(logs_dir, name)) and name != \"logfile.txt\" and name != \"merged_results.json\"]\n",
    "        if len(files) >= 1: merge_JsonFiles(main_hyper_dir, logs_dir, files)\n",
    "\n",
    "        res_file = logs_dir+\"merged_results.json\"\n",
    "        def without_keys(d, keys):\n",
    "            return {x: d[x] for x in d if x not in keys}\n",
    "\n",
    "\n",
    "        if os.path.isfile(res_file):\n",
    "            with open(res_file, 'r') as f:\n",
    "                complete_file = json.load(f)\n",
    "\n",
    "            newlist = sorted(complete_file, key=lambda d: d['mean_rewards'], reverse=True) \n",
    "            params = []\n",
    "            for i, f in enumerate(newlist):\n",
    "                label = \"disc : \"+str(f['discount'])+\" | \"+\"lr : \"+str(f['learning_rate'])+\" | \"+\"entropy : \"+str(f['entropy_factor'])+\" | \"+\"update : \"+str(f['time_to_update'])\n",
    "                plt.figure(figsize=[35,4])\n",
    "                plt.plot(f['rewards'], label = label)\n",
    "                plt.legend()\n",
    "                max_mean_reward = f['mean_rewards']\n",
    "                params.append(f)\n",
    "                print(without_keys(f,\"rewards\"))\n",
    "                if i == 10:\n",
    "                    break\n",
    "            plt.title(\"Evaluation rewards\"); plt.grid();\n",
    "            plt.show()\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from Hyperparam_kt_DDQN/keras_tunning_epsilon/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING:\n",
    "\n",
    "    dir = r\"Hyperparam_kt_DDQN\"\n",
    "    project_name = \"keras_tunning_epsilon\"\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/DDQN_epsilon/\", exploration_tech ='epsilon' ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 50,\n",
    "            # distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "            directory= dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "\n",
    "    # project_name = \"keras_tunning_boltzman\"\n",
    "    # tuner = kt.BayesianOptimization(\n",
    "    #         MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyperV2/DDQN_boltzman/\", exploration_tech ='boltzman' ),\n",
    "    #         objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "    #         max_trials = 20,\n",
    "    #         # distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    #         directory= dir,\n",
    "    #         project_name=project_name\n",
    "    #     )\n",
    "    # tuner.search(x=[0], y=[1])\n",
    "   \n",
    "else : \n",
    "    print(\"Acquiring parameters ....\")\n",
    "    # run_training(training_steps, learning_rate, tau_update_network, exploration_tech, discount, time_to_update, dense_units, writer, end_of_episode)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from Hyperparam_kt_DDQN/keras_tunning_epsilon/tuner0.json\n",
      "Trial id :37 | Score :-135.6 --> {'discount': 0.99, 'learning_rate': 0.004943338266926713, 'time_to_update': 800, 'dense_units': 116, 'tau_update_network': 0.1}\n",
      "Trial id :18 | Score :-136.02 --> {'discount': 0.99, 'learning_rate': 0.0030296137455032425, 'time_to_update': 800, 'dense_units': 160, 'tau_update_network': 0.025184133983643338}\n",
      "Trial id :33 | Score :-137.36 --> {'discount': 0.99, 'learning_rate': 0.0031508505504027704, 'time_to_update': 700, 'dense_units': 109, 'tau_update_network': 0.1}\n"
     ]
    }
   ],
   "source": [
    "dir = r\"Hyperparam_kt_DDQN\"\n",
    "exploration_tech=\"epsilon\"\n",
    "\n",
    "project_name = \"keras_tunning_epsilon\"\n",
    "tuner = kt.BayesianOptimization(\n",
    "        MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/DDQN_epsilon/\", exploration_tech ='epsilon' ),\n",
    "        objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "        max_trials = 50,\n",
    "        # distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "        directory= dir,\n",
    "        project_name=project_name\n",
    "    )\n",
    "\n",
    "hyperparam_combination =[]\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=3):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "\n",
    "    disc = trials.hyperparameters.values[\"discount\"]\n",
    "    end_ep = 1000\n",
    "    train_steps = 5000000\n",
    "    d = trials.hyperparameters.values[\"dense_units\"]\n",
    "    lr = trials.hyperparameters.values[\"learning_rate\"]\n",
    "    time_to_update = trials.hyperparameters.values[\"time_to_update\"]\n",
    "    tau_update_network = trials.hyperparameters.values[\"tau_update_network\"]\n",
    "\n",
    "    hyperparam_combination.append((disc, end_ep, lr, tau_update_network, exploration_tech, train_steps, d, time_to_update))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " [(0.99, 1000, 0.004943338266926713, 0.1, 'epsilon', 5000000, 116, 800),\n",
       "  (0.99,\n",
       "   1000,\n",
       "   0.0030296137455032425,\n",
       "   0.025184133983643338,\n",
       "   'epsilon',\n",
       "   5000000,\n",
       "   160,\n",
       "   800),\n",
       "  (0.99, 1000, 0.0031508505504027704, 0.1, 'epsilon', 5000000, 109, 700)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyperparam_combination), hyperparam_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_hyperparam(total_files_number = 4, TUNING_TYPE= \"MANUAL\",  logs_dir=logs_dir,  main_hyper_dir = main_hyper_dir, conda_python_exec= conda_python_exec, py_file = \"mountaincar_DDQN.py\" ,TUNING_TYPE = TUNING_TYPE, hyperparam_combination = hyperparam_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from Hyperparam_kt_DDQN/keras_tunning_epsilon/tuner0.json\n",
      "Trial id :37 | Score :-135.6 --> {'discount': 0.99, 'learning_rate': 0.004943338266926713, 'time_to_update': 800, 'dense_units': 116, 'tau_update_network': 0.1}\n",
      "Trial number :  50\n",
      "Moviepy - Building video ./DDQN_epsilon_video.mp4.\n",
      "Moviepy - Writing video ./DDQN_epsilon_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./DDQN_epsilon_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mean reward ' epsilon ': -138.6\n",
      "INFO:tensorflow:Reloading Tuner from Hyperparam_kt_DDQN/keras_tunning_boltzman/tuner0.json\n",
      "Trial id :20 | Score :-124.56 --> {'discount': 0.99, 'learning_rate': 0.001763393784940077, 'time_to_update': 600, 'dense_units': 299, 'tau_update_network': 0.07370751378110506}\n",
      "Trial number :  50\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).Target_DQN_agent.d0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).Target_DQN_agent.d0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).Target_DQN_agent.Qvalues.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).Target_DQN_agent.Qvalues.bias\n",
      "Moviepy - Building video ./DDQN_boltzman_video.mp4.\n",
      "Moviepy - Writing video ./DDQN_boltzman_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./DDQN_boltzman_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mean reward ' boltzman ': -124.92\n"
     ]
    }
   ],
   "source": [
    "val_env = gym.make(ENV, render_mode = \"rgb_array\")\n",
    "dir = r\"Hyperparam_kt_DDQN\"\n",
    "\n",
    "for exploration_tech in [\"epsilon\", \"boltzman\"]:\n",
    "\n",
    "    project_name = \"keras_tunning_\"+exploration_tech\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/DDQN_\"+exploration_tech+\"/\", exploration_tech =exploration_tech),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 50,\n",
    "            directory= dir,\n",
    "            project_name=project_name\n",
    "        )\n",
    "\n",
    "    for trials in tuner.oracle.get_best_trials(num_trials=1):\n",
    "        print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)\n",
    "        \n",
    "\n",
    "    env_model = tuner.get_best_models()[0]\n",
    "    final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./DDQN_\"+exploration_tech+\"_video.mp4\")\n",
    "    print(\"Final mean reward '\",exploration_tech,\"':\", np.mean(final_rewards))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e97097829de60fff0360ae9237ac6de2911bdcd7f22e62fb944aef663864db87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
