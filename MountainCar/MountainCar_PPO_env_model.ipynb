{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking enviroment states and action sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 08:59:10.459054: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices available:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Selected port: 60063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 08:59:11.926532: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.10.1 at http://localhost:60063/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append(\"..\")\n",
    "from PPO_Agent_env_model import * #PPO_Agent_v2 PPO_Agent_with_Guided_AC\n",
    "from ENV_DETAILS import *\n",
    "from RUN_TENSORBOARD import *\n",
    "\n",
    "events_folder = \"./logs_hyper\"\n",
    "main(\"./logs_hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"MountainCarContinuous-v0\"\n",
    "SUCESS_CRITERIA_VALUE = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_VALUE\"]\n",
    "SUCESS_CRITERIA_EPOCH = ENV_DETAILS[ENV][\"SUCESS_CRITERIA_EPOCH\"]\n",
    "\n",
    "EPISODES = ENV_DETAILS[ENV][\"EPISODES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<Continuous_MountainCarEnv<MountainCarContinuous-v0>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = gym.make(ENV)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.08909131117508, 44.83980253291923)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "# Initialize an empty list to store rewards\n",
    "rewards = []\n",
    "\n",
    "# Run episodes and collect rewards\n",
    "for episode in range(100000):\n",
    "    done = False\n",
    "    while not done:\n",
    "        observation, reward, done, info = env.step(env.action_space.sample())\n",
    "        rewards.append(reward)\n",
    "\n",
    "# Calculate normalization factor (e.g., mean or standard deviation)\n",
    "mean_reward = np.mean(rewards)\n",
    "std_dev_reward = np.std(rewards)\n",
    "mean_reward, std_dev_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31957495, -0.03074787], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.observation_space.sample()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box([-1.2  -0.07], [0.6  0.07], (2,), float32), 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.594648"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Hyperparam run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNING_TYPE = \"BAYES\"\n",
    "HYPERPARAM_TUNING = True\n",
    "writer= \"logs_hyper/fit_PPO_env_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Complete [01h 22m 51s]\n",
      "total_train_reward: -38.70161461742905\n",
      "\n",
      "Best total_train_reward So Far: -18.00936588068234\n",
      "Total elapsed time: 1d 04h 39m 59s\n",
      "\n",
      "Search: Running Trial #10\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.92              |0.95              |gae_lambda\n",
      "0.00052156        |0.00062083        |lr_actor_critic\n",
      "0.00021605        |0.00028073        |lr_model\n",
      "0.036902          |0.44401           |entropy_coeff\n",
      "137               |181               |n_dense_layers_model0\n",
      "0.044857          |0.1191            |kl_divergence_target\n",
      "\n",
      "Trial number :  10\n",
      "Epoch: 2000 : Reward eval/Train: -33.864097517911304/-33.926706032003025 \n",
      "Epoch: 4000 : Reward eval/Train: -33.57852362108403/-34.015668226595935 \n",
      "Epoch: 6000 : Reward eval/Train: -33.72551417967788/-34.07141546409457 \n",
      "Epoch: 8000 : Reward eval/Train: -33.54683640055913/-33.84707524130174 \n",
      "Epoch: 10000 : Reward eval/Train: -33.674725430324784/-33.712454601253896 \n",
      "Epoch: 12000 : Reward eval/Train: -33.57885260933984/-33.63902526611504 \n",
      "Epoch: 14000 : Reward eval/Train: -33.608886063842604/-33.666393750105115 \n",
      "Epoch: 16000 : Reward eval/Train: -33.58903954865063/-33.819505223271456 \n",
      "Epoch: 18000 : Reward eval/Train: -33.62207832652914/-33.97581901260616 \n",
      "Epoch: 20000 : Reward eval/Train: -33.65381047372615/-34.07656759217342 \n",
      "Epoch: 22000 : Reward eval/Train: -33.61708189694963/-34.10987072902391 \n",
      "Epoch: 24000 : Reward eval/Train: -33.794872999869604/-34.104720977346 \n",
      "Epoch: 26000 : Reward eval/Train: -33.81702171528412/-34.18895603660028 \n",
      "Epoch: 28000 : Reward eval/Train: -33.92831763056697/-34.2228592681812 \n",
      "Epoch: 30000 : Reward eval/Train: -33.990991629310344/-34.186364232076265 \n",
      "Epoch: 32000 : Reward eval/Train: -33.97311326262779/-34.28209237717532 \n",
      "Epoch: 34000 : Reward eval/Train: -34.029757523920765/-34.36529665515372 \n",
      "Epoch: 36000 : Reward eval/Train: -33.95766101260771/-34.351347573631365 \n",
      "Epoch: 38000 : Reward eval/Train: -33.983528456922066/-34.51435457244214 \n",
      "Epoch: 40000 : Reward eval/Train: -33.99837646816757/-34.57822857231108 \n",
      "Epoch: 42000 : Reward eval/Train: -34.0749702187962/-34.664031115944276 \n",
      "Epoch: 44000 : Reward eval/Train: -34.13882909889222/-34.636347504048835 \n",
      "Epoch: 46000 : Reward eval/Train: -34.12395342412003/-34.660631178407414 \n",
      "Epoch: 48000 : Reward eval/Train: -34.18587419386803/-34.695157753805006 \n",
      "Epoch: 50000 : Reward eval/Train: -34.25839753148298/-34.72896490369223 \n",
      "Epoch: 52000 : Reward eval/Train: -34.35521495439926/-34.847335728305694 \n",
      "Epoch: 54000 : Reward eval/Train: -34.41280454595379/-34.888575313735814 \n",
      "Epoch: 56000 : Reward eval/Train: -34.52907568097585/-34.99674394733921 \n",
      "Epoch: 58000 : Reward eval/Train: -34.6404824162924/-35.03456254402496 \n",
      "Epoch: 60000 : Reward eval/Train: -34.71263427980545/-35.11876862249785 \n",
      "Epoch: 62000 : Reward eval/Train: -34.83187584561639/-35.13315521134991 \n",
      "Epoch: 64000 : Reward eval/Train: -34.94240374879874/-35.22323678793706 \n",
      "Epoch: 66000 : Reward eval/Train: -35.04890366196498/-35.26112074472841 \n",
      "Epoch: 68000 : Reward eval/Train: -35.13574401848503/-35.370477573524674 \n",
      "Epoch: 70000 : Reward eval/Train: -35.24534093132524/-35.42184094449394 \n",
      "Epoch: 72000 : Reward eval/Train: -35.31971539537895/-35.53751229971172 \n",
      "Epoch: 74000 : Reward eval/Train: -35.443234836626765/-35.60497813301431 \n",
      "Epoch: 76000 : Reward eval/Train: -35.58780601023343/-35.70678985577505 \n",
      "Epoch: 78000 : Reward eval/Train: -35.69343361004374/-35.80622740455588 \n",
      "Epoch: 80000 : Reward eval/Train: -35.853993250203565/-35.9486574293975 \n",
      "Epoch: 82000 : Reward eval/Train: -36.00379251850079/-36.10030970083788 \n",
      "Epoch: 84000 : Reward eval/Train: -36.20287016848106/-36.245858730075284 \n",
      "Epoch: 86000 : Reward eval/Train: -36.35058160254378/-36.39884381854163 \n",
      "Epoch: 88000 : Reward eval/Train: -36.52270191644161/-36.55591218362982 \n",
      "Epoch: 90000 : Reward eval/Train: -36.717926041455215/-36.706693908322094 \n",
      "Epoch: 92000 : Reward eval/Train: -36.92257613141305/-36.92539303601473 \n",
      "Epoch: 94000 : Reward eval/Train: -37.16014681112754/-37.1051825884883 \n",
      "Epoch: 96000 : Reward eval/Train: -37.3768161630023/-37.33769328690096 \n",
      "Epoch: 98000 : Reward eval/Train: -37.60543475917079/-37.558542824062805 \n",
      "Epoch: 100000 : Reward eval/Train: -37.85023787654148/-37.8209375947833 \n",
      "Epoch: 102000 : Reward eval/Train: -38.13305474634513/-38.064555617366956 \n",
      "Epoch: 104000 : Reward eval/Train: -38.43144120306188/-38.319231809221584 \n",
      "Epoch: 106000 : Reward eval/Train: -38.703173307952/-38.61739696523223 \n",
      "Epoch: 108000 : Reward eval/Train: -39.012823526538874/-38.9218252495346 \n",
      "Epoch: 110000 : Reward eval/Train: -39.30704951959878/-39.211541048219864 \n",
      "Epoch: 112000 : Reward eval/Train: -39.645519871246066/-39.51087924492559 \n",
      "Epoch: 114000 : Reward eval/Train: -39.96243615141969/-39.853002824816514 \n",
      "Epoch: 116000 : Reward eval/Train: -40.32222299765771/-40.15862326635991 \n",
      "Epoch: 118000 : Reward eval/Train: -40.67239907275287/-40.465672845928296 \n",
      "Epoch: 120000 : Reward eval/Train: -41.00931062895224/-40.81349932533393 \n",
      "Epoch: 122000 : Reward eval/Train: -41.37385340791474/-41.163370043684765 \n",
      "Epoch: 124000 : Reward eval/Train: -41.7245655509928/-41.50150869360522 \n",
      "Epoch: 126000 : Reward eval/Train: -42.0847464495892/-41.86391781833761 \n",
      "Epoch: 128000 : Reward eval/Train: -42.45004589207668/-42.213082041424826 \n",
      "Epoch: 130000 : Reward eval/Train: -42.787940164438254/-42.55116011351902 \n",
      "Epoch: 132000 : Reward eval/Train: -43.14679621771431/-42.87585956450492 \n",
      "Epoch: 134000 : Reward eval/Train: -43.50978363907833/-43.232673963500794 \n",
      "Epoch: 136000 : Reward eval/Train: -43.88381941262211/-43.574621505085986 \n",
      "Epoch: 138000 : Reward eval/Train: -44.24166386429453/-43.94689567807004 \n",
      "Epoch: 140000 : Reward eval/Train: -44.609400105744236/-44.30187022975061 \n",
      "Epoch: 142000 : Reward eval/Train: -44.95533528040011/-44.65693256983023 \n",
      "Epoch: 144000 : Reward eval/Train: -45.306369733411366/-45.01571055388974 \n",
      "Epoch: 146000 : Reward eval/Train: -45.6702787752286/-45.373311971473385 \n",
      "Epoch: 148000 : Reward eval/Train: -46.02613681995239/-45.708034029264 \n",
      "Epoch: 150000 : Reward eval/Train: -46.383054168169295/-46.05874402901362 \n",
      "Epoch: 152000 : Reward eval/Train: -46.72029535325065/-46.39350470863538 \n",
      "Epoch: 154000 : Reward eval/Train: -47.07248704481259/-46.73473356595743 \n",
      "Epoch: 156000 : Reward eval/Train: -47.423981705607524/-47.0689384672647 \n",
      "Epoch: 158000 : Reward eval/Train: -47.770757638037175/-47.41201857065606 \n",
      "Epoch: 160000 : Reward eval/Train: -48.1144635009437/-47.77171611363962 \n",
      "Epoch: 162000 : Reward eval/Train: -48.4558092915814/-48.08183730744179 \n",
      "Epoch: 164000 : Reward eval/Train: -48.7755505716869/-48.42399602429124 \n",
      "Epoch: 166000 : Reward eval/Train: -49.107737045045376/-48.74539829089868 \n",
      "Epoch: 168000 : Reward eval/Train: -49.4329983928478/-49.102370429786454 \n",
      "Epoch: 170000 : Reward eval/Train: -49.76599061151987/-49.43744538287012 \n",
      "Epoch: 172000 : Reward eval/Train: -50.09615176087027/-49.75820690856053 \n",
      "Epoch: 174000 : Reward eval/Train: -50.41942369077298/-50.09205452279448 \n",
      "Epoch: 176000 : Reward eval/Train: -50.74757799066647/-50.42505263513326 \n",
      "Epoch: 178000 : Reward eval/Train: -51.072396865060156/-50.746337916361625 \n",
      "Epoch: 180000 : Reward eval/Train: -51.3957437463257/-51.06281843267152 \n",
      "Epoch: 182000 : Reward eval/Train: -51.712361714023515/-51.383574799505816 \n",
      "Epoch: 184000 : Reward eval/Train: -52.00752512106868/-51.670666971056285 \n",
      "Epoch: 186000 : Reward eval/Train: -52.3336315911061/-51.98813061372457 \n",
      "Epoch: 188000 : Reward eval/Train: -52.639697460000264/-52.29277757066031 \n",
      "Epoch: 190000 : Reward eval/Train: -52.9347631004293/-52.593572119786586 \n",
      "Epoch: 192000 : Reward eval/Train: -53.23163047172901/-52.899824656572356 \n",
      "Epoch: 194000 : Reward eval/Train: -53.51616576282254/-53.20609141575243 \n",
      "Epoch: 196000 : Reward eval/Train: -53.82429964006615/-53.507083541341316 \n",
      "Epoch: 198000 : Reward eval/Train: -54.102615011450744/-53.782102329515865 \n",
      "Epoch: 200000 : Reward eval/Train: -54.379699114597734/-54.07528481858474 \n",
      "Epoch: 202000 : Reward eval/Train: -54.67332179692232/-54.37575467413026 \n",
      "Epoch: 204000 : Reward eval/Train: -54.960360650753124/-54.65629279629468 \n",
      "Epoch: 206000 : Reward eval/Train: -55.24754176913389/-54.9330021076138 \n",
      "Epoch: 208000 : Reward eval/Train: -55.5257897122848/-55.22045590726558 \n",
      "Epoch: 210000 : Reward eval/Train: -55.80449532872508/-55.505996727743735 \n",
      "Epoch: 212000 : Reward eval/Train: -56.08023835840573/-55.78093481258313 \n",
      "Epoch: 214000 : Reward eval/Train: -56.34951648915573/-56.05105735787145 \n",
      "Epoch: 216000 : Reward eval/Train: -56.6124435491412/-56.30857848460081 \n",
      "Epoch: 218000 : Reward eval/Train: -56.85861259934594/-56.56644989819397 \n",
      "Epoch: 220000 : Reward eval/Train: -57.11269846381669/-56.830849754080816 \n",
      "Epoch: 222000 : Reward eval/Train: -57.37853361959866/-57.08873128338122 \n",
      "Epoch: 224000 : Reward eval/Train: -57.632941934787596/-57.33278172053148 \n",
      "Epoch: 226000 : Reward eval/Train: -57.86790093550405/-57.582651889444136 \n",
      "Epoch: 228000 : Reward eval/Train: -58.11403547003756/-57.82330468757225 \n",
      "Epoch: 230000 : Reward eval/Train: -58.35158223749354/-58.047896832208686 \n",
      "Epoch: 232000 : Reward eval/Train: -58.575737251572995/-58.28227222966213 \n",
      "Epoch: 234000 : Reward eval/Train: -58.808145767852565/-58.515336801593335 \n",
      "Epoch: 236000 : Reward eval/Train: -59.03176973393121/-58.74573793967398 \n",
      "Epoch: 238000 : Reward eval/Train: -59.2529884424092/-58.96858814850083 \n",
      "Epoch: 240000 : Reward eval/Train: -59.476355684377964/-59.18576093706277 \n",
      "Epoch: 242000 : Reward eval/Train: -59.70160529150088/-59.40119412792348 \n",
      "Epoch: 244000 : Reward eval/Train: -59.91343629695247/-59.61561525120684 \n",
      "Epoch: 246000 : Reward eval/Train: -60.124418242377395/-59.823975259748494 \n",
      "Epoch: 248000 : Reward eval/Train: -60.33503926750134/-60.04088481297537 \n",
      "Epoch: 250000 : Reward eval/Train: -60.54762279715193/-60.253721997779586 \n",
      "Epoch: 252000 : Reward eval/Train: -60.74642831174283/-60.46003157723639 \n",
      "Epoch: 254000 : Reward eval/Train: -60.94390737097347/-60.66211916093585 \n",
      "Epoch: 256000 : Reward eval/Train: -61.141361256301195/-60.86354869552678 \n",
      "Epoch: 258000 : Reward eval/Train: -61.334825226394585/-61.06053285187848 \n",
      "Epoch: 260000 : Reward eval/Train: -61.529510393694416/-61.24756494642029 \n",
      "Epoch: 262000 : Reward eval/Train: -61.72576157093938/-61.441240709978636 \n",
      "Epoch: 264000 : Reward eval/Train: -61.91239350312997/-61.63107940741802 \n",
      "Epoch: 266000 : Reward eval/Train: -62.09005719064254/-61.81733711706091 \n",
      "Epoch: 268000 : Reward eval/Train: -62.27278286022862/-62.00344890639923 \n",
      "Epoch: 270000 : Reward eval/Train: -62.44913663056856/-62.189927046600026 \n",
      "Epoch: 272000 : Reward eval/Train: -62.63039886106673/-62.364681113367276 \n",
      "Epoch: 274000 : Reward eval/Train: -62.80601313482026/-62.53368537739144 \n",
      "Epoch: 276000 : Reward eval/Train: -62.98251640980723/-62.71347159395004 \n",
      "Epoch: 278000 : Reward eval/Train: -63.15914291817221/-62.883064633170534 \n",
      "Epoch: 280000 : Reward eval/Train: -63.33498836793858/-63.047173363176576 \n",
      "Epoch: 282000 : Reward eval/Train: -63.505283970044204/-63.21462409556661 \n",
      "Epoch: 284000 : Reward eval/Train: -63.66044894792775/-63.37950955620632 \n",
      "Epoch: 286000 : Reward eval/Train: -63.81248816972555/-63.54723855315627 \n",
      "Epoch: 288000 : Reward eval/Train: -63.96595924088885/-63.70884575211163 \n",
      "Epoch: 290000 : Reward eval/Train: -64.12691478577577/-63.87800995321433 \n",
      "Epoch: 292000 : Reward eval/Train: -64.28171319985753/-64.03497616771955 \n",
      "Epoch: 294000 : Reward eval/Train: -64.44012336216377/-64.18817510394845 \n",
      "Epoch: 296000 : Reward eval/Train: -64.59148219293559/-64.34544657976636 \n",
      "Epoch: 298000 : Reward eval/Train: -64.74022305445142/-64.50086097283484 \n",
      "Epoch: 300000 : Reward eval/Train: -64.89508629911836/-64.64824428411137 \n",
      "Epoch: 302000 : Reward eval/Train: -65.04521299778087/-64.79930395594364 \n",
      "Epoch: 304000 : Reward eval/Train: -65.19025171051894/-64.94911772821449 \n",
      "Epoch: 306000 : Reward eval/Train: -65.33747653691916/-65.09388165632727 \n",
      "Epoch: 308000 : Reward eval/Train: -65.4801087496212/-65.23898666811701 \n",
      "Epoch: 310000 : Reward eval/Train: -65.62709971904572/-65.3876627530253 \n",
      "Epoch: 312000 : Reward eval/Train: -65.76496233354437/-65.52932029480144 \n",
      "Epoch: 314000 : Reward eval/Train: -65.9051213127069/-65.27988640286137 \n",
      "Epoch: 316000 : Reward eval/Train: -66.03918670501173/-65.41457745698459 \n",
      "Epoch: 318000 : Reward eval/Train: -66.1715620144742/-65.55385897664726 \n",
      "Epoch: 320000 : Reward eval/Train: -66.30623901930291/-65.69148990314116 \n",
      "Epoch: 322000 : Reward eval/Train: -66.43157477700018/-65.82067069382416 \n",
      "Epoch: 324000 : Reward eval/Train: -66.5547615011643/-65.94555977171149 \n",
      "Epoch: 326000 : Reward eval/Train: -66.67502538884594/-66.06670039118839 \n",
      "Epoch: 328000 : Reward eval/Train: -66.7889177456828/-66.19019756645413 \n",
      "Epoch: 330000 : Reward eval/Train: -66.90557895013023/-66.31683484902128 \n",
      "Epoch: 332000 : Reward eval/Train: -67.03030669602288/-66.44833496311325 \n",
      "Epoch: 334000 : Reward eval/Train: -67.14869067609366/-66.56461238097711 \n",
      "Epoch: 336000 : Reward eval/Train: -67.2652195934331/-66.68487208679129 \n",
      "Epoch: 338000 : Reward eval/Train: -67.38065962226128/-66.8007695694804 \n",
      "Epoch: 340000 : Reward eval/Train: -67.48876551595495/-66.91505386816534 \n",
      "Epoch: 342000 : Reward eval/Train: -67.59929022332956/-67.03825157603136 \n",
      "Epoch: 344000 : Reward eval/Train: -67.71091269573061/-67.1618400677051 \n",
      "Epoch: 346000 : Reward eval/Train: -67.81994619275652/-67.2810630737033 \n",
      "Epoch: 348000 : Reward eval/Train: -67.93464296510058/-67.3970182025754 \n",
      "Epoch: 350000 : Reward eval/Train: -68.05001516158235/-67.51214720326436 \n",
      "Epoch: 352000 : Reward eval/Train: -68.1620396967122/-67.62578226630231 \n",
      "Epoch: 354000 : Reward eval/Train: -68.27248646591458/-67.74059244042631 \n",
      "Epoch: 356000 : Reward eval/Train: -68.3867286093276/-67.85477202998665 \n",
      "Epoch: 358000 : Reward eval/Train: -68.49496269709113/-67.96926843012078 \n",
      "Epoch: 360000 : Reward eval/Train: -68.60502414127224/-68.07341647100385 \n",
      "Epoch: 362000 : Reward eval/Train: -68.71662070165091/-68.17602106257526 \n",
      "Epoch: 364000 : Reward eval/Train: -68.82060238558132/-68.28220325159634 \n",
      "Epoch: 366000 : Reward eval/Train: -68.93164354498522/-68.39558176706974 \n",
      "Epoch: 368000 : Reward eval/Train: -69.04121156708469/-68.50785832177463 \n",
      "Epoch: 370000 : Reward eval/Train: -69.14635778724775/-68.61043302847763 \n",
      "Epoch: 372000 : Reward eval/Train: -69.24994727799626/-68.71304601685986 \n",
      "Epoch: 374000 : Reward eval/Train: -69.35030630802208/-68.81735953558085 \n",
      "Epoch: 376000 : Reward eval/Train: -69.45084054600774/-68.92237064721813 \n",
      "Epoch: 378000 : Reward eval/Train: -69.5455062800563/-69.03128188539831 \n",
      "Epoch: 380000 : Reward eval/Train: -69.64445819227593/-69.13970994227049 \n",
      "Epoch: 382000 : Reward eval/Train: -69.74214202778602/-69.24263161051078 \n",
      "Epoch: 384000 : Reward eval/Train: -69.84073719939953/-69.34380972407436 \n",
      "Epoch: 386000 : Reward eval/Train: -69.93708952383616/-69.44201406667501 \n",
      "Epoch: 388000 : Reward eval/Train: -70.03712040486178/-69.53768875647185 \n",
      "Epoch: 390000 : Reward eval/Train: -70.13300176122614/-69.63369364942494 \n",
      "Epoch: 392000 : Reward eval/Train: -70.22750713641551/-69.72820467780248 \n",
      "Epoch: 394000 : Reward eval/Train: -70.32449126139818/-69.817927308343 \n",
      "Epoch: 396000 : Reward eval/Train: -70.41031899909416/-69.91184320904968 \n",
      "Epoch: 398000 : Reward eval/Train: -70.50191138697987/-70.00371712046713 \n",
      "Epoch: 400000 : Reward eval/Train: -70.5923793524143/-70.0953619480996 \n",
      "Epoch: 402000 : Reward eval/Train: -70.68147487957008/-70.18916641162055 \n",
      "Epoch: 404000 : Reward eval/Train: -70.77512630971529/-70.28277906548679 \n",
      "Epoch: 406000 : Reward eval/Train: -70.86059663201637/-70.37556196678872 \n",
      "Epoch: 408000 : Reward eval/Train: -70.69433220976823/-70.46769302754339 \n",
      "Epoch: 410000 : Reward eval/Train: -70.77886647867315/-70.55528910470056 \n",
      "Epoch: 412000 : Reward eval/Train: -70.86725446685692/-70.64582102145434 \n",
      "Epoch: 414000 : Reward eval/Train: -70.95407463577912/-70.73320372567474 \n",
      "Epoch: 416000 : Reward eval/Train: -71.04147965710322/-70.81916049084066 \n",
      "Epoch: 418000 : Reward eval/Train: -71.12949076983655/-70.90451831386181 \n",
      "Epoch: 420000 : Reward eval/Train: -71.21253564478835/-70.98932888832874 \n",
      "Epoch: 422000 : Reward eval/Train: -71.29629477108179/-71.0739648234272 \n",
      "Epoch: 424000 : Reward eval/Train: -71.3816254523301/-71.1558680074658 \n",
      "Epoch: 426000 : Reward eval/Train: -71.45937223753816/-71.2395821614978 \n",
      "Epoch: 428000 : Reward eval/Train: -71.54259058925442/-71.32308105067277 \n",
      "Epoch: 430000 : Reward eval/Train: -71.62508076249925/-71.39964466979653 \n",
      "Epoch: 432000 : Reward eval/Train: -71.70397603104082/-71.47623117929486 \n",
      "Epoch: 434000 : Reward eval/Train: -71.78236022709217/-71.55239641674912 \n",
      "Epoch: 436000 : Reward eval/Train: -71.8597968359097/-71.63066944490525 \n",
      "Epoch: 438000 : Reward eval/Train: -71.94187235204481/-71.7094051721805 \n",
      "Epoch: 440000 : Reward eval/Train: -72.02111353518687/-71.78366865619368 \n",
      "Epoch: 442000 : Reward eval/Train: -72.0965002905743/-71.85904417125552 \n",
      "Epoch: 444000 : Reward eval/Train: -72.17311503713157/-71.93600337783916 \n",
      "Epoch: 446000 : Reward eval/Train: -72.24422713424781/-72.00735587730082 \n",
      "Epoch: 448000 : Reward eval/Train: -72.31703957449984/-72.08408784370707 \n",
      "Epoch: 450000 : Reward eval/Train: -72.39169126961094/-72.15856899715989 \n",
      "Epoch: 452000 : Reward eval/Train: -72.46394766159572/-72.23532967088936 \n",
      "Epoch: 454000 : Reward eval/Train: -72.53779932136398/-72.31503171160023 \n",
      "Epoch: 456000 : Reward eval/Train: -72.61383455154152/-72.38766863980202 \n",
      "Epoch: 458000 : Reward eval/Train: -72.68492473125292/-72.45995813552656 \n",
      "Epoch: 460000 : Reward eval/Train: -72.75313754520963/-72.53590420565318 \n",
      "Epoch: 462000 : Reward eval/Train: -72.82595334846225/-72.61405872113956 \n",
      "Epoch: 464000 : Reward eval/Train: -72.89695271304672/-72.6878105247898 \n",
      "Epoch: 466000 : Reward eval/Train: -72.96752976304667/-72.7621712646534 \n",
      "Epoch: 468000 : Reward eval/Train: -73.0419180097502/-72.83274898273258 \n",
      "Epoch: 470000 : Reward eval/Train: -73.10959849655079/-72.90614318936603 \n",
      "Epoch: 472000 : Reward eval/Train: -73.17593558919877/-72.97923523301039 \n",
      "Epoch: 474000 : Reward eval/Train: -73.24110765236055/-73.04940816932299 \n",
      "Epoch: 476000 : Reward eval/Train: -73.3115169736792/-73.11598701536815 \n",
      "Epoch: 478000 : Reward eval/Train: -73.37903681595347/-73.18269013508356 \n",
      "Epoch: 480000 : Reward eval/Train: -73.4448819988751/-73.01965020213453 \n",
      "Epoch: 482000 : Reward eval/Train: -73.28232044665239/-73.08755938529225 \n",
      "Epoch: 484000 : Reward eval/Train: -73.34742656264996/-73.15544800644423 \n",
      "Epoch: 486000 : Reward eval/Train: -73.40720807663816/-73.2205655197945 \n",
      "Epoch: 488000 : Reward eval/Train: -73.47011787648232/-73.28381402781336 \n",
      "Epoch: 490000 : Reward eval/Train: -73.52945137338088/-73.34967958169162 \n",
      "Epoch: 492000 : Reward eval/Train: -73.59130141443183/-73.40898474206374 \n",
      "Epoch: 494000 : Reward eval/Train: -73.65679007526366/-73.46827597479826 \n",
      "Epoch: 496000 : Reward eval/Train: -73.71841444908034/-73.52767451673387 \n",
      "Epoch: 498000 : Reward eval/Train: -73.78156268028646/-73.59396819026178 \n",
      "Epoch: 500000 : Reward eval/Train: -73.83990961500076/-73.6574464796164 \n",
      "Epoch: 502000 : Reward eval/Train: -73.89823574089851/-73.71900217227888 \n",
      "Epoch: 504000 : Reward eval/Train: -73.95741380801043/-73.77897231482379 \n",
      "Epoch: 506000 : Reward eval/Train: -74.01852801473022/-73.84103529956153 \n",
      "Epoch: 508000 : Reward eval/Train: -74.08191720067711/-73.90399630674675 \n",
      "Epoch: 510000 : Reward eval/Train: -74.14484702328407/-73.96234070367923 \n",
      "Epoch: 512000 : Reward eval/Train: -74.2036534616002/-74.02493428597084 \n",
      "Epoch: 514000 : Reward eval/Train: -74.26433347952212/-74.08618609110529 \n",
      "Epoch: 516000 : Reward eval/Train: -74.32601933702821/-74.14225528569393 \n",
      "Epoch: 518000 : Reward eval/Train: -74.3838154183781/-74.20132411576165 \n",
      "Epoch: 520000 : Reward eval/Train: -74.44375995330223/-74.06488270394551 \n",
      "Epoch: 522000 : Reward eval/Train: -74.23035265477108/-74.12589814638959 \n",
      "Epoch: 524000 : Reward eval/Train: -74.28396572219893/-74.18515362395831 \n",
      "Epoch: 526000 : Reward eval/Train: -74.33963655964709/-74.23911971278854 \n",
      "Epoch: 528000 : Reward eval/Train: -74.39711677728178/-74.2971067684822 \n",
      "Epoch: 530000 : Reward eval/Train: -74.45460665277987/-74.3538205482719 \n",
      "Epoch: 532000 : Reward eval/Train: -74.51061027556284/-74.41002876096991 \n",
      "Epoch: 534000 : Reward eval/Train: -74.57027245370324/-74.46757818978381 \n",
      "Epoch: 536000 : Reward eval/Train: -74.62519255829031/-74.52292817743921 \n",
      "Epoch: 538000 : Reward eval/Train: -74.67532646915339/-74.58214817880437 \n",
      "Epoch: 540000 : Reward eval/Train: -74.72764424209053/-74.63911970777531 \n",
      "Epoch: 542000 : Reward eval/Train: -74.78537895746075/-74.69265327537597 \n",
      "Epoch: 544000 : Reward eval/Train: -74.83958596434468/-74.74526924891387 \n",
      "Epoch: 546000 : Reward eval/Train: -74.89482784729388/-74.8010246461586 \n",
      "Epoch: 548000 : Reward eval/Train: -74.94747223961798/-74.85699018575984 \n",
      "Epoch: 550000 : Reward eval/Train: -75.00420345425111/-74.91069179665398 \n",
      "Epoch: 552000 : Reward eval/Train: -75.05843419084908/-74.96819269121856 \n",
      "Epoch: 554000 : Reward eval/Train: -75.11417815157199/-75.02160980988502 \n",
      "Epoch: 556000 : Reward eval/Train: -75.17087204018442/-75.07775806027308 \n",
      "Epoch: 558000 : Reward eval/Train: -75.2256383161473/-75.13111930897742 \n",
      "Epoch: 560000 : Reward eval/Train: -75.279217687866/-75.17952852643813 \n",
      "Epoch: 562000 : Reward eval/Train: -75.33308946649953/-75.23176367273079 \n",
      "Epoch: 564000 : Reward eval/Train: -75.38686817134005/-75.28205281921873 \n",
      "Epoch: 566000 : Reward eval/Train: -75.44100993571787/-75.3352469331026 \n",
      "Epoch: 568000 : Reward eval/Train: -75.49652957716918/-75.38582998118395 \n",
      "Epoch: 570000 : Reward eval/Train: -75.54833558898702/-75.43738574801593 \n",
      "Epoch: 572000 : Reward eval/Train: -75.60045909087056/-75.48818635199042 \n",
      "Epoch: 574000 : Reward eval/Train: -75.64998140894052/-75.53960441262286 \n",
      "Epoch: 576000 : Reward eval/Train: -75.70075959525089/-75.58897259772912 \n",
      "Epoch: 578000 : Reward eval/Train: -75.52192621707734/-75.63942074198437 \n",
      "Epoch: 580000 : Reward eval/Train: -75.57415417590796/-75.69252866902654 \n",
      "Epoch: 582000 : Reward eval/Train: -75.42446631111943/-75.74147656543235 \n",
      "Epoch: 584000 : Reward eval/Train: -75.47615283664734/-75.78963773673206 \n",
      "Epoch: 586000 : Reward eval/Train: -75.52825561200748/-75.83903387648861 \n",
      "Epoch: 588000 : Reward eval/Train: -75.58130662592283/-75.88781079578473 \n",
      "Epoch: 590000 : Reward eval/Train: -75.63027698724025/-75.93975078300603 \n",
      "Epoch: 592000 : Reward eval/Train: -75.67964493295528/-75.98781547847798 \n",
      "Epoch: 594000 : Reward eval/Train: -75.73245722894706/-76.0354688121961 \n",
      "Epoch: 596000 : Reward eval/Train: -75.78099861961962/-76.08441284846234 \n",
      "Epoch: 598000 : Reward eval/Train: -75.83163131377124/-76.13065148713191 \n",
      "Epoch: 600000 : Reward eval/Train: -75.87710750195033/-76.17828896158979 \n",
      "Epoch: 602000 : Reward eval/Train: -75.71324699469045/-76.22763351419933 \n",
      "Epoch: 604000 : Reward eval/Train: -75.76167287376697/-76.27406432863083 \n",
      "Epoch: 606000 : Reward eval/Train: -75.80991093010066/-76.32608706917787 \n",
      "Epoch: 608000 : Reward eval/Train: -75.85821819916107/-76.37193938061067 \n",
      "Epoch: 610000 : Reward eval/Train: -75.90949537298052/-76.42093475423488 \n",
      "Epoch: 612000 : Reward eval/Train: -75.95090102864296/-76.4691828849322 \n",
      "Epoch: 614000 : Reward eval/Train: -76.0003426451646/-76.51753515106869 \n",
      "Epoch: 616000 : Reward eval/Train: -76.04714324456423/-76.56258902777572 \n",
      "Epoch: 618000 : Reward eval/Train: -76.09345374448601/-76.6078113886809 \n",
      "Epoch: 620000 : Reward eval/Train: -76.14114209909343/-76.65446850447785 \n",
      "Epoch: 622000 : Reward eval/Train: -76.18234501560741/-76.69969230013879 \n",
      "Epoch: 624000 : Reward eval/Train: -76.23017464652833/-76.74170928391143 \n",
      "Epoch: 626000 : Reward eval/Train: -76.27691207437336/-76.78588568975181 \n",
      "Epoch: 628000 : Reward eval/Train: -76.32408362001192/-76.83107325184929 \n",
      "Epoch: 630000 : Reward eval/Train: -76.37062434254165/-76.8763280871525 \n",
      "Epoch: 632000 : Reward eval/Train: -76.41569391700176/-76.91985272871796 \n",
      "Epoch: 634000 : Reward eval/Train: -76.45826704071123/-76.96666562965318 \n",
      "Epoch: 636000 : Reward eval/Train: -76.50309804742645/-77.01152860994236 \n",
      "Epoch: 638000 : Reward eval/Train: -76.54587940151/-77.055082837626 \n",
      "Epoch: 640000 : Reward eval/Train: -76.3478092312801/-77.09840832661531 \n",
      "Epoch: 642000 : Reward eval/Train: -76.39257865053087/-77.13958201817353 \n",
      "Epoch: 644000 : Reward eval/Train: -76.43897610461892/-77.18215093886114 \n",
      "Epoch: 646000 : Reward eval/Train: -76.2737592527425/-77.22637689459489 \n",
      "Epoch: 648000 : Reward eval/Train: -76.31540679441856/-77.26706276637957 \n",
      "Epoch: 650000 : Reward eval/Train: -76.36131141390509/-77.31030656100592 \n",
      "Epoch: 652000 : Reward eval/Train: -76.40685052993504/-77.35224466995517 \n",
      "Epoch: 654000 : Reward eval/Train: -76.45029646234927/-77.20092781451375 \n",
      "Epoch: 656000 : Reward eval/Train: -76.49289410418572/-77.2433084337107 \n",
      "Epoch: 658000 : Reward eval/Train: -76.53398289084198/-77.28212513396555 \n",
      "Epoch: 660000 : Reward eval/Train: -76.57239225148658/-77.32353948679327 \n",
      "Epoch: 662000 : Reward eval/Train: -76.61259006672988/-77.3631604923427 \n",
      "Epoch: 664000 : Reward eval/Train: -76.65281588505047/-77.40300189306485 \n",
      "Epoch: 666000 : Reward eval/Train: -76.69271065446677/-77.44369015727614 \n",
      "Epoch: 668000 : Reward eval/Train: -76.73316380755435/-77.48346694092687 \n",
      "Epoch: 670000 : Reward eval/Train: -76.77202750019377/-77.51877703273558 \n",
      "Epoch: 672000 : Reward eval/Train: -76.81229929069845/-77.55720700596176 \n",
      "Epoch: 674000 : Reward eval/Train: -76.85327933931269/-77.5952533609323 \n",
      "Epoch: 676000 : Reward eval/Train: -76.892276929677/-77.63195501037231 \n",
      "Epoch: 678000 : Reward eval/Train: -76.93071377462124/-77.67188151570528 \n"
     ]
    }
   ],
   "source": [
    "if HYPERPARAM_TUNING: \n",
    "    \n",
    "    # Trial id :08 | Score :-71.19572300865248 --> {'lr_actor_critic': 0.0001443924191964091, 'dense_units_act_crit_0': 138, 'dense_units_act_crit_1': 210, 'kl_divergence_target': 0.05}\n",
    "    # Trial id :02 | Score :-72.41986896912546 --> {'lr_actor_critic': 0.000829844022383942, 'dense_units_act_crit_0': 234, 'dense_units_act_crit_1': 215, 'kl_divergence_target': 0.1}\n",
    "    # Trial id :03 | Score :-74.04910396664376 --> {'lr_actor_critic': 0.000987130392464792, 'dense_units_act_crit_0': 255, 'dense_units_act_crit_1': 188, 'kl_divergence_target': 0.1}\n",
    "\n",
    "    dir = r\"Hyperparam_kt_ppo\"\n",
    "    project_name = \"keras_tunning_ppo_env_model\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "            MyHyperModel( hyper_dir = dir+\"/\"+project_name,  writer = \"logs_hyper/ppo_env_model/\", evaluation_epoch = env._max_episode_steps, training_steps = 1000000,\n",
    "                sucess_criteria_epochs = SUCESS_CRITERIA_EPOCH, sucess_criteria_value= SUCESS_CRITERIA_VALUE,\n",
    "                #discount_min = 0.96, discount_max = 0.99, \n",
    "                discount= 0.99, # WAS THIS\n",
    "                #gae_factor = 0.95,  # WAS THIS\n",
    "                gae_min = 0.92, gae_max = 0.96, \n",
    "                policy_clip =0.2,\n",
    "                lr_actor_crit_min = 0.00001, lr_actor_crit_max = 0.001,\n",
    "                #entropy_factor = 0.05,  # WAS THIS\n",
    "                entropy_factor_min = 0.001, entropy_factor_max = 0.5,\n",
    "                lr_model_min = 0.00001, lr_model_max =  0.001, kl_divergence_target = None,#0.1\n",
    "                dense_layers = [42,62],\n",
    "                dense_min = 32, dense_max = 256, #WAS THIS ONE\n",
    "                environment_name=ENV, num_layers_act = 2, #max_num_layers_act = 2\n",
    "                num_layers_model = 1,  # WAS 2\n",
    "                training_epoch = 1,\n",
    "                memory_size = env._max_episode_steps, \n",
    "                normalize_reward=False, normalize_advantage= False,\n",
    "                scaling_factor_reward = 0.1\n",
    "                #memory_size_max= env._max_episode_steps\n",
    "                ),\n",
    "            objective= kt.Objective('total_train_reward', direction=\"max\"), \n",
    "            max_trials = 30,\n",
    "            # distribution_strategy= strategy,\n",
    "            directory=dir,\n",
    "            project_name=project_name,\n",
    "            #seed=0\n",
    "        )\n",
    "    tuner.search(x=[0], y=[1])\n",
    "else : \n",
    "    \n",
    "        print(\"Acquiring parameters ....\")\n",
    "\n",
    "        dense_units_act_crit = [128,64]\n",
    "        num_layers_actor_critic = len(dense_units_act_crit)\n",
    "        num_layer_m = 1\n",
    "        dense_units_model = [32]\n",
    "\n",
    "        model = run_training(training_steps = 120000,\n",
    "                            discount = 0.99, \n",
    "                            dense_units_act_crit = dense_units_act_crit,  \n",
    "                            dense_units_model = dense_units_model,  \n",
    "                            num_layer_a_c = num_layers_actor_critic,  \n",
    "                            num_layer_m = num_layer_m, \n",
    "                            writer = writer, \n",
    "                            environment_name = ENV, \n",
    "                            return_agent = True, \n",
    "                            lr_actor_critic= 0.0001,  \n",
    "                            lr_model = 0.001,\n",
    "                            sucess_criteria_epochs=SUCESS_CRITERIA_EPOCH, \n",
    "                            sucess_criteria_value = SUCESS_CRITERIA_VALUE, \n",
    "                            gae_lambda = 0.95, \n",
    "                            entropy_coeff = 0.05, \n",
    "                            policy_clip = 0.2, training_epoch = 20, \n",
    "                            scaling_factor_reward = 0.1, \n",
    "                            kl_divergence_target = 0.1,\n",
    "                            memory_size =  env._max_episode_steps,\n",
    "                            normalize_reward=False, normalize_advantage= False, use_mlflow = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_tech = \"soft\"\n",
    "hyperparam_combination=[]\n",
    "\n",
    "for trials in tuner.oracle.get_best_trials(num_trials=3):\n",
    "    print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for best_hps in tuner.get_best_hyperparameters(num_trials=3):\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=3)[-1]\n",
    "#env_model = tuner.get_best_models()[0]\n",
    "print(f\"Best Hyperparameters: {best_hps.__dict__}\")\n",
    "env_model = tuner.hypermodel.build(best_hps)  # Build the model with best hyperparameters\n",
    "env_model = rerun_training(training_steps  = 1500000, \n",
    "                        model = env_model\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = gym.make(ENV) #, render_mode = \"rgb_array\"\n",
    "final_rewards = final_evaluation(env_model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_intrinsic_\"+exploration_tech+\"_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_env = gym.make(ENV)#, render_mode = \"rgb_array\"\n",
    "# dir = r\"Hyperparam_kt_ppo\"\n",
    "# for trials in tuner.oracle.get_best_trials(num_trials=1):\n",
    "#     print(f\"Trial id :{trials.trial_id} | Score :{trials.score} -->\",trials.hyperparameters.values)    \n",
    "#     training_steps = 1000000\n",
    "#     entropy_factor = trials.hyperparameters.values[\"entropy_coeff\"]\n",
    "#     discount = trials.hyperparameters.values[\"discount\"]\n",
    "#     gae = trials.hyperparameters.values[\"gae_lambda\"]\n",
    "#     policy_clip = 0.2#trials.hyperparameters.values[\"policy_clip\"]\n",
    "#     scaling_factor_reward= 0.1#trials.hyperparameters.values[\"scaling_factor_reward\"]\n",
    "    \n",
    "    \n",
    "#     lr_actor=  trials.hyperparameters.values[\"lr_actor_critic\"]\n",
    "#     lr_critic=  None#trials.hyperparameters.values[\"lr_critic\"]\n",
    "#     lr_model=  None#trials.hyperparameters.values[\"lr_model\"]\n",
    "    \n",
    "#     try:\n",
    "#         n_dense_layers_actor = trials.hyperparameters.values[\"n_dense_layers_actor\"]\n",
    "#     except : \n",
    "#         n_dense_layers_actor = 1\n",
    "        \n",
    "#     try:\n",
    "#         n_dense_layers_critic = trials.hyperparameters.values[\"n_dense_layers_critic\"]\n",
    "#     except:\n",
    "#         n_dense_layers_critic = 1\n",
    "        \n",
    "\n",
    "#     dense_layers_actor = []\n",
    "#     for i in range(n_dense_layers_actor):\n",
    "#         dense_layers_actor.append(trials.hyperparameters.values['dense_units_act_crit_'+str(i)])\n",
    "\n",
    "#     dense_layers_critic = []\n",
    "#     # for i in range(n_dense_layers_critic):\n",
    "#     #     dense_layers_critic.append(trials.hyperparameters.values['dense_units_crit_'+str(i)])\n",
    "\n",
    "    \n",
    "#     n_dense_layers_model = 1\n",
    "#     dense_layers_model = []\n",
    "#     for i in range(n_dense_layers_model):\n",
    "#         dense_layers_model.append(trials.hyperparameters.values['n_dense_layers_model'+str(i)])\n",
    "\n",
    "#     model = run_training(\n",
    "#         training_steps = training_steps,   \n",
    "#             discount = discount,\n",
    "#             dense_units_act = dense_layers_actor, \n",
    "#             dense_units_crit = dense_layers_critic,\n",
    "#             dense_units_model = dense_layers_model,\n",
    "#             num_layer_a = n_dense_layers_actor,\n",
    "#             num_layer_c = n_dense_layers_critic,\n",
    "#             num_layer_m = n_dense_layers_model,\n",
    "#             writer = writer,  \n",
    "#             save_factor=50000, \n",
    "#             sucess_criteria_epochs =SUCESS_CRITERIA_EPOCH, \n",
    "#             sucess_criteria_value = SUCESS_CRITERIA_VALUE,\n",
    "#             environment_name=ENV,\n",
    "#             evaluation_epoch = env._max_episode_steps,\n",
    "#             return_agent = True,\n",
    "#             lr_actor= lr_actor, \n",
    "#             lr_critic= lr_critic,\n",
    "#             lr_model= lr_model,\n",
    "#             gae_lambda=gae,\n",
    "#             training_epoch= 200,\n",
    "#             entropy_coeff= entropy_factor,\n",
    "#             policy_clip = policy_clip,\n",
    "#             memory_size= env._max_episode_steps,\n",
    "#             id = int(trials.trial_id),\n",
    "#             scaling_factor_reward = scaling_factor_reward)\n",
    "        \n",
    "#     break\n",
    "# final_rewards = final_evaluation(model,val_env,n_tries=200, exploration=exploration_tech,  video_name = \"./ppo_intrinsic_\"+exploration_tech+\"_video.mp4\", env= ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MountainCar_A3C_TF1.ipynb",
   "provenance": [
    {
     "file_id": "13V9JMDwUOkrM9DucPAwwgBQfllmy9kjl",
     "timestamp": 1578567519575
    },
    {
     "file_id": "1rnkjQiF2XrsLm9SMvOuTxe8vX_7hZozI",
     "timestamp": 1578558966437
    }
   ]
  },
  "kernelspec": {
   "display_name": "rl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7879325296583a7806f15309d0945146a04fe73b0286fa5dbd4cdc57d601416b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
